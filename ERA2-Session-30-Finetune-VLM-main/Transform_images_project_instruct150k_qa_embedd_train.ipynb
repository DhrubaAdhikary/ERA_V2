{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhrubaAdhikary/ERA_V2/blob/master/ERA2-Session-30-Finetune-VLM-main/Transform_images_project_instruct150k_qa_embedd_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1235bafd-9625-44da-88cd-8c347abce2d5",
      "metadata": {
        "id": "1235bafd-9625-44da-88cd-8c347abce2d5"
      },
      "outputs": [],
      "source": [
        "!pip install glob2 peft wandb datasets trl==0.8.5 transformers accelerate -q\n",
        "!pip install -U bitsandbytes #flash_attn -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from transformers import AutoProcessor, AutoTokenizer\n",
        "# from PIL import Image\n",
        "# import requests\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import json\n",
        "\n",
        "# class llavaDataset(Dataset):\n",
        "#     \"\"\"\n",
        "#     Custom Dataset class to load and preprocess question-answer dataset with images.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, qa_dataset, clip_model_name):\n",
        "#         self.processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "#         self.qa_dataset = qa_dataset\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.qa_dataset)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         \"\"\"\n",
        "#         Retrieves a single data sample including the image, question, and answer.\n",
        "#         \"\"\"\n",
        "#         img_url = self.qa_dataset.iloc[idx]['img_url']\n",
        "#         ques = torch.tensor(\n",
        "#             np.array(np.matrix(self.qa_dataset.iloc[idx]['input']))[0]\n",
        "#         )\n",
        "#         ans = torch.tensor(\n",
        "#             np.array(np.matrix(self.qa_dataset.iloc[idx]['label']))[0]\n",
        "#         )\n",
        "\n",
        "#         # Load and process the image\n",
        "#         image_load = Image.open(requests.get(img_url, stream=True).raw)\n",
        "#         image_processed = self.processor(images=image_load, return_tensors=\"pt\")['pixel_values']\n",
        "#         image_processed = image_processed.squeeze(0)\n",
        "\n",
        "#         return image_processed, ques, ans\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     \"\"\"\n",
        "#     Custom collate function to batch image, question, and answer tensors.\n",
        "#     \"\"\"\n",
        "#     images = torch.stack([item[0] for item in batch])\n",
        "#     questions = torch.nn.utils.rnn.pad_sequence(\n",
        "#         [item[1] for item in batch], batch_first=True, padding_value=0\n",
        "#     )\n",
        "#     answers = torch.nn.utils.rnn.pad_sequence(\n",
        "#         [item[2] for item in batch], batch_first=True, padding_value=0\n",
        "#     )\n",
        "\n",
        "#     return {\n",
        "#         'images': images,\n",
        "#         'questions': questions,\n",
        "#         'answers': answers\n",
        "#     }\n",
        "\n",
        "# def get_dataloader(qa_dataset, clip_model_name, batch_size=32, shuffle=True):\n",
        "#     \"\"\"\n",
        "#     Function to create a DataLoader for llavaDataset.\n",
        "#     \"\"\"\n",
        "#     dataset = llavaDataset(qa_dataset, clip_model_name)\n",
        "#     return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Load CSV file\n",
        "#     csv_file = 'train_token.csv'\n",
        "#     qa_dataset = pd.read_csv(csv_file)\n",
        "\n",
        "#     # Define model and tokenizer\n",
        "#     clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "#     # Create DataLoader\n",
        "#     dataloader = get_dataloader(qa_dataset, clip_model_name, batch_size=8)\n",
        "\n",
        "#     # Example usage\n",
        "#     for batch in dataloader:\n",
        "#         print(batch)\n",
        "\n",
        "#     # Load JSON file\n",
        "#     with open('llava_instruct_150k.json') as f:\n",
        "#         data = json.load(f)\n",
        "\n",
        "#     # Flatten the data and create a sample\n",
        "#     data_instruct150_sample_val_flatten = []\n",
        "#     r = 0\n",
        "\n",
        "#     for a_idx, d in enumerate(data):\n",
        "#         image = d['image']\n",
        "#         image_url = f'http://images.cocodataset.org/train2017/{image}'\n",
        "#         conv_iter = iter(d['conversations'])\n",
        "#         for i in conv_iter:\n",
        "#             gpt_ans = next(conv_iter)\n",
        "#             if len(gpt_ans['value']) > 200:  # Filter long answers\n",
        "#                 continue\n",
        "#             if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n",
        "#                 image_q = i['value'].replace('<image>\\n', '').replace('\\n<image>', '') + ' [QA]'\n",
        "#                 image_a = gpt_ans['value'] + AutoTokenizer.from_pretrained(clip_model_name).eos_token\n",
        "#                 data_instruct150_sample_val_flatten.append([image_url, image_q, image_a])\n",
        "\n",
        "#         if a_idx % 10000 == 0:\n",
        "#             print(f\"{10000 * r} processed\")\n",
        "#             r += 1\n"
      ],
      "metadata": {
        "id": "HkE-MoJ2z6D_"
      },
      "id": "HkE-MoJ2z6D_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "22d74dae-af88-40d4-9b17-da067eaabff8",
      "metadata": {
        "id": "22d74dae-af88-40d4-9b17-da067eaabff8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPVisionModel, AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import gc\n",
        "import numpy as np\n",
        "import os\n",
        "import glob2\n",
        "# from dataset import collate_fn, llavadataset\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pickle\n",
        "\n",
        "# import wandb\n",
        "# from google.colab import userdata\n",
        "# wandb1 = userdata.get('wandb')\n",
        "# os.environ[\"WANDB_API_KEY\"] = wandb1\n",
        "os.environ[\"WANDB_API_KEY\"] = \"ad2ee17247b83846ef223b692508bd06453d50e7\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# teacher forcing simulated annealing scheduler\n",
        "# Below code is used simulated annealing scheduler called frange_cycle_linear.\n",
        "# This function generates a cyclic schedule for a hyperparameter, often used in teacher forcing or other training techniques where a parameter\n",
        "# (such as the probability of applying teacher forcing) needs to change gradually over time.\n",
        "\n",
        "# The frange_cycle_linear function is commonly used to schedule the teacher forcing ratio during training, which might start at a low value\n",
        "# (e.g., almost always letting the model predict on its own) and gradually increase (i.e., using ground truth more often).\n",
        "# With multiple cycles, this ratio oscillates throughout the training, allowing the model to learn in different regimes over time.\n",
        "\n",
        "def frange_cycle_linear(n_iter, start=0.0001, stop=0.9999,  n_cycle=1, ratio=0.8):\n",
        "    # n_iter : total number of iterations for which the schedule will be computed.\n",
        "    # start : initial value of the schedule. For teacher forcing, this could represent the starting probability of forcing the model to use the ground-truth data during training.\n",
        "    # stop : maximum or final value of the schedule, typically representing the probability of not using teacher forcing.\n",
        "    # n_cycle : means how many times the schedule will oscillate from start to stop.\n",
        "    # ratio :  fraction of each cycle where the parameter linearly increases from start to stop.\n",
        "\n",
        "    # Create  Schedule Array:\n",
        "    # This initializes an array L of length n_iter with all elements set to stop. This means that if no further changes are made, the parameter will stay at the stop value throughout the training.\n",
        "    L = np.ones(n_iter) * stop\n",
        "\n",
        "    # period: Defines the length of each cycle, i.e., how many iterations each cycle spans. If n_cycle=1, the entire schedule is a single cycle; if n_cycle=2, the period is half the total iterations.\n",
        "    # step: This defines the amount by which the parameter will increase in each iteration during the linear growth phase of the cycle.\n",
        "    # The ratio controls how much of the period is used for this linear increase.\n",
        "    period = n_iter/n_cycle\n",
        "    step = (stop-start)/(period*ratio) # linear schedule\n",
        "\n",
        "    # runs for each cycle.\n",
        "    # This loop ensures that the parameter starts at start, increases linearly over part of the cycle (determined by ratio), and then stays at stop for the remainder of the cycle.\n",
        "    for c in range(n_cycle):\n",
        "        # For each cycle, start the parameter value (v) at start and initialize an index (i) to 0.\n",
        "        v, i = start, 0\n",
        "        #  For each cycle, increment the value v by the step size and place it into the correct index of the array L.\n",
        "        while v <= stop and (int(i+c*period) < n_iter):\n",
        "            # Update the schedule at the correct position in L.\n",
        "            L[int(i+c*period)] = v\n",
        "            # The value increases linearly from start towards stop.\n",
        "            v += step\n",
        "            i += 1\n",
        "\n",
        "    # After constructing the schedule in L, the function returns 1 - L.\n",
        "    # This effectively inverts the values in L, making the schedule start from (1 - stop) and end at (1 - start).\n",
        "    # This is often done to control the probability of certain actions, such as teacher forcing.\n",
        "    return (1 - L)"
      ],
      "metadata": {
        "id": "Tdy92wAEE9pm"
      },
      "id": "Tdy92wAEE9pm",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define models\n",
        "phi_model_name  = \"microsoft/Phi-3-mini-128k-instruct\"#\"microsoft/phi-2\"\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "device = 'cuda'\n",
        "max_steps = 100000"
      ],
      "metadata": {
        "id": "L27tf0djE9uv"
      },
      "id": "L27tf0djE9uv",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annealing_teacher_forcing_scheduler = frange_cycle_linear(max_steps)\n",
        "\n",
        "class SimpleResBlock(nn.Module):\n",
        "    def __init__(self, phi_embed):\n",
        "        super().__init__()\n",
        "        # Layer Normalization: Normalizes the input to have zero mean and unit variance across the feature dimension.\n",
        "        # It helps stabilize the training by ensuring that the values passed through the network remain in a consistent range.\n",
        "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
        "\n",
        "        # nn.Sequential defines a sequential container, meaning a series of layers applied one after another.\n",
        "        # nn.Linear(phi_embed, phi_embed): A fully connected (linear) layer that takes an input of size phi_embed and outputs the same size (phi_embed).\n",
        "        # GELU (Gaussian Error Linear Unit) is an activation function that smooths out nonlinearities.\n",
        "        # It's similar to ReLU, but it has a smoother gradient, which can improve performance in some architectures.\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(phi_embed, phi_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(phi_embed, phi_embed)\n",
        "        )\n",
        "    # This method defines how the input x passes through the block during forward propagation.\n",
        "    # It takes an input tensor x, normalizes it, processes it through two linear layers with a GELU activation in between, and then adds the original input x back to the output of the transformation.\n",
        "    # The residual connection is important because it allows the model to retain information from earlier layers and makes it easier to train deep models.\n",
        "    def forward(self, x):\n",
        "        x = self.pre_norm(x)\n",
        "        # key part of a residual block\n",
        "        return x + self.proj(x)"
      ],
      "metadata": {
        "id": "R_TvyC9GE911"
      },
      "id": "R_TvyC9GE911",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code defines a neural network model called CLIPPhi2Model,\n",
        "# which combines two pretrained models:\n",
        "# a CLIP vision model for image embeddings\n",
        "# and a causal language model (Phi-2)\n",
        "# for text generation.\n",
        "class CLIPPhi2Model(torch.nn.Module):\n",
        "    # size of the image embeddings from the CLIP model (768-dimensional).\n",
        "    # size of the text embeddings from the Phi-2 model (3072 -dimensional).\n",
        "    def __init__(self, clip_embed=768, phi_embed=3072):\n",
        "        super().__init__()\n",
        "\n",
        "        #  End-of-sequence token ID for the language model.\n",
        "        self.EOS_TOKEN_ID    = 50256\n",
        "        # token to represent the presence of an image.\n",
        "        self.IMAGE_TOKEN_ID  = 23893 # token for comment\n",
        "\n",
        "        # pretrained models\n",
        "        self.phi_model = AutoModelForCausalLM.from_pretrained(phi_model_name,\n",
        "                                            torch_dtype=torch.float16,\n",
        "                                            trust_remote_code=True)\n",
        "        self.clip_model = CLIPVisionModel.from_pretrained(clip_model_name)\n",
        "\n",
        "        # projection layers\n",
        "        #  A linear layer that projects the lower-dimensional image embeddings (768 from CLIP) to match the higher-dimensional text embeddings (2560 from Phi-2).\n",
        "        self.projection = torch.nn.Linear(clip_embed, phi_embed)\n",
        "        # A simple residual block (SimpleResBlock) that processes the projected image embeddings.\n",
        "        self.resblock = SimpleResBlock(phi_embed)\n",
        "\n",
        "        # Freeze Weights\n",
        "        # IMPORTANT:\n",
        "        # The pretrained models (both Phi-2 and CLIP) are frozen so their weights are not updated during training.\n",
        "        # This is common in transfer learning to use pretrained features while only training new layers.\n",
        "        for network in [self.phi_model, self.clip_model]:\n",
        "            for param in network.parameters():\n",
        "                param.requires_grad_(False)\n",
        "\n",
        "        # load checkpoint weights\n",
        "        # If pre-trained projection and residual block weights are available (clipphi_proj.pth and clipphi_resblock.pth), they are loaded.\n",
        "        if os.path.isfile('model_chkpt/clipphi_proj.pth'):\n",
        "            self.projection.load_state_dict(torch.load('model_chkpt/clipphi_proj.pth'))\n",
        "            self.resblock.load_state_dict(torch.load('model_chkpt/clipphi_resblock.pth'))\n",
        "\n",
        "\n",
        "    # To generate text (e.g., captions) from images using the model.\n",
        "    # images: The processed input image data. max_length: Maximum length of the generated text.  tokenizer: Tokenizer to convert text into token IDs and vice versa.\n",
        "    def generate(self,images,max_length,tokenizer):\n",
        "        # clip model output for image\n",
        "        # input images are passed through the CLIP model (clip_model) to get their embeddings.\n",
        "        clip_outputs = self.clip_model(**images)\n",
        "        # remove cls token\n",
        "        images = clip_outputs.last_hidden_state[:,1:,:]\n",
        "        # The CLIP image embeddings are projected to the same dimensionality as the Phi-2 embeddings (2560) using self.projection\n",
        "        image_embeds = self.projection(images)\n",
        "        # This is further processed by the residual block (self.resblock).\n",
        "        image_embeds = self.resblock(image_embeds).to(torch.float16)\n",
        "\n",
        "        # Batch Size: Extract the number of images (batch_size).\n",
        "        batch_size = images.size(0)\n",
        "        # predicted_caption: Initialize a tensor to hold the generated caption tokens. It’s filled with the EOS token (50256), which represents the end-of-sequence.\n",
        "        predicted_caption = torch.full((batch_size,max_length),50256)\n",
        "        # Image Token Embedding: A custom token representing the image is embedded using the Phi-2 model's token embeddings.\n",
        "        img_token_tensor = torch.tensor(self.IMAGE_TOKEN_ID).repeat(batch_size, 1)\n",
        "        # This acts as the start of the sequence (bos_token) for generation.\n",
        "        bos_token_embeds = self.phi_model.model.embed_tokens(img_token_tensor.to(image_embeds.device))\n",
        "        # Concatenate: The image embeddings (image_embeds) are concatenated with the BOS token embeddings (bos_token_embeds) to create the initial input for the text generation process.\n",
        "        combined_embeds  = torch.cat([image_embeds, bos_token_embeds], dim=1) # 4,9,2560\n",
        "\n",
        "\n",
        "        # Prepare for the Next Token: The predicted token is embedded using the Phi-2 model's embeddings (embed_tokens) and concatenated with the existing sequence of embeddings (combined_embeds). This updated embedding sequence is then used in the next iteration to predict the next token.\n",
        "\n",
        "        # Repeat: The process repeats for the entire caption length.\n",
        "        # Loop through max_length: For each position pos (up to max_length - 1), the model generates one token at a time.\n",
        "        for pos in range(max_length - 1):\n",
        "            # pass through the model\n",
        "\n",
        "            # Model Forward Pass: The Phi-2 language model takes the combined_embeds (which includes both the image and any previously generated tokens) as input and predicts the next token's logits.\n",
        "            model_output_logits = self.phi_model.forward(inputs_embeds = combined_embeds)['logits'] # 4,49,51200\n",
        "            predicted_word_token_logits = model_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
        "            # Token Prediction: The logits for the last position ([:, -1, :]) are extracted and passed through\n",
        "            # torch.argmax to get the predicted token with the highest probability.\n",
        "            # This token is stored in the predicted_caption tensor.\n",
        "            predicted_word_token = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
        "            predicted_caption[:,pos] = predicted_word_token.view(1,-1).to('cpu')\n",
        "            # The predicted token is embedded using the Phi-2 model's embeddings (embed_tokens).\n",
        "            next_token_embeds = self.phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
        "            # This is concatenated with the existing sequence of embeddings (combined_embeds).\n",
        "            combined_embeds   = torch.cat([combined_embeds, next_token_embeds], dim=1)\n",
        "            # This updated embedding sequence is then used in the next iteration to predict the next token.\n",
        "\n",
        "        # After generating the tokens for all positions, the method returns the complete predicted_caption tensor containing the token IDs for the generated caption.\n",
        "        return predicted_caption\n",
        "\n",
        "    def forward(self, images, target_captions,step,max_steps):\n",
        "        # batch_size: The number of samples in the batch.\n",
        "        # target_length: The length of the target captions (number of tokens).\n",
        "        batch_size    = target_captions.size(0)\n",
        "        target_length = target_captions.shape[1]\n",
        "         #print(f\"GPU memory {torch.cuda.max_memory_allocated()/ (1024 ** 3):.2f} GB\")\n",
        "\n",
        "        # clip model output for image\n",
        "        # Input: The input images are passed through the CLIP model, which outputs image embeddings.\n",
        "        clip_outputs = self.clip_model(**images)\n",
        "        # Remove CLS Token: The CLS token (used for classification) is removed, as it’s not needed for caption generation.\n",
        "        images = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
        "\n",
        "        # projection layer\n",
        "        # The 768-dimensional CLIP image embeddings are projected to the 2560-dimensional space required by the Phi-2 model using a linear layer.\n",
        "        image_embeds = self.projection(images).to(torch.float16)\n",
        "        #image_embeds = self.resblock(image_embeds).to(torch.float16)\n",
        "\n",
        "        # add comment token from phi2\n",
        "        # A special image token (self.IMAGE_TOKEN_ID) is embedded using the Phi-2 model's embedding layer.\n",
        "        img_token_tensor = torch.tensor(self.IMAGE_TOKEN_ID).repeat(batch_size, 1)\n",
        "        # This token is a placeholder to mark where the image information ends and the text generation starts.\n",
        "        img_token_embeds = self.phi_model.model.embed_tokens(img_token_tensor.to(image_embeds.device))\n",
        "        # The image embeddings are concatenated with the image token embeddings to form combined_embeds, which is the input to the Phi-2 model for text generation.\n",
        "        combined_embeds  = torch.cat([image_embeds, img_token_embeds], dim=1) # 4,49,2560\n",
        "        del clip_outputs\n",
        "        del image_embeds\n",
        "\n",
        "        # for loss\n",
        "        loss = 0\n",
        "        # In each iteration, the Phi-2 model generates one token at a time based on the current input embeddings (combined_embeds), which include both image and previously generated token embeddings.\n",
        "        for pos in range(target_length - 1):\n",
        "            # pass through the model\n",
        "            # The Phi-2 model processes the current embeddings (combined_embeds) to generate the next token's logits (model_output_logits).\n",
        "            model_output_logits = self.phi_model.forward(inputs_embeds = combined_embeds)['logits'] # 4,49,51200\n",
        "            # The logits for the last predicted token in the sequence are extracted ([:, -1, :]), and then reshaped to match the dimensions expected by the loss function.\n",
        "            predicted_word_token_logits = model_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
        "            # The cross-entropy loss is computed between the predicted token logits and the actual target token at the current position (target_captions[:, pos]).\n",
        "            # The loss is smoothed with label_smoothing to avoid overconfident predictions, and tokens with the EOS_TOKEN_ID are ignored.\n",
        "            pos_loss = F.cross_entropy(predicted_word_token_logits.view(-1,predicted_word_token_logits.size(-1)), target_captions[:, pos].contiguous().view(-1), ignore_index=self.EOS_TOKEN_ID,label_smoothing=0.1)\n",
        "            # print(f\"pos {pos} loss {pos_loss}\")\n",
        "            # The loss for the current token is added to the total batch loss.\n",
        "            loss += pos_loss\n",
        "\n",
        "            # Store Predicted Token: The predicted token is stored in the predicted_caption tensor, which will hold the complete generated sequence.\n",
        "            predicted_word_token = torch.argmax(predicted_word_token_logits, dim=-1) # 4,1\n",
        "            #print(f\"predicted_word_token {predicted_word_token} and target_captions {target_captions[:,pos]}\")\n",
        "            # Teacher Forcing: For the first few tokens (up to pos <= 5) and early in training (step <= int(0.6 * max_steps)),the model uses teacher forcing.\n",
        "            # In this case, instead of relying on its own predictions, the model is fed the correct target token from target_captions.\n",
        "            # do teacher forcing or model output based on annealing scheduler probability\n",
        "            if pos <= 5 and step <= int(0.6 * max_steps): # teacher forcing\n",
        "                next_token_embeds = self.phi_model.model.embed_tokens(target_captions[:,pos].unsqueeze(1)) # 4,1,2560\n",
        "            else:\n",
        "                next_token_embeds = self.phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
        "\n",
        "            # The predicted token is embedded and concatenated to the existing embeddings to generate the next token in the sequence.\n",
        "            combined_embeds   = torch.cat([combined_embeds, next_token_embeds], dim=1)\n",
        "\n",
        "        #average_loss\n",
        "        # The total loss is averaged over all token positions to get the final loss for the batch.\n",
        "        loss = loss / target_length\n",
        "\n",
        "        # for efficient memory utilization\n",
        "        del combined_embeds\n",
        "        del model_output_logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "zW_qCBb5E95u"
      },
      "id": "zW_qCBb5E95u",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This funcation evaluates a trained model on a single batch of data from a validation set.\n",
        "# It compares the model’s predictions (generated text) with the ground truth (target captions) and prints both the target and predicted captions.\n",
        "def model_validate_one_batch(model,device,val_dataloader,max_length,tokenizer):\n",
        "    # This switches the model into evaluation mode, which ensures certain behaviors like dropout and batch normalization are disabled during inference.\n",
        "    model.eval()\n",
        "    # This disables gradient tracking, which reduces memory usage and speeds up computations since gradients are not needed during evaluation or inference.\n",
        "    with torch.no_grad():\n",
        "        # val_dataloader: The validation data loader provides batches of images and their corresponding target captions.\n",
        "        # For each batch, images contains the input images, and target_captions contains the ground-truth captions.\n",
        "        for batch_idx, (images, target_captions) in enumerate(val_dataloader):\n",
        "            images = {'pixel_values': images.to(device)}\n",
        "            target_captions = target_captions.to(device)\n",
        "            # decodes the tokenized target captions back into human-readable text.\n",
        "            # It uses the tokenizer associated with the model to convert the numerical token IDs into their corresponding text.\n",
        "            # The 50256 token ID corresponds to a special token (likely the end-of-sequence or padding token), which is ignored during decoding.\n",
        "            target_captions_decoded = tokenizer.batch_decode(target_captions,ignore_index = 50256)\n",
        "            # This calls the model’s generate method to create text predictions from the input images.\n",
        "            predicted_captions = model.generate(images,max_length,tokenizer)\n",
        "            # The predicted token sequences are also decoded back into human-readable text, just like the target captions.\n",
        "            predicted_captions_decoded = tokenizer.batch_decode(predicted_captions,ignore_index = 50256)\n",
        "\n",
        "            # iterates over the decoded predicted captions and prints both the target and predicted captions for comparison.\n",
        "            # pc_idx: The index of the current caption in the batch.\n",
        "            # target_captions_decoded[pc_idx]: The ground-truth caption for the corresponding image.\n",
        "            # predicted_captions_decoded[pc_idx]: The predicted caption generated by the model for the same image.\n",
        "            for pc_idx,pc in enumerate(predicted_captions_decoded):\n",
        "                print(f\"{pc_idx} - Target captions:\\n {target_captions_decoded[pc_idx]}  \\n{pc_idx} - predicted_captions:\\n {pc} \")\n",
        "            return # validate only 1 batch"
      ],
      "metadata": {
        "id": "sR-WdIEKE-EI"
      },
      "id": "sR-WdIEKE-EI",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4e79ab0e-7c21-42a4-a121-23f0168a4765",
      "metadata": {
        "id": "4e79ab0e-7c21-42a4-a121-23f0168a4765"
      },
      "outputs": [],
      "source": [
        "# This function, train_model(), is responsible for training the multimodal model that combines image (CLIP) and text (Phi2) embeddings.\n",
        "# The function iteratively processes batches from the training data, calculates the loss, and updates the model parameters.\n",
        "# It also handles periodic model validation, saving checkpoints, and logging progress\n",
        "def train_model(model, train_loader, val_dataloader,optimizer, device,max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer):\n",
        "    print(f\"Training started.\")\n",
        "\n",
        "    # max_step_reached: A flag to track if the maximum number of training steps has been reached.\n",
        "    max_step_reached = 0\n",
        "    # step: Tracks the current step in the training process.\n",
        "    step = 0\n",
        "    # max_length: Sets the maximum length for the generated captions (set to 20 tokens here).\n",
        "    max_length = 20\n",
        "    # running_loss: Accumulates the loss across multiple steps, used for logging the average loss.\n",
        "    running_loss = 0.\n",
        "    # model.train(): Puts the model in training mode (this affects dropout and batch normalization layers, if any).\n",
        "    model.train()\n",
        "\n",
        "    #This outer loop iterates over a large number of epochs\n",
        "    #The inner loop iterates through the train_loader, processing batches of images and their corresponding captions.\n",
        "    # batch_idx: The index of the current batch.\n",
        "    for epoch in range(100000):\n",
        "        for batch_idx, (images, target_captions) in enumerate(train_loader):\n",
        "\n",
        "            # images: The image data is prepared as a dictionary with the key 'pixel_values' and moved to the appropriate device (GPU).\n",
        "            # target_captions: The target captions (text) are also moved to the device.\n",
        "            # manage OOM issue, skip batch for long captions\n",
        "            if target_captions.shape[1] >= max_token_filter:\n",
        "                print(f\"Batch skipped as captions too long.\")\n",
        "                continue\n",
        "            images = {'pixel_values': images.to(device)}\n",
        "            target_captions = target_captions.to(device)\n",
        "\n",
        "            # Clears the gradients before backpropagation (a standard step to prevent accumulating gradients from previous batches).\n",
        "            optimizer.zero_grad()\n",
        "            # The model processes the images and target captions to compute the loss for this batch.\n",
        "            # The step and max_steps parameters may influence the annealing of teacher forcing or other training aspects.\n",
        "            loss = model(images, target_captions,step,max_steps)\n",
        "            #print(f\"teacher {teacher_forcing} and loss {loss}\")\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # log step\n",
        "            if (step % log_step == 0):\n",
        "                if step == 0:\n",
        "                    print(f\"Step {step}/{max_steps}: Avg Running Loss = {running_loss}\")\n",
        "                else:\n",
        "                    print(f\"Step {step}/{max_steps}: Avg Running Loss = {running_loss /log_step}\")\n",
        "                running_loss = 0.\n",
        "            wandb.log({\"step\": step, \"train_loss\": loss.item()})\n",
        "\n",
        "            # increment step\n",
        "            step += 1\n",
        "            teacher_forcing = False\n",
        "\n",
        "            # loss backprop\n",
        "            # loss.backward(): Computes the gradients for all trainable parameters using backpropagation.\n",
        "            # optimizer.step(): Updates the model parameters based on the computed gradients.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # save model\n",
        "            if step % model_save_step == 0 or (step == max_steps):\n",
        "                print(\"Saving Checkpoint for step : \", step)\n",
        "                torch.save(model.projection.state_dict(),'model_chkpt/clipphi_proj.pth')\n",
        "                torch.save(model.resblock.state_dict(),'model_chkpt/clipphi_resblock.pth')\n",
        "\n",
        "            # check random validation of images\n",
        "            if step % model_val_step == 0 or (step == max_steps):\n",
        "                model_validate_one_batch(model,device,val_dataloader,max_length,tokenizer)\n",
        "                model.train()\n",
        "\n",
        "            # global max steps reached\n",
        "            if step >= max_steps:\n",
        "                max_step_reached = 1\n",
        "                break\n",
        "\n",
        "        if max_step_reached == 1:\n",
        "            break\n",
        "    print(f\"Reached the max steps. Training stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "np0c9CEysMqP",
        "outputId": "38be666f-4570-4685-ca22-5d9e2e4ec700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "np0c9CEysMqP",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoProcessor, AutoTokenizer\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pickle\n",
        "import requests\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "class llavadataset(Dataset):\n",
        "  def __init__(self, coco_data, phi_model_name, clip_model_name,train_flag,tokenizer):\n",
        "\n",
        "    self.tokenizer  = tokenizer\n",
        "    self.processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
        "    self.caption_dataset = coco_data\n",
        "\n",
        "    train_size = int(0.9 * len(self.caption_dataset))\n",
        "    print(f\"Train size {train_size} and validation size {len(self.caption_dataset) - train_size}\")\n",
        "\n",
        "    if train_flag == 'train':\n",
        "      self.caption_dataset = self.caption_dataset[0:train_size]\n",
        "    else:\n",
        "      self.caption_dataset = self.caption_dataset[train_size:]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.caption_dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # from image perspective\n",
        "\n",
        "    img_url = self.caption_dataset.loc[idx]['image_url']\n",
        "    caption = self.caption_dataset.loc[idx]['caption']\n",
        "    # image load\n",
        "    image_load = Image.open(requests.get(img_url,stream=True).raw)\n",
        "    #image_load = Image.open(img_url)\n",
        "    image_processed = self.processor(images=image_load, return_tensors=\"pt\") ['pixel_values']\n",
        "    image_processed = image_processed.squeeze(0)\n",
        "    a = self.tokenizer(caption, return_tensors=\"pt\", return_attention_mask=False)\n",
        "    return(image_processed , a['input_ids'].squeeze(0))\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    image_embeddings, captions = zip(*batch)\n",
        "    image_embeddings_stacked = torch.stack(image_embeddings, dim=0)\n",
        "    captions_padded = torch.nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=50256)\n",
        "    return (image_embeddings_stacked, captions_padded)"
      ],
      "metadata": {
        "id": "76VhN-GM4GnG"
      },
      "id": "76VhN-GM4GnG",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2e2eaac0-f74d-4db4-bf53-cc25ec2d7c7e",
      "metadata": {
        "id": "2e2eaac0-f74d-4db4-bf53-cc25ec2d7c7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# defines the workflow for training a multimodal GPT model.\n",
        "# which is likely based on a combination of CLIP and the Phi2 language model.\n",
        "# The code handles loading the data, setting up the model, and defining the training process.\n",
        "def main():\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/captions.pickle\", \"rb\") as fp:   # Unpickling\n",
        "        coco_unpickle = pickle.load(fp)\n",
        "\n",
        "    train_batch_size = 1\n",
        "    val_batch_size   = 1\n",
        "    tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
        "\n",
        "    # model\n",
        "    MModalGPT        = CLIPPhi2Model().to(device)\n",
        "    # The maximum number of training steps (iterations) is set to 20,000.\n",
        "    max_steps        = 20000\n",
        "    model_save_step  = 100\n",
        "    model_val_step   = 100\n",
        "    log_step         = 100\n",
        "    # Limits the maximum number of tokens (words, subwords) in the processed inputs, likely to filter out long captions or questions.\n",
        "    max_token_filter = 35\n",
        "\n",
        "    # train_dataloader: A PyTorch DataLoader for the training dataset.\n",
        "    # llavadataset: A custom dataset class that combines the loaded COCO dataset (coco_unpickle), the tokenizer, and other settings. It processes both image and text data.\n",
        "    # collate_fn: A custom function (collate_fn) to pad or batch the input data correctly, including images and tokenized text.\n",
        "    # val_dataloader: The DataLoader for the validation dataset, which is similar to the training DataLoader but with a smaller batch size (val_batch_size=2).\n",
        "\n",
        "    # data loaders\n",
        "    train_dataloader = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'train',tokenizer),\n",
        "                      collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
        "\n",
        "    val_dataloader   = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'val',tokenizer),\n",
        "                      collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-4)\n",
        "    train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7fde431c-86b8-4838-bff5-51139d5924ed",
      "metadata": {
        "id": "7fde431c-86b8-4838-bff5-51139d5924ed",
        "outputId": "077082e3-ffc3-45d1-9195-326eeb315708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651,
          "referenced_widgets": [
            "a90284283b9448ffb383df427c4e3ed7",
            "5ea61095f10b4e41aa9605d30ac451d7",
            "819b6e7023a244ac9f2b9d7cdf13e955",
            "d61ac3a73a1141fc8fd1c36bafae7963",
            "10c755bdeec1424c9420f2be592ec8b3",
            "d772fc57e40647418f501512276f5f68",
            "0cbe91e61cb049f98a43bbd1b1ebe43a",
            "7e40f41f89464793bda99aee375f47eb",
            "ad8ff6d2d18b4002a888716864cfe2cf",
            "370997e4a81b48b6a2c11ffa899fd388",
            "3bcd89350a154b9e88ef523110ec6bfa",
            "3badb624cbb8414c8d23bc7b92aadbf3",
            "92404a2fa24a405e8502bf99d82c4f88",
            "baafce6ee2e44b4298265f1d22688b15",
            "12da9893fcf34230b13a10caafd32cd5",
            "0e565ddb363f4cfa9294ec8eb63d6684",
            "22fe3e59bd324ecdb96b5c80ce6dae38",
            "0beb0ff565e44a23ab0914be01c16af0",
            "2efc9f573dba4b94a3e7bb20afd09d23"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2xe7c7z5) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a90284283b9448ffb383df427c4e3ed7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clip_phi3_finetune</strong> at: <a href='https://wandb.ai/dhruba/clip_phi2_project/runs/2xe7c7z5' target=\"_blank\">https://wandb.ai/dhruba/clip_phi2_project/runs/2xe7c7z5</a><br/> View project at: <a href='https://wandb.ai/dhruba/clip_phi2_project' target=\"_blank\">https://wandb.ai/dhruba/clip_phi2_project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241102_111935-2xe7c7z5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2xe7c7z5). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241102_113341-yhc478vu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dhruba/clip_phi2_project/runs/yhc478vu' target=\"_blank\">clip_phi3_finetune</a></strong> to <a href='https://wandb.ai/dhruba/clip_phi2_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dhruba/clip_phi2_project' target=\"_blank\">https://wandb.ai/dhruba/clip_phi2_project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dhruba/clip_phi2_project/runs/yhc478vu' target=\"_blank\">https://wandb.ai/dhruba/clip_phi2_project/runs/yhc478vu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad8ff6d2d18b4002a888716864cfe2cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 20.81 MiB is free. Process 23999 has 39.54 GiB memory in use. Of the allocated memory 37.72 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ed1dd041d103>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#  This controls the precision used for 32-bit floating-point matrix multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_float32_matmul_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'medium'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-df82d92f4bb4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mMModalGPT\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mCLIPPhi2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# The maximum number of training steps (iterations) is set to 20,000.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmax_steps\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 20.81 MiB is free. Process 23999 has 39.54 GiB memory in use. Of the allocated memory 37.72 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Initialize Weights & Biases (WandB) for Experiment Tracking:\n",
        "wandb.init(project=\"clip_phi2_project\", name=\"clip_phi3_finetune\")\n",
        "# enables Automatic Mixed Precision (AMP) in PyTorch, specifically for CUDA-enabled GPUs.\n",
        "# Mixed Precision refers to using both 16-bit and 32-bit floating-point types during training.\n",
        "torch.amp.autocast('cuda', enabled=True)\n",
        "# torch.cuda.empty_cache(): This function frees up unused memory held by PyTorch in the CUDA memory cache.\n",
        "torch.cuda.empty_cache()\n",
        "# clears up any unreferenced memory\n",
        "gc.collect()\n",
        "#  This controls the precision used for 32-bit floating-point matrix multiplication\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/captions.pickle\", \"rb\") as fp:   # Unpickling\n",
        "      coco_unpickle = pickle.load(fp)\n",
        "\n",
        "train_batch_size = 1\n",
        "val_batch_size   = 1\n",
        "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
        "\n",
        "# model\n",
        "MModalGPT        = CLIPPhi2Model().to(device)\n",
        "# The maximum number of training steps (iterations) is set to 20,000.\n",
        "max_steps        = 20000\n",
        "model_save_step  = 100\n",
        "model_val_step   = 100\n",
        "log_step         = 100\n",
        "# Limits the maximum number of tokens (words, subwords) in the processed inputs, likely to filter out long captions or questions.\n",
        "max_token_filter = 35\n",
        "\n",
        "# train_dataloader: A PyTorch DataLoader for the training dataset.\n",
        "# llavadataset: A custom dataset class that combines the loaded COCO dataset (coco_unpickle), the tokenizer, and other settings. It processes both image and text data.\n",
        "# collate_fn: A custom function (collate_fn) to pad or batch the input data correctly, including images and tokenized text.\n",
        "# val_dataloader: The DataLoader for the validation dataset, which is similar to the training DataLoader but with a smaller batch size (val_batch_size=2).\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'train',tokenizer),\n",
        "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 0, shuffle=True, pin_memory=True)\n",
        "\n",
        "val_dataloader   = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'val',tokenizer),\n",
        "                  collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 0, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-4)\n",
        "# model_validate_one_batch(MModalGPT,device,val_dataloader,20,tokenizer)\n"
      ],
      "metadata": {
        "id": "wfR-uZvP3Dtx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef4f6833d45b48d3820e680c3ba5b73f",
            "e54abd183adf4dcdafd40deb11b882fc",
            "5222fb7502984aa084a3cb144b8d9f68",
            "4415f2baf6eb44e9b3d9365f312a7b6e",
            "5b0d9fa49ba04546bcb8ebd2f6ba8244",
            "2e24e7ebb89348cb8fa3aef4c5137ff9",
            "6033f66d793645bb8e2e9d5cc03704d6",
            "3f0874949ec740f19ae825d8889e000e",
            "94312f4747d54b60af55aa67a404ffa9",
            "304c109da9714f7ba96da740fff3ae21",
            "cd48c55aaf26404092ca9669c781e143"
          ]
        },
        "outputId": "8ba82b23-3d27-4f23-b01a-780884ccec73"
      },
      "id": "wfR-uZvP3Dtx",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef4f6833d45b48d3820e680c3ba5b73f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size 179793 and validation size 19977\n",
            "Train size 179793 and validation size 19977\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\", line 413, in get_loc\n    return self._range.index(new_key)\nValueError: 9585 is not in range\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-26-3401d8e7909b>\", line 35, in __getitem__\n    img_url = self.caption_dataset.loc[idx]['image_url']\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\", line 1191, in __getitem__\n    return self._getitem_axis(maybe_callable, axis=axis)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\", line 1431, in _getitem_axis\n    return self._get_label(key, axis=axis)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\", line 1381, in _get_label\n    return self.obj.xs(label, axis=axis)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 4301, in xs\n    loc = index.get_loc(key)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\", line 415, in get_loc\n    raise KeyError(key) from err\nKeyError: 9585\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-11929cf6dfbf>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMModalGPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel_validate_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMModalGPT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-e33c0ef3c260>\u001b[0m in \u001b[0;36mmodel_validate_one_batch\u001b[0;34m(model, device, val_dataloader, max_length, tokenizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# val_dataloader: The validation data loader provides batches of images and their corresponding target captions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# For each batch, images contains the input images, and target_captions contains the ground-truth captions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_captions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtarget_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\", line 413, in get_loc\n    return self._range.index(new_key)\nValueError: 9585 is not in range\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-26-3401d8e7909b>\", line 35, in __getitem__\n    img_url = self.caption_dataset.loc[idx]['image_url']\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\", line 1191, in __getitem__\n    return self._getitem_axis(maybe_callable, axis=axis)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\", line 1431, in _getitem_axis\n    return self._get_label(key, axis=axis)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\", line 1381, in _get_label\n    return self.obj.xs(label, axis=axis)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 4301, in xs\n    loc = index.get_loc(key)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\", line 415, in get_loc\n    raise KeyError(key) from err\nKeyError: 9585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "# Print the values in the batch\n",
        "print(\"Batch values:\")\n"
      ],
      "metadata": {
        "id": "H9S7I2q6qzHB",
        "outputId": "87ffc14f-7b1e-46b0-f351-1133223e9db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "H9S7I2q6qzHB",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>Traceback (most recent call last):\n",
            "\n",
            "Exception ignored in: Exception ignored in: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430><function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "\n",
            "\n",
            "        Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "self._shutdown_workers()self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "          File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "self._shutdown_workers()self._shutdown_workers()    \n",
            "    \n",
            "if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():    if w.is_alive():    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    AssertionError    AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process': assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
            "\n",
            "can only test a child processcan only test a child processAssertionErrorAssertionError\n",
            ": \n",
            "can only test a child process: \n",
            "can only test a child process\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430><function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "Exception ignored in:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>        self._shutdown_workers()self._shutdown_workers()\n",
            "\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "            if w.is_alive():self._shutdown_workers()\n",
            "if w.is_alive():Exception ignored in: \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "        \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    if w.is_alive():Traceback (most recent call last):\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "AssertionError\n",
            "    :     self._shutdown_workers()AssertionErrorcan only test a child process\n",
            ": \n",
            "can only test a child process  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "    if w.is_alive():\n",
            "AssertionError: Exception ignored in:   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "\n",
            "AssertionErrorTraceback (most recent call last):\n",
            "Exception ignored in:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            ": <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>can only test a child process\n",
            "\n",
            "    self._shutdown_workers()Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "        Exception ignored in: if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>self._shutdown_workers()\n",
            "\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "\n",
            "    if w.is_alive():AssertionErrorself._shutdown_workers(): \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "can only test a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "AssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "can only test a child process\n",
            "AssertionErrorException ignored in: : <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>can only test a child process\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "        self._shutdown_workers()\n",
            "if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
            ": AssertionErrorcan only test a child process\n",
            ": can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x797efdd04430>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch values:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zj7xP-uDrX3s"
      },
      "id": "Zj7xP-uDrX3s",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a90284283b9448ffb383df427c4e3ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ea61095f10b4e41aa9605d30ac451d7",
              "IPY_MODEL_819b6e7023a244ac9f2b9d7cdf13e955"
            ],
            "layout": "IPY_MODEL_d61ac3a73a1141fc8fd1c36bafae7963"
          }
        },
        "5ea61095f10b4e41aa9605d30ac451d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c755bdeec1424c9420f2be592ec8b3",
            "placeholder": "​",
            "style": "IPY_MODEL_d772fc57e40647418f501512276f5f68",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "819b6e7023a244ac9f2b9d7cdf13e955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cbe91e61cb049f98a43bbd1b1ebe43a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e40f41f89464793bda99aee375f47eb",
            "value": 1
          }
        },
        "d61ac3a73a1141fc8fd1c36bafae7963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c755bdeec1424c9420f2be592ec8b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d772fc57e40647418f501512276f5f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cbe91e61cb049f98a43bbd1b1ebe43a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e40f41f89464793bda99aee375f47eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad8ff6d2d18b4002a888716864cfe2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_370997e4a81b48b6a2c11ffa899fd388",
              "IPY_MODEL_3bcd89350a154b9e88ef523110ec6bfa",
              "IPY_MODEL_3badb624cbb8414c8d23bc7b92aadbf3"
            ],
            "layout": "IPY_MODEL_92404a2fa24a405e8502bf99d82c4f88"
          }
        },
        "370997e4a81b48b6a2c11ffa899fd388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baafce6ee2e44b4298265f1d22688b15",
            "placeholder": "​",
            "style": "IPY_MODEL_12da9893fcf34230b13a10caafd32cd5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3bcd89350a154b9e88ef523110ec6bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e565ddb363f4cfa9294ec8eb63d6684",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22fe3e59bd324ecdb96b5c80ce6dae38",
            "value": 2
          }
        },
        "3badb624cbb8414c8d23bc7b92aadbf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0beb0ff565e44a23ab0914be01c16af0",
            "placeholder": "​",
            "style": "IPY_MODEL_2efc9f573dba4b94a3e7bb20afd09d23",
            "value": " 2/2 [00:02&lt;00:00,  1.42s/it]"
          }
        },
        "92404a2fa24a405e8502bf99d82c4f88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baafce6ee2e44b4298265f1d22688b15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12da9893fcf34230b13a10caafd32cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e565ddb363f4cfa9294ec8eb63d6684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22fe3e59bd324ecdb96b5c80ce6dae38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0beb0ff565e44a23ab0914be01c16af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efc9f573dba4b94a3e7bb20afd09d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef4f6833d45b48d3820e680c3ba5b73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e54abd183adf4dcdafd40deb11b882fc",
              "IPY_MODEL_5222fb7502984aa084a3cb144b8d9f68",
              "IPY_MODEL_4415f2baf6eb44e9b3d9365f312a7b6e"
            ],
            "layout": "IPY_MODEL_5b0d9fa49ba04546bcb8ebd2f6ba8244"
          }
        },
        "e54abd183adf4dcdafd40deb11b882fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e24e7ebb89348cb8fa3aef4c5137ff9",
            "placeholder": "​",
            "style": "IPY_MODEL_6033f66d793645bb8e2e9d5cc03704d6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5222fb7502984aa084a3cb144b8d9f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f0874949ec740f19ae825d8889e000e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94312f4747d54b60af55aa67a404ffa9",
            "value": 2
          }
        },
        "4415f2baf6eb44e9b3d9365f312a7b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_304c109da9714f7ba96da740fff3ae21",
            "placeholder": "​",
            "style": "IPY_MODEL_cd48c55aaf26404092ca9669c781e143",
            "value": " 2/2 [00:02&lt;00:00,  1.29s/it]"
          }
        },
        "5b0d9fa49ba04546bcb8ebd2f6ba8244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e24e7ebb89348cb8fa3aef4c5137ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6033f66d793645bb8e2e9d5cc03704d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f0874949ec740f19ae825d8889e000e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94312f4747d54b60af55aa67a404ffa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "304c109da9714f7ba96da740fff3ae21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd48c55aaf26404092ca9669c781e143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}