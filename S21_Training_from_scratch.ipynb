{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLI2cWjqjigl8sQBkxpQHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhrubaAdhikary/ERA_V2/blob/master/S21_Training_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45toRlhdWtJY",
        "outputId": "c738b6f9-a7c4-42ef-9e36-407b239cf92b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_hXYY4FNWZkM"
      },
      "outputs": [],
      "source": [
        "# !python /content/train_get2-9-speedup9.py\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 256  # reduced max sequence length to fit into 4GB GPU\n",
        "    vocab_size: int = 50304  # number of tokens\n",
        "    n_layer: int = 6  # increased number of layers for better learning\n",
        "    n_head: int = 8  # increased number of heads for better learning\n",
        "    n_embd: int = 256  # increased embedding dimension for better learning\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T)\n",
        "        y = (buf[1:]).view(B, T)\n",
        "        self.current_position += B*T\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 50  # increased warmup steps\n",
        "max_steps = 1000  # increased max steps\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "train_loader = DataLoaderLite(B=4, T=256)\n",
        "\n",
        "import time\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "\n",
        "num_epochs = 1\n",
        "accumulation_steps = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for step in range(max_steps):\n",
        "        t0 = time.time()\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            lr = get_lr(step)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        print(f\"Step {step + 1}/{max_steps}, Loss: {loss.item():.4f}, Time: {dt * 1000:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhgcCgM7Z_Uj",
        "outputId": "83494ad3-9fca-47d0-a3a4-2b8039d20e71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 330 batches\n",
            "Epoch 1/1\n",
            "Step 1/1000, Loss: 10.8596, Time: 18.97 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c413576343d3>:209: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2/1000, Loss: 10.8898, Time: 15.01 ms\n",
            "Step 3/1000, Loss: 10.8724, Time: 14.18 ms\n",
            "Step 4/1000, Loss: 10.8408, Time: 17.93 ms\n",
            "Step 5/1000, Loss: 10.3939, Time: 14.90 ms\n",
            "Step 6/1000, Loss: 10.2975, Time: 14.00 ms\n",
            "Step 7/1000, Loss: 10.3101, Time: 14.42 ms\n",
            "Step 8/1000, Loss: 10.2598, Time: 14.82 ms\n",
            "Step 9/1000, Loss: 10.2861, Time: 14.10 ms\n",
            "Step 10/1000, Loss: 10.2660, Time: 14.82 ms\n",
            "Step 11/1000, Loss: 10.2019, Time: 14.77 ms\n",
            "Step 12/1000, Loss: 10.3912, Time: 14.28 ms\n",
            "Step 13/1000, Loss: 10.1505, Time: 13.87 ms\n",
            "Step 14/1000, Loss: 10.1383, Time: 15.51 ms\n",
            "Step 15/1000, Loss: 10.1753, Time: 13.80 ms\n",
            "Step 16/1000, Loss: 10.2071, Time: 17.75 ms\n",
            "Step 17/1000, Loss: 10.1107, Time: 14.15 ms\n",
            "Step 18/1000, Loss: 10.0649, Time: 14.63 ms\n",
            "Step 19/1000, Loss: 10.0208, Time: 14.64 ms\n",
            "Step 20/1000, Loss: 10.0421, Time: 17.49 ms\n",
            "Step 21/1000, Loss: 9.8443, Time: 14.30 ms\n",
            "Step 22/1000, Loss: 10.0723, Time: 14.19 ms\n",
            "Step 23/1000, Loss: 9.8215, Time: 14.55 ms\n",
            "Step 24/1000, Loss: 9.8278, Time: 16.72 ms\n",
            "Step 25/1000, Loss: 9.9177, Time: 14.15 ms\n",
            "Step 26/1000, Loss: 9.9199, Time: 14.19 ms\n",
            "Step 27/1000, Loss: 9.9289, Time: 14.12 ms\n",
            "Step 28/1000, Loss: 9.8178, Time: 14.85 ms\n",
            "Step 29/1000, Loss: 9.8318, Time: 14.64 ms\n",
            "Step 30/1000, Loss: 9.8528, Time: 14.24 ms\n",
            "Step 31/1000, Loss: 9.8107, Time: 13.99 ms\n",
            "Step 32/1000, Loss: 9.7523, Time: 22.35 ms\n",
            "Step 33/1000, Loss: 9.8294, Time: 15.01 ms\n",
            "Step 34/1000, Loss: 9.7992, Time: 14.11 ms\n",
            "Step 35/1000, Loss: 9.7433, Time: 14.12 ms\n",
            "Step 36/1000, Loss: 9.5615, Time: 16.03 ms\n",
            "Step 37/1000, Loss: 9.5415, Time: 14.27 ms\n",
            "Step 38/1000, Loss: 9.7503, Time: 14.33 ms\n",
            "Step 39/1000, Loss: 9.4946, Time: 14.42 ms\n",
            "Step 40/1000, Loss: 9.6947, Time: 19.97 ms\n",
            "Step 41/1000, Loss: 9.5827, Time: 14.56 ms\n",
            "Step 42/1000, Loss: 9.6098, Time: 15.29 ms\n",
            "Step 43/1000, Loss: 9.6329, Time: 14.04 ms\n",
            "Step 44/1000, Loss: 9.5089, Time: 17.66 ms\n",
            "Step 45/1000, Loss: 9.3923, Time: 14.20 ms\n",
            "Step 46/1000, Loss: 9.3144, Time: 14.42 ms\n",
            "Step 47/1000, Loss: 9.5891, Time: 13.86 ms\n",
            "Step 48/1000, Loss: 9.4818, Time: 14.54 ms\n",
            "Step 49/1000, Loss: 9.4255, Time: 14.10 ms\n",
            "Step 50/1000, Loss: 9.2825, Time: 18.81 ms\n",
            "Step 51/1000, Loss: 9.2946, Time: 15.04 ms\n",
            "Step 52/1000, Loss: 9.3731, Time: 15.09 ms\n",
            "Step 53/1000, Loss: 9.1855, Time: 14.59 ms\n",
            "Step 54/1000, Loss: 9.1530, Time: 14.84 ms\n",
            "Step 55/1000, Loss: 9.2620, Time: 14.48 ms\n",
            "Step 56/1000, Loss: 9.1987, Time: 15.33 ms\n",
            "Step 57/1000, Loss: 8.9458, Time: 14.17 ms\n",
            "Step 58/1000, Loss: 9.0567, Time: 14.24 ms\n",
            "Step 59/1000, Loss: 8.8313, Time: 14.18 ms\n",
            "Step 60/1000, Loss: 8.9257, Time: 14.78 ms\n",
            "Step 61/1000, Loss: 8.7109, Time: 14.41 ms\n",
            "Step 62/1000, Loss: 8.8724, Time: 14.08 ms\n",
            "Step 63/1000, Loss: 8.8652, Time: 13.86 ms\n",
            "Step 64/1000, Loss: 8.8676, Time: 14.74 ms\n",
            "Step 65/1000, Loss: 8.4915, Time: 14.28 ms\n",
            "Step 66/1000, Loss: 8.6526, Time: 15.36 ms\n",
            "Step 67/1000, Loss: 8.6569, Time: 13.95 ms\n",
            "Step 68/1000, Loss: 8.4443, Time: 14.87 ms\n",
            "Step 69/1000, Loss: 8.3794, Time: 14.37 ms\n",
            "Step 70/1000, Loss: 8.3885, Time: 14.20 ms\n",
            "Step 71/1000, Loss: 8.2864, Time: 18.60 ms\n",
            "Step 72/1000, Loss: 8.3933, Time: 14.55 ms\n",
            "Step 73/1000, Loss: 8.2804, Time: 14.54 ms\n",
            "Step 74/1000, Loss: 8.3843, Time: 14.18 ms\n",
            "Step 75/1000, Loss: 8.3108, Time: 14.21 ms\n",
            "Step 76/1000, Loss: 8.3205, Time: 14.72 ms\n",
            "Step 77/1000, Loss: 8.0882, Time: 13.95 ms\n",
            "Step 78/1000, Loss: 8.1476, Time: 14.19 ms\n",
            "Step 79/1000, Loss: 8.2277, Time: 14.50 ms\n",
            "Step 80/1000, Loss: 8.2043, Time: 14.50 ms\n",
            "Step 81/1000, Loss: 8.2083, Time: 14.02 ms\n",
            "Step 82/1000, Loss: 8.1434, Time: 13.74 ms\n",
            "Step 83/1000, Loss: 8.1521, Time: 13.84 ms\n",
            "Step 84/1000, Loss: 8.1107, Time: 15.79 ms\n",
            "Step 85/1000, Loss: 7.8802, Time: 13.78 ms\n",
            "Step 86/1000, Loss: 7.9798, Time: 14.04 ms\n",
            "Step 87/1000, Loss: 7.9430, Time: 15.09 ms\n",
            "Step 88/1000, Loss: 7.8138, Time: 14.37 ms\n",
            "Step 89/1000, Loss: 7.8197, Time: 19.15 ms\n",
            "Step 90/1000, Loss: 7.8264, Time: 13.76 ms\n",
            "Step 91/1000, Loss: 7.7564, Time: 13.82 ms\n",
            "Step 92/1000, Loss: 7.7545, Time: 15.89 ms\n",
            "Step 93/1000, Loss: 7.6724, Time: 14.51 ms\n",
            "Step 94/1000, Loss: 7.7061, Time: 22.60 ms\n",
            "Step 95/1000, Loss: 7.7832, Time: 25.43 ms\n",
            "Step 96/1000, Loss: 7.8142, Time: 22.91 ms\n",
            "Step 97/1000, Loss: 7.6403, Time: 22.84 ms\n",
            "Step 98/1000, Loss: 7.5810, Time: 19.45 ms\n",
            "Step 99/1000, Loss: 7.7136, Time: 18.70 ms\n",
            "Step 100/1000, Loss: 7.6481, Time: 19.46 ms\n",
            "Step 101/1000, Loss: 7.5018, Time: 18.45 ms\n",
            "Step 102/1000, Loss: 7.5561, Time: 18.23 ms\n",
            "Step 103/1000, Loss: 7.5798, Time: 18.53 ms\n",
            "Step 104/1000, Loss: 7.4889, Time: 19.23 ms\n",
            "Step 105/1000, Loss: 7.3844, Time: 22.72 ms\n",
            "Step 106/1000, Loss: 7.4460, Time: 21.10 ms\n",
            "Step 107/1000, Loss: 7.1405, Time: 20.63 ms\n",
            "Step 108/1000, Loss: 7.2197, Time: 19.01 ms\n",
            "Step 109/1000, Loss: 7.2302, Time: 18.03 ms\n",
            "Step 110/1000, Loss: 7.1781, Time: 18.75 ms\n",
            "Step 111/1000, Loss: 7.3354, Time: 17.66 ms\n",
            "Step 112/1000, Loss: 7.4229, Time: 21.88 ms\n",
            "Step 113/1000, Loss: 7.3240, Time: 22.25 ms\n",
            "Step 114/1000, Loss: 7.1731, Time: 23.01 ms\n",
            "Step 115/1000, Loss: 7.2390, Time: 18.27 ms\n",
            "Step 116/1000, Loss: 7.0845, Time: 19.38 ms\n",
            "Step 117/1000, Loss: 6.9477, Time: 19.06 ms\n",
            "Step 118/1000, Loss: 6.9109, Time: 22.57 ms\n",
            "Step 119/1000, Loss: 7.1320, Time: 18.79 ms\n",
            "Step 120/1000, Loss: 6.9109, Time: 22.75 ms\n",
            "Step 121/1000, Loss: 6.8558, Time: 21.89 ms\n",
            "Step 122/1000, Loss: 6.8151, Time: 22.34 ms\n",
            "Step 123/1000, Loss: 6.8843, Time: 24.27 ms\n",
            "Step 124/1000, Loss: 6.9427, Time: 28.67 ms\n",
            "Step 125/1000, Loss: 6.5694, Time: 22.74 ms\n",
            "Step 126/1000, Loss: 6.8431, Time: 22.57 ms\n",
            "Step 127/1000, Loss: 6.7627, Time: 38.79 ms\n",
            "Step 128/1000, Loss: 6.8559, Time: 19.45 ms\n",
            "Step 129/1000, Loss: 6.8206, Time: 14.87 ms\n",
            "Step 130/1000, Loss: 6.6625, Time: 15.77 ms\n",
            "Step 131/1000, Loss: 6.8534, Time: 14.48 ms\n",
            "Step 132/1000, Loss: 6.5561, Time: 14.67 ms\n",
            "Step 133/1000, Loss: 6.8157, Time: 14.47 ms\n",
            "Step 134/1000, Loss: 6.7688, Time: 16.36 ms\n",
            "Step 135/1000, Loss: 6.7683, Time: 20.78 ms\n",
            "Step 136/1000, Loss: 6.7269, Time: 15.81 ms\n",
            "Step 137/1000, Loss: 7.1267, Time: 14.98 ms\n",
            "Step 138/1000, Loss: 6.5842, Time: 14.83 ms\n",
            "Step 139/1000, Loss: 6.4463, Time: 16.39 ms\n",
            "Step 140/1000, Loss: 6.8053, Time: 15.27 ms\n",
            "Step 141/1000, Loss: 6.4643, Time: 18.79 ms\n",
            "Step 142/1000, Loss: 6.3737, Time: 16.71 ms\n",
            "Step 143/1000, Loss: 6.9729, Time: 15.66 ms\n",
            "Step 144/1000, Loss: 6.8549, Time: 14.74 ms\n",
            "Step 145/1000, Loss: 6.4724, Time: 14.70 ms\n",
            "Step 146/1000, Loss: 6.4432, Time: 14.53 ms\n",
            "Step 147/1000, Loss: 6.4099, Time: 15.35 ms\n",
            "Step 148/1000, Loss: 6.6564, Time: 15.29 ms\n",
            "Step 149/1000, Loss: 6.5650, Time: 14.60 ms\n",
            "Step 150/1000, Loss: 6.5363, Time: 14.44 ms\n",
            "Step 151/1000, Loss: 6.7973, Time: 14.17 ms\n",
            "Step 152/1000, Loss: 6.5904, Time: 16.31 ms\n",
            "Step 153/1000, Loss: 6.6281, Time: 14.67 ms\n",
            "Step 154/1000, Loss: 6.5480, Time: 14.08 ms\n",
            "Step 155/1000, Loss: 6.4238, Time: 14.16 ms\n",
            "Step 156/1000, Loss: 6.2978, Time: 14.81 ms\n",
            "Step 157/1000, Loss: 6.2632, Time: 14.10 ms\n",
            "Step 158/1000, Loss: 6.3219, Time: 14.49 ms\n",
            "Step 159/1000, Loss: 6.2038, Time: 14.53 ms\n",
            "Step 160/1000, Loss: 6.4567, Time: 14.93 ms\n",
            "Step 161/1000, Loss: 6.4147, Time: 14.22 ms\n",
            "Step 162/1000, Loss: 6.3862, Time: 14.24 ms\n",
            "Step 163/1000, Loss: 6.3551, Time: 13.87 ms\n",
            "Step 164/1000, Loss: 6.5989, Time: 14.46 ms\n",
            "Step 165/1000, Loss: 6.6361, Time: 14.27 ms\n",
            "Step 166/1000, Loss: 6.5715, Time: 14.31 ms\n",
            "Step 167/1000, Loss: 6.4878, Time: 13.96 ms\n",
            "Step 168/1000, Loss: 6.3989, Time: 14.23 ms\n",
            "Step 169/1000, Loss: 6.5039, Time: 14.32 ms\n",
            "Step 170/1000, Loss: 6.2839, Time: 15.06 ms\n",
            "Step 171/1000, Loss: 6.3268, Time: 14.41 ms\n",
            "Step 172/1000, Loss: 6.2533, Time: 15.79 ms\n",
            "Step 173/1000, Loss: 6.3494, Time: 14.17 ms\n",
            "Step 174/1000, Loss: 6.1466, Time: 14.08 ms\n",
            "Step 175/1000, Loss: 6.2229, Time: 13.99 ms\n",
            "Step 176/1000, Loss: 6.6428, Time: 14.67 ms\n",
            "Step 177/1000, Loss: 6.4402, Time: 15.34 ms\n",
            "Step 178/1000, Loss: 6.4815, Time: 14.26 ms\n",
            "Step 179/1000, Loss: 6.5821, Time: 14.05 ms\n",
            "Step 180/1000, Loss: 6.4856, Time: 14.86 ms\n",
            "Step 181/1000, Loss: 6.4790, Time: 14.02 ms\n",
            "Step 182/1000, Loss: 6.1689, Time: 14.53 ms\n",
            "Step 183/1000, Loss: 6.4692, Time: 13.81 ms\n",
            "Step 184/1000, Loss: 6.4826, Time: 16.37 ms\n",
            "Step 185/1000, Loss: 6.3257, Time: 14.43 ms\n",
            "Step 186/1000, Loss: 6.4924, Time: 14.08 ms\n",
            "Step 187/1000, Loss: 6.2532, Time: 13.99 ms\n",
            "Step 188/1000, Loss: 5.9515, Time: 15.56 ms\n",
            "Step 189/1000, Loss: 5.9971, Time: 13.91 ms\n",
            "Step 190/1000, Loss: 6.1313, Time: 15.86 ms\n",
            "Step 191/1000, Loss: 6.5559, Time: 14.01 ms\n",
            "Step 192/1000, Loss: 6.6084, Time: 15.00 ms\n",
            "Step 193/1000, Loss: 6.3862, Time: 13.87 ms\n",
            "Step 194/1000, Loss: 6.2163, Time: 14.20 ms\n",
            "Step 195/1000, Loss: 6.3092, Time: 14.02 ms\n",
            "Step 196/1000, Loss: 6.3319, Time: 15.76 ms\n",
            "Step 197/1000, Loss: 6.1137, Time: 14.24 ms\n",
            "Step 198/1000, Loss: 6.2972, Time: 14.35 ms\n",
            "Step 199/1000, Loss: 6.2505, Time: 14.09 ms\n",
            "Step 200/1000, Loss: 6.1697, Time: 14.68 ms\n",
            "Step 201/1000, Loss: 6.4417, Time: 15.83 ms\n",
            "Step 202/1000, Loss: 6.1863, Time: 16.93 ms\n",
            "Step 203/1000, Loss: 6.2709, Time: 13.97 ms\n",
            "Step 204/1000, Loss: 6.4378, Time: 15.18 ms\n",
            "Step 205/1000, Loss: 6.2309, Time: 14.39 ms\n",
            "Step 206/1000, Loss: 5.8969, Time: 16.44 ms\n",
            "Step 207/1000, Loss: 6.3553, Time: 22.96 ms\n",
            "Step 208/1000, Loss: 6.5226, Time: 27.98 ms\n",
            "Step 209/1000, Loss: 6.6154, Time: 17.66 ms\n",
            "Step 210/1000, Loss: 6.4933, Time: 14.30 ms\n",
            "Step 211/1000, Loss: 6.3991, Time: 14.12 ms\n",
            "Step 212/1000, Loss: 6.6340, Time: 21.11 ms\n",
            "Step 213/1000, Loss: 6.6753, Time: 14.22 ms\n",
            "Step 214/1000, Loss: 6.4209, Time: 13.81 ms\n",
            "Step 215/1000, Loss: 6.3707, Time: 14.29 ms\n",
            "Step 216/1000, Loss: 6.4698, Time: 17.13 ms\n",
            "Step 217/1000, Loss: 6.3518, Time: 13.92 ms\n",
            "Step 218/1000, Loss: 6.4510, Time: 16.88 ms\n",
            "Step 219/1000, Loss: 6.2142, Time: 14.15 ms\n",
            "Step 220/1000, Loss: 6.3131, Time: 14.65 ms\n",
            "Step 221/1000, Loss: 6.4096, Time: 13.88 ms\n",
            "Step 222/1000, Loss: 6.5126, Time: 14.19 ms\n",
            "Step 223/1000, Loss: 6.2553, Time: 15.46 ms\n",
            "Step 224/1000, Loss: 6.6223, Time: 14.85 ms\n",
            "Step 225/1000, Loss: 6.5323, Time: 16.56 ms\n",
            "Step 226/1000, Loss: 6.6493, Time: 14.38 ms\n",
            "Step 227/1000, Loss: 6.7982, Time: 15.50 ms\n",
            "Step 228/1000, Loss: 6.7800, Time: 14.90 ms\n",
            "Step 229/1000, Loss: 6.4473, Time: 14.25 ms\n",
            "Step 230/1000, Loss: 6.5053, Time: 14.50 ms\n",
            "Step 231/1000, Loss: 6.4652, Time: 14.20 ms\n",
            "Step 232/1000, Loss: 6.8211, Time: 14.73 ms\n",
            "Step 233/1000, Loss: 6.3696, Time: 14.44 ms\n",
            "Step 234/1000, Loss: 6.2460, Time: 29.93 ms\n",
            "Step 235/1000, Loss: 6.3288, Time: 53.00 ms\n",
            "Step 236/1000, Loss: 6.3610, Time: 32.37 ms\n",
            "Step 237/1000, Loss: 6.6546, Time: 38.49 ms\n",
            "Step 238/1000, Loss: 6.4945, Time: 38.14 ms\n",
            "Step 239/1000, Loss: 6.2475, Time: 26.50 ms\n",
            "Step 240/1000, Loss: 6.5351, Time: 19.00 ms\n",
            "Step 241/1000, Loss: 6.1331, Time: 14.71 ms\n",
            "Step 242/1000, Loss: 6.2941, Time: 14.63 ms\n",
            "Step 243/1000, Loss: 6.3181, Time: 14.42 ms\n",
            "Step 244/1000, Loss: 6.7572, Time: 14.69 ms\n",
            "Step 245/1000, Loss: 6.4490, Time: 14.13 ms\n",
            "Step 246/1000, Loss: 6.1704, Time: 19.09 ms\n",
            "Step 247/1000, Loss: 6.2946, Time: 17.23 ms\n",
            "Step 248/1000, Loss: 6.4714, Time: 14.72 ms\n",
            "Step 249/1000, Loss: 6.2714, Time: 21.79 ms\n",
            "Step 250/1000, Loss: 6.4932, Time: 42.83 ms\n",
            "Step 251/1000, Loss: 6.5096, Time: 25.13 ms\n",
            "Step 252/1000, Loss: 6.4214, Time: 36.91 ms\n",
            "Step 253/1000, Loss: 6.3995, Time: 59.60 ms\n",
            "Step 254/1000, Loss: 6.1796, Time: 27.51 ms\n",
            "Step 255/1000, Loss: 6.0817, Time: 18.70 ms\n",
            "Step 256/1000, Loss: 6.4139, Time: 14.88 ms\n",
            "Step 257/1000, Loss: 5.9758, Time: 14.99 ms\n",
            "Step 258/1000, Loss: 5.8814, Time: 14.00 ms\n",
            "Step 259/1000, Loss: 6.4017, Time: 14.28 ms\n",
            "Step 260/1000, Loss: 6.1776, Time: 14.87 ms\n",
            "Step 261/1000, Loss: 6.1366, Time: 14.26 ms\n",
            "Step 262/1000, Loss: 6.2060, Time: 17.05 ms\n",
            "Step 263/1000, Loss: 6.4003, Time: 17.19 ms\n",
            "Step 264/1000, Loss: 6.4902, Time: 16.86 ms\n",
            "Step 265/1000, Loss: 6.6347, Time: 14.05 ms\n",
            "Step 266/1000, Loss: 6.7387, Time: 14.85 ms\n",
            "Step 267/1000, Loss: 6.3756, Time: 14.62 ms\n",
            "Step 268/1000, Loss: 6.4753, Time: 15.21 ms\n",
            "Step 269/1000, Loss: 6.5912, Time: 15.87 ms\n",
            "Step 270/1000, Loss: 6.5597, Time: 14.07 ms\n",
            "Step 271/1000, Loss: 6.2964, Time: 14.37 ms\n",
            "Step 272/1000, Loss: 6.3132, Time: 14.57 ms\n",
            "Step 273/1000, Loss: 6.4359, Time: 17.42 ms\n",
            "Step 274/1000, Loss: 6.4783, Time: 16.29 ms\n",
            "Step 275/1000, Loss: 6.3298, Time: 14.47 ms\n",
            "Step 276/1000, Loss: 6.4888, Time: 14.90 ms\n",
            "Step 277/1000, Loss: 6.2117, Time: 14.31 ms\n",
            "Step 278/1000, Loss: 6.2265, Time: 14.59 ms\n",
            "Step 279/1000, Loss: 6.2329, Time: 14.35 ms\n",
            "Step 280/1000, Loss: 6.2663, Time: 15.69 ms\n",
            "Step 281/1000, Loss: 5.9761, Time: 14.12 ms\n",
            "Step 282/1000, Loss: 6.3893, Time: 14.11 ms\n",
            "Step 283/1000, Loss: 5.9705, Time: 15.77 ms\n",
            "Step 284/1000, Loss: 6.2219, Time: 15.07 ms\n",
            "Step 285/1000, Loss: 6.1574, Time: 14.83 ms\n",
            "Step 286/1000, Loss: 6.7507, Time: 14.65 ms\n",
            "Step 287/1000, Loss: 5.8923, Time: 14.16 ms\n",
            "Step 288/1000, Loss: 6.5006, Time: 14.68 ms\n",
            "Step 289/1000, Loss: 6.4531, Time: 14.62 ms\n",
            "Step 290/1000, Loss: 6.2878, Time: 14.75 ms\n",
            "Step 291/1000, Loss: 6.3316, Time: 15.11 ms\n",
            "Step 292/1000, Loss: 6.5639, Time: 16.23 ms\n",
            "Step 293/1000, Loss: 6.3784, Time: 14.37 ms\n",
            "Step 294/1000, Loss: 5.9987, Time: 22.28 ms\n",
            "Step 295/1000, Loss: 6.2347, Time: 15.36 ms\n",
            "Step 296/1000, Loss: 6.1586, Time: 15.47 ms\n",
            "Step 297/1000, Loss: 6.2455, Time: 14.79 ms\n",
            "Step 298/1000, Loss: 5.9723, Time: 17.31 ms\n",
            "Step 299/1000, Loss: 6.2405, Time: 18.44 ms\n",
            "Step 300/1000, Loss: 6.3603, Time: 14.82 ms\n",
            "Step 301/1000, Loss: 6.5786, Time: 23.61 ms\n",
            "Step 302/1000, Loss: 6.3620, Time: 23.43 ms\n",
            "Step 303/1000, Loss: 6.3576, Time: 20.50 ms\n",
            "Step 304/1000, Loss: 6.1811, Time: 20.67 ms\n",
            "Step 305/1000, Loss: 6.0766, Time: 18.28 ms\n",
            "Step 306/1000, Loss: 6.2401, Time: 18.92 ms\n",
            "Step 307/1000, Loss: 6.1891, Time: 20.25 ms\n",
            "Step 308/1000, Loss: 6.0805, Time: 20.17 ms\n",
            "Step 309/1000, Loss: 5.9743, Time: 19.23 ms\n",
            "Step 310/1000, Loss: 5.9253, Time: 18.60 ms\n",
            "Step 311/1000, Loss: 5.9427, Time: 18.51 ms\n",
            "Step 312/1000, Loss: 5.8916, Time: 19.08 ms\n",
            "Step 313/1000, Loss: 6.0340, Time: 18.16 ms\n",
            "Step 314/1000, Loss: 6.0577, Time: 22.13 ms\n",
            "Step 315/1000, Loss: 6.0299, Time: 21.97 ms\n",
            "Step 316/1000, Loss: 6.1783, Time: 20.45 ms\n",
            "Step 317/1000, Loss: 6.0190, Time: 17.50 ms\n",
            "Step 318/1000, Loss: 5.8827, Time: 18.48 ms\n",
            "Step 319/1000, Loss: 6.0898, Time: 20.44 ms\n",
            "Step 320/1000, Loss: 6.3195, Time: 24.06 ms\n",
            "Step 321/1000, Loss: 6.2776, Time: 22.05 ms\n",
            "Step 322/1000, Loss: 6.4544, Time: 25.71 ms\n",
            "Step 323/1000, Loss: 6.5671, Time: 25.54 ms\n",
            "Step 324/1000, Loss: 6.3723, Time: 21.43 ms\n",
            "Step 325/1000, Loss: 6.5265, Time: 19.50 ms\n",
            "Step 326/1000, Loss: 6.4624, Time: 24.09 ms\n",
            "Step 327/1000, Loss: 6.0985, Time: 20.65 ms\n",
            "Step 328/1000, Loss: 6.1797, Time: 22.96 ms\n",
            "Step 329/1000, Loss: 6.2272, Time: 23.04 ms\n",
            "Step 330/1000, Loss: 6.1904, Time: 21.34 ms\n",
            "Step 331/1000, Loss: 6.2679, Time: 20.93 ms\n",
            "Step 332/1000, Loss: 6.0126, Time: 26.27 ms\n",
            "Step 333/1000, Loss: 6.3382, Time: 25.40 ms\n",
            "Step 334/1000, Loss: 6.0446, Time: 22.87 ms\n",
            "Step 335/1000, Loss: 6.4222, Time: 24.24 ms\n",
            "Step 336/1000, Loss: 6.0749, Time: 23.16 ms\n",
            "Step 337/1000, Loss: 6.1335, Time: 14.71 ms\n",
            "Step 338/1000, Loss: 5.9531, Time: 16.94 ms\n",
            "Step 339/1000, Loss: 6.2082, Time: 16.52 ms\n",
            "Step 340/1000, Loss: 6.0553, Time: 15.31 ms\n",
            "Step 341/1000, Loss: 5.9599, Time: 14.60 ms\n",
            "Step 342/1000, Loss: 6.3902, Time: 14.16 ms\n",
            "Step 343/1000, Loss: 6.0960, Time: 14.20 ms\n",
            "Step 344/1000, Loss: 6.3688, Time: 14.86 ms\n",
            "Step 345/1000, Loss: 6.2384, Time: 14.56 ms\n",
            "Step 346/1000, Loss: 6.0762, Time: 14.06 ms\n",
            "Step 347/1000, Loss: 5.9549, Time: 14.48 ms\n",
            "Step 348/1000, Loss: 5.9117, Time: 14.85 ms\n",
            "Step 349/1000, Loss: 5.8552, Time: 16.90 ms\n",
            "Step 350/1000, Loss: 5.9177, Time: 14.03 ms\n",
            "Step 351/1000, Loss: 5.6770, Time: 14.20 ms\n",
            "Step 352/1000, Loss: 6.2763, Time: 14.89 ms\n",
            "Step 353/1000, Loss: 5.7129, Time: 14.55 ms\n",
            "Step 354/1000, Loss: 5.5266, Time: 13.95 ms\n",
            "Step 355/1000, Loss: 5.9131, Time: 14.49 ms\n",
            "Step 356/1000, Loss: 5.7744, Time: 14.93 ms\n",
            "Step 357/1000, Loss: 5.8456, Time: 14.59 ms\n",
            "Step 358/1000, Loss: 5.6443, Time: 14.02 ms\n",
            "Step 359/1000, Loss: 5.8776, Time: 14.13 ms\n",
            "Step 360/1000, Loss: 5.8418, Time: 14.51 ms\n",
            "Step 361/1000, Loss: 5.8603, Time: 14.73 ms\n",
            "Step 362/1000, Loss: 5.5164, Time: 17.19 ms\n",
            "Step 363/1000, Loss: 6.0592, Time: 14.06 ms\n",
            "Step 364/1000, Loss: 5.9662, Time: 15.20 ms\n",
            "Step 365/1000, Loss: 5.9549, Time: 14.31 ms\n",
            "Step 366/1000, Loss: 5.6631, Time: 13.96 ms\n",
            "Step 367/1000, Loss: 5.7614, Time: 14.92 ms\n",
            "Step 368/1000, Loss: 6.0168, Time: 17.29 ms\n",
            "Step 369/1000, Loss: 5.4677, Time: 14.85 ms\n",
            "Step 370/1000, Loss: 5.9290, Time: 15.54 ms\n",
            "Step 371/1000, Loss: 6.0301, Time: 14.05 ms\n",
            "Step 372/1000, Loss: 6.0917, Time: 20.93 ms\n",
            "Step 373/1000, Loss: 5.9522, Time: 16.13 ms\n",
            "Step 374/1000, Loss: 5.8682, Time: 15.98 ms\n",
            "Step 375/1000, Loss: 5.9795, Time: 18.19 ms\n",
            "Step 376/1000, Loss: 5.9506, Time: 17.07 ms\n",
            "Step 377/1000, Loss: 6.2373, Time: 16.37 ms\n",
            "Step 378/1000, Loss: 5.6942, Time: 15.01 ms\n",
            "Step 379/1000, Loss: 6.2899, Time: 14.15 ms\n",
            "Step 380/1000, Loss: 5.8557, Time: 15.01 ms\n",
            "Step 381/1000, Loss: 5.7946, Time: 14.22 ms\n",
            "Step 382/1000, Loss: 5.8437, Time: 14.15 ms\n",
            "Step 383/1000, Loss: 6.1077, Time: 14.06 ms\n",
            "Step 384/1000, Loss: 5.8073, Time: 17.15 ms\n",
            "Step 385/1000, Loss: 6.1275, Time: 14.90 ms\n",
            "Step 386/1000, Loss: 5.7692, Time: 14.00 ms\n",
            "Step 387/1000, Loss: 5.8651, Time: 14.63 ms\n",
            "Step 388/1000, Loss: 6.1896, Time: 15.31 ms\n",
            "Step 389/1000, Loss: 5.7216, Time: 15.40 ms\n",
            "Step 390/1000, Loss: 5.6165, Time: 21.70 ms\n",
            "Step 391/1000, Loss: 5.7393, Time: 14.35 ms\n",
            "Step 392/1000, Loss: 5.9150, Time: 15.31 ms\n",
            "Step 393/1000, Loss: 5.8444, Time: 14.62 ms\n",
            "Step 394/1000, Loss: 5.8503, Time: 14.01 ms\n",
            "Step 395/1000, Loss: 5.6885, Time: 14.17 ms\n",
            "Step 396/1000, Loss: 5.8494, Time: 14.64 ms\n",
            "Step 397/1000, Loss: 5.8802, Time: 14.97 ms\n",
            "Step 398/1000, Loss: 5.3084, Time: 16.92 ms\n",
            "Step 399/1000, Loss: 5.7295, Time: 14.68 ms\n",
            "Step 400/1000, Loss: 5.7463, Time: 14.89 ms\n",
            "Step 401/1000, Loss: 5.5627, Time: 18.59 ms\n",
            "Step 402/1000, Loss: 5.5665, Time: 14.24 ms\n",
            "Step 403/1000, Loss: 5.9099, Time: 13.96 ms\n",
            "Step 404/1000, Loss: 6.0542, Time: 14.61 ms\n",
            "Step 405/1000, Loss: 5.8914, Time: 17.23 ms\n",
            "Step 406/1000, Loss: 5.8457, Time: 14.03 ms\n",
            "Step 407/1000, Loss: 5.8439, Time: 15.66 ms\n",
            "Step 408/1000, Loss: 5.8695, Time: 14.85 ms\n",
            "Step 409/1000, Loss: 5.9526, Time: 14.88 ms\n",
            "Step 410/1000, Loss: 5.5953, Time: 14.17 ms\n",
            "Step 411/1000, Loss: 6.2726, Time: 13.86 ms\n",
            "Step 412/1000, Loss: 6.1339, Time: 14.47 ms\n",
            "Step 413/1000, Loss: 5.9563, Time: 14.69 ms\n",
            "Step 414/1000, Loss: 5.8771, Time: 15.04 ms\n",
            "Step 415/1000, Loss: 5.8122, Time: 14.37 ms\n",
            "Step 416/1000, Loss: 6.0517, Time: 15.30 ms\n",
            "Step 417/1000, Loss: 5.9578, Time: 13.70 ms\n",
            "Step 418/1000, Loss: 5.5725, Time: 19.52 ms\n",
            "Step 419/1000, Loss: 6.2366, Time: 14.05 ms\n",
            "Step 420/1000, Loss: 6.1492, Time: 15.28 ms\n",
            "Step 421/1000, Loss: 6.0254, Time: 14.71 ms\n",
            "Step 422/1000, Loss: 5.8793, Time: 14.31 ms\n",
            "Step 423/1000, Loss: 6.0085, Time: 14.62 ms\n",
            "Step 424/1000, Loss: 6.2223, Time: 14.67 ms\n",
            "Step 425/1000, Loss: 6.1939, Time: 19.38 ms\n",
            "Step 426/1000, Loss: 6.0897, Time: 14.10 ms\n",
            "Step 427/1000, Loss: 6.2217, Time: 13.85 ms\n",
            "Step 428/1000, Loss: 6.1926, Time: 14.27 ms\n",
            "Step 429/1000, Loss: 6.4037, Time: 18.10 ms\n",
            "Step 430/1000, Loss: 5.9859, Time: 14.03 ms\n",
            "Step 431/1000, Loss: 6.1347, Time: 14.47 ms\n",
            "Step 432/1000, Loss: 6.2261, Time: 14.44 ms\n",
            "Step 433/1000, Loss: 6.3629, Time: 14.56 ms\n",
            "Step 434/1000, Loss: 5.9306, Time: 14.85 ms\n",
            "Step 435/1000, Loss: 6.0514, Time: 13.80 ms\n",
            "Step 436/1000, Loss: 6.3005, Time: 16.94 ms\n",
            "Step 437/1000, Loss: 5.8460, Time: 15.72 ms\n",
            "Step 438/1000, Loss: 5.7157, Time: 13.85 ms\n",
            "Step 439/1000, Loss: 5.9618, Time: 14.34 ms\n",
            "Step 440/1000, Loss: 5.9986, Time: 14.75 ms\n",
            "Step 441/1000, Loss: 6.1512, Time: 16.71 ms\n",
            "Step 442/1000, Loss: 6.1648, Time: 15.76 ms\n",
            "Step 443/1000, Loss: 6.3128, Time: 18.92 ms\n",
            "Step 444/1000, Loss: 6.0856, Time: 15.42 ms\n",
            "Step 445/1000, Loss: 6.1891, Time: 14.76 ms\n",
            "Step 446/1000, Loss: 5.7935, Time: 13.98 ms\n",
            "Step 447/1000, Loss: 6.0321, Time: 14.11 ms\n",
            "Step 448/1000, Loss: 5.9074, Time: 14.78 ms\n",
            "Step 449/1000, Loss: 6.1741, Time: 14.30 ms\n",
            "Step 450/1000, Loss: 5.7441, Time: 14.04 ms\n",
            "Step 451/1000, Loss: 5.9410, Time: 14.71 ms\n",
            "Step 452/1000, Loss: 5.9066, Time: 14.69 ms\n",
            "Step 453/1000, Loss: 5.9410, Time: 14.56 ms\n",
            "Step 454/1000, Loss: 5.6924, Time: 15.24 ms\n",
            "Step 455/1000, Loss: 5.5205, Time: 51.20 ms\n",
            "Step 456/1000, Loss: 5.8707, Time: 34.52 ms\n",
            "Step 457/1000, Loss: 5.8159, Time: 35.97 ms\n",
            "Step 458/1000, Loss: 5.8615, Time: 36.43 ms\n",
            "Step 459/1000, Loss: 6.0609, Time: 38.15 ms\n",
            "Step 460/1000, Loss: 5.9315, Time: 15.52 ms\n",
            "Step 461/1000, Loss: 6.1516, Time: 14.46 ms\n",
            "Step 462/1000, Loss: 5.4740, Time: 14.26 ms\n",
            "Step 463/1000, Loss: 6.1475, Time: 14.50 ms\n",
            "Step 464/1000, Loss: 5.8957, Time: 15.11 ms\n",
            "Step 465/1000, Loss: 5.9743, Time: 17.23 ms\n",
            "Step 466/1000, Loss: 5.6117, Time: 15.45 ms\n",
            "Step 467/1000, Loss: 6.5498, Time: 14.90 ms\n",
            "Step 468/1000, Loss: 5.8372, Time: 14.70 ms\n",
            "Step 469/1000, Loss: 5.5157, Time: 13.72 ms\n",
            "Step 470/1000, Loss: 5.8477, Time: 14.60 ms\n",
            "Step 471/1000, Loss: 5.7153, Time: 14.02 ms\n",
            "Step 472/1000, Loss: 5.5601, Time: 14.97 ms\n",
            "Step 473/1000, Loss: 6.3496, Time: 14.12 ms\n",
            "Step 474/1000, Loss: 5.8841, Time: 14.35 ms\n",
            "Step 475/1000, Loss: 5.7407, Time: 14.32 ms\n",
            "Step 476/1000, Loss: 5.7203, Time: 14.79 ms\n",
            "Step 477/1000, Loss: 5.6908, Time: 14.37 ms\n",
            "Step 478/1000, Loss: 5.6910, Time: 14.45 ms\n",
            "Step 479/1000, Loss: 6.0362, Time: 18.41 ms\n",
            "Step 480/1000, Loss: 5.8967, Time: 14.92 ms\n",
            "Step 481/1000, Loss: 6.2554, Time: 14.06 ms\n",
            "Step 482/1000, Loss: 5.7673, Time: 14.17 ms\n",
            "Step 483/1000, Loss: 5.9934, Time: 14.02 ms\n",
            "Step 484/1000, Loss: 5.8647, Time: 15.92 ms\n",
            "Step 485/1000, Loss: 5.7877, Time: 14.00 ms\n",
            "Step 486/1000, Loss: 5.4832, Time: 15.15 ms\n",
            "Step 487/1000, Loss: 5.5428, Time: 15.30 ms\n",
            "Step 488/1000, Loss: 5.7708, Time: 15.40 ms\n",
            "Step 489/1000, Loss: 5.4611, Time: 17.78 ms\n",
            "Step 490/1000, Loss: 5.6377, Time: 14.36 ms\n",
            "Step 491/1000, Loss: 5.7855, Time: 14.90 ms\n",
            "Step 492/1000, Loss: 5.7713, Time: 15.21 ms\n",
            "Step 493/1000, Loss: 5.7141, Time: 13.96 ms\n",
            "Step 494/1000, Loss: 5.7321, Time: 21.58 ms\n",
            "Step 495/1000, Loss: 6.1326, Time: 15.08 ms\n",
            "Step 496/1000, Loss: 6.0059, Time: 15.22 ms\n",
            "Step 497/1000, Loss: 5.9252, Time: 14.75 ms\n",
            "Step 498/1000, Loss: 5.5449, Time: 16.04 ms\n",
            "Step 499/1000, Loss: 5.9148, Time: 15.23 ms\n",
            "Step 500/1000, Loss: 5.7380, Time: 15.50 ms\n",
            "Step 501/1000, Loss: 5.6424, Time: 15.58 ms\n",
            "Step 502/1000, Loss: 5.4109, Time: 14.53 ms\n",
            "Step 503/1000, Loss: 5.7113, Time: 14.71 ms\n",
            "Step 504/1000, Loss: 5.5234, Time: 15.47 ms\n",
            "Step 505/1000, Loss: 5.6359, Time: 15.78 ms\n",
            "Step 506/1000, Loss: 5.8982, Time: 14.62 ms\n",
            "Step 507/1000, Loss: 5.9811, Time: 24.92 ms\n",
            "Step 508/1000, Loss: 5.9652, Time: 24.45 ms\n",
            "Step 509/1000, Loss: 6.0879, Time: 19.52 ms\n",
            "Step 510/1000, Loss: 5.7969, Time: 19.49 ms\n",
            "Step 511/1000, Loss: 5.9152, Time: 21.72 ms\n",
            "Step 512/1000, Loss: 5.5980, Time: 21.53 ms\n",
            "Step 513/1000, Loss: 5.9168, Time: 17.89 ms\n",
            "Step 514/1000, Loss: 5.8211, Time: 19.53 ms\n",
            "Step 515/1000, Loss: 5.8019, Time: 18.60 ms\n",
            "Step 516/1000, Loss: 6.0190, Time: 19.86 ms\n",
            "Step 517/1000, Loss: 5.8073, Time: 18.71 ms\n",
            "Step 518/1000, Loss: 5.2308, Time: 19.11 ms\n",
            "Step 519/1000, Loss: 5.1247, Time: 26.95 ms\n",
            "Step 520/1000, Loss: 5.5813, Time: 20.63 ms\n",
            "Step 521/1000, Loss: 6.0323, Time: 17.92 ms\n",
            "Step 522/1000, Loss: 5.7861, Time: 17.69 ms\n",
            "Step 523/1000, Loss: 5.8587, Time: 18.10 ms\n",
            "Step 524/1000, Loss: 5.6096, Time: 18.37 ms\n",
            "Step 525/1000, Loss: 5.7005, Time: 21.87 ms\n",
            "Step 526/1000, Loss: 5.6086, Time: 22.62 ms\n",
            "Step 527/1000, Loss: 5.5560, Time: 29.28 ms\n",
            "Step 528/1000, Loss: 5.7956, Time: 20.04 ms\n",
            "Step 529/1000, Loss: 5.7602, Time: 20.93 ms\n",
            "Step 530/1000, Loss: 5.4557, Time: 23.32 ms\n",
            "Step 531/1000, Loss: 5.9579, Time: 26.53 ms\n",
            "Step 532/1000, Loss: 5.6243, Time: 20.05 ms\n",
            "Step 533/1000, Loss: 5.7701, Time: 23.51 ms\n",
            "Step 534/1000, Loss: 5.8271, Time: 26.19 ms\n",
            "Step 535/1000, Loss: 5.7076, Time: 22.19 ms\n",
            "Step 536/1000, Loss: 5.3260, Time: 23.65 ms\n",
            "Step 537/1000, Loss: 5.8593, Time: 23.06 ms\n",
            "Step 538/1000, Loss: 5.8139, Time: 22.35 ms\n",
            "Step 539/1000, Loss: 6.1374, Time: 26.24 ms\n",
            "Step 540/1000, Loss: 5.9921, Time: 25.72 ms\n",
            "Step 541/1000, Loss: 6.0004, Time: 19.15 ms\n",
            "Step 542/1000, Loss: 5.8488, Time: 16.57 ms\n",
            "Step 543/1000, Loss: 6.1943, Time: 14.55 ms\n",
            "Step 544/1000, Loss: 5.8536, Time: 15.08 ms\n",
            "Step 545/1000, Loss: 5.8360, Time: 14.70 ms\n",
            "Step 546/1000, Loss: 5.7779, Time: 14.42 ms\n",
            "Step 547/1000, Loss: 5.8400, Time: 13.90 ms\n",
            "Step 548/1000, Loss: 5.9540, Time: 15.63 ms\n",
            "Step 549/1000, Loss: 5.6921, Time: 14.28 ms\n",
            "Step 550/1000, Loss: 5.6115, Time: 14.40 ms\n",
            "Step 551/1000, Loss: 5.8763, Time: 15.16 ms\n",
            "Step 552/1000, Loss: 6.0835, Time: 16.51 ms\n",
            "Step 553/1000, Loss: 5.8039, Time: 18.00 ms\n",
            "Step 554/1000, Loss: 5.9406, Time: 14.20 ms\n",
            "Step 555/1000, Loss: 6.0971, Time: 14.08 ms\n",
            "Step 556/1000, Loss: 6.3164, Time: 15.80 ms\n",
            "Step 557/1000, Loss: 6.3816, Time: 17.56 ms\n",
            "Step 558/1000, Loss: 6.1014, Time: 14.72 ms\n",
            "Step 559/1000, Loss: 5.9548, Time: 16.14 ms\n",
            "Step 560/1000, Loss: 6.1072, Time: 15.72 ms\n",
            "Step 561/1000, Loss: 6.0213, Time: 14.28 ms\n",
            "Step 562/1000, Loss: 6.1677, Time: 14.16 ms\n",
            "Step 563/1000, Loss: 5.8506, Time: 14.84 ms\n",
            "Step 564/1000, Loss: 5.8234, Time: 18.13 ms\n",
            "Step 565/1000, Loss: 5.8240, Time: 14.71 ms\n",
            "Step 566/1000, Loss: 5.6624, Time: 13.98 ms\n",
            "Step 567/1000, Loss: 6.2275, Time: 14.25 ms\n",
            "Step 568/1000, Loss: 5.9426, Time: 16.15 ms\n",
            "Step 569/1000, Loss: 5.7596, Time: 15.15 ms\n",
            "Step 570/1000, Loss: 5.9304, Time: 24.63 ms\n",
            "Step 571/1000, Loss: 5.7110, Time: 37.08 ms\n",
            "Step 572/1000, Loss: 5.8751, Time: 32.54 ms\n",
            "Step 573/1000, Loss: 5.9031, Time: 21.51 ms\n",
            "Step 574/1000, Loss: 6.2001, Time: 17.38 ms\n",
            "Step 575/1000, Loss: 6.1410, Time: 16.22 ms\n",
            "Step 576/1000, Loss: 5.7114, Time: 17.34 ms\n",
            "Step 577/1000, Loss: 5.7952, Time: 14.41 ms\n",
            "Step 578/1000, Loss: 5.7759, Time: 14.39 ms\n",
            "Step 579/1000, Loss: 5.7106, Time: 14.18 ms\n",
            "Step 580/1000, Loss: 5.9334, Time: 15.09 ms\n",
            "Step 581/1000, Loss: 5.9776, Time: 19.00 ms\n",
            "Step 582/1000, Loss: 5.7347, Time: 14.56 ms\n",
            "Step 583/1000, Loss: 5.9507, Time: 14.10 ms\n",
            "Step 584/1000, Loss: 5.6733, Time: 17.72 ms\n",
            "Step 585/1000, Loss: 5.5065, Time: 14.59 ms\n",
            "Step 586/1000, Loss: 5.7247, Time: 15.14 ms\n",
            "Step 587/1000, Loss: 5.5278, Time: 15.87 ms\n",
            "Step 588/1000, Loss: 5.3895, Time: 15.47 ms\n",
            "Step 589/1000, Loss: 5.9291, Time: 14.00 ms\n",
            "Step 590/1000, Loss: 5.4685, Time: 15.01 ms\n",
            "Step 591/1000, Loss: 5.6894, Time: 14.67 ms\n",
            "Step 592/1000, Loss: 5.7816, Time: 15.06 ms\n",
            "Step 593/1000, Loss: 5.9015, Time: 14.29 ms\n",
            "Step 594/1000, Loss: 5.7974, Time: 14.51 ms\n",
            "Step 595/1000, Loss: 6.1536, Time: 13.84 ms\n",
            "Step 596/1000, Loss: 6.2887, Time: 14.80 ms\n",
            "Step 597/1000, Loss: 5.7405, Time: 14.13 ms\n",
            "Step 598/1000, Loss: 5.7190, Time: 14.55 ms\n",
            "Step 599/1000, Loss: 6.1316, Time: 14.50 ms\n",
            "Step 600/1000, Loss: 6.1327, Time: 15.38 ms\n",
            "Step 601/1000, Loss: 5.7626, Time: 14.25 ms\n",
            "Step 602/1000, Loss: 5.6294, Time: 14.47 ms\n",
            "Step 603/1000, Loss: 5.9795, Time: 14.90 ms\n",
            "Step 604/1000, Loss: 6.0276, Time: 20.41 ms\n",
            "Step 605/1000, Loss: 5.7135, Time: 14.64 ms\n",
            "Step 606/1000, Loss: 5.7579, Time: 14.18 ms\n",
            "Step 607/1000, Loss: 5.7251, Time: 14.20 ms\n",
            "Step 608/1000, Loss: 5.6847, Time: 14.93 ms\n",
            "Step 609/1000, Loss: 5.6071, Time: 14.00 ms\n",
            "Step 610/1000, Loss: 5.5838, Time: 14.76 ms\n",
            "Step 611/1000, Loss: 5.4631, Time: 14.52 ms\n",
            "Step 612/1000, Loss: 5.8590, Time: 15.16 ms\n",
            "Step 613/1000, Loss: 5.4391, Time: 14.47 ms\n",
            "Step 614/1000, Loss: 5.6462, Time: 14.33 ms\n",
            "Step 615/1000, Loss: 5.8572, Time: 14.50 ms\n",
            "Step 616/1000, Loss: 6.4834, Time: 17.56 ms\n",
            "Step 617/1000, Loss: 5.5630, Time: 14.27 ms\n",
            "Step 618/1000, Loss: 5.8095, Time: 16.20 ms\n",
            "Step 619/1000, Loss: 5.9564, Time: 14.22 ms\n",
            "Step 620/1000, Loss: 5.7547, Time: 16.44 ms\n",
            "Step 621/1000, Loss: 5.7362, Time: 16.05 ms\n",
            "Step 622/1000, Loss: 5.9236, Time: 14.90 ms\n",
            "Step 623/1000, Loss: 5.8174, Time: 14.24 ms\n",
            "Step 624/1000, Loss: 5.4079, Time: 15.66 ms\n",
            "Step 625/1000, Loss: 5.7124, Time: 18.83 ms\n",
            "Step 626/1000, Loss: 5.5231, Time: 14.36 ms\n",
            "Step 627/1000, Loss: 5.8187, Time: 14.32 ms\n",
            "Step 628/1000, Loss: 5.3900, Time: 19.08 ms\n",
            "Step 629/1000, Loss: 5.7022, Time: 14.64 ms\n",
            "Step 630/1000, Loss: 5.7052, Time: 13.88 ms\n",
            "Step 631/1000, Loss: 6.1524, Time: 14.29 ms\n",
            "Step 632/1000, Loss: 5.8755, Time: 15.37 ms\n",
            "Step 633/1000, Loss: 5.8966, Time: 21.10 ms\n",
            "Step 634/1000, Loss: 5.5695, Time: 13.96 ms\n",
            "Step 635/1000, Loss: 5.5838, Time: 14.15 ms\n",
            "Step 636/1000, Loss: 5.8649, Time: 14.80 ms\n",
            "Step 637/1000, Loss: 5.6992, Time: 15.13 ms\n",
            "Step 638/1000, Loss: 5.5114, Time: 14.42 ms\n",
            "Step 639/1000, Loss: 5.4811, Time: 16.78 ms\n",
            "Step 640/1000, Loss: 5.4679, Time: 15.07 ms\n",
            "Step 641/1000, Loss: 5.5153, Time: 14.60 ms\n",
            "Step 642/1000, Loss: 5.3073, Time: 14.33 ms\n",
            "Step 643/1000, Loss: 5.5374, Time: 15.75 ms\n",
            "Step 644/1000, Loss: 5.5905, Time: 15.45 ms\n",
            "Step 645/1000, Loss: 5.4922, Time: 14.79 ms\n",
            "Step 646/1000, Loss: 5.4402, Time: 14.18 ms\n",
            "Step 647/1000, Loss: 5.4787, Time: 14.64 ms\n",
            "Step 648/1000, Loss: 5.3033, Time: 15.20 ms\n",
            "Step 649/1000, Loss: 5.7164, Time: 14.61 ms\n",
            "Step 650/1000, Loss: 5.7319, Time: 18.04 ms\n",
            "Step 651/1000, Loss: 5.9077, Time: 13.96 ms\n",
            "Step 652/1000, Loss: 6.1235, Time: 14.75 ms\n",
            "Step 653/1000, Loss: 6.1941, Time: 14.07 ms\n",
            "Step 654/1000, Loss: 5.7543, Time: 16.47 ms\n",
            "Step 655/1000, Loss: 6.1759, Time: 14.24 ms\n",
            "Step 656/1000, Loss: 6.1234, Time: 14.78 ms\n",
            "Step 657/1000, Loss: 5.7512, Time: 14.19 ms\n",
            "Step 658/1000, Loss: 5.4868, Time: 14.24 ms\n",
            "Step 659/1000, Loss: 5.8018, Time: 14.21 ms\n",
            "Step 660/1000, Loss: 5.7531, Time: 14.67 ms\n",
            "Step 661/1000, Loss: 5.9587, Time: 13.91 ms\n",
            "Step 662/1000, Loss: 5.4895, Time: 15.69 ms\n",
            "Step 663/1000, Loss: 6.0267, Time: 15.27 ms\n",
            "Step 664/1000, Loss: 5.6929, Time: 16.41 ms\n",
            "Step 665/1000, Loss: 6.0437, Time: 14.24 ms\n",
            "Step 666/1000, Loss: 5.4611, Time: 14.46 ms\n",
            "Step 667/1000, Loss: 5.8266, Time: 14.16 ms\n",
            "Step 668/1000, Loss: 5.6231, Time: 16.40 ms\n",
            "Step 669/1000, Loss: 5.8633, Time: 16.88 ms\n",
            "Step 670/1000, Loss: 5.5206, Time: 14.45 ms\n",
            "Step 671/1000, Loss: 5.6405, Time: 14.23 ms\n",
            "Step 672/1000, Loss: 6.2225, Time: 15.18 ms\n",
            "Step 673/1000, Loss: 5.6882, Time: 14.20 ms\n",
            "Step 674/1000, Loss: 5.7863, Time: 16.92 ms\n",
            "Step 675/1000, Loss: 5.9499, Time: 14.54 ms\n",
            "Step 676/1000, Loss: 5.8047, Time: 15.25 ms\n",
            "Step 677/1000, Loss: 5.6355, Time: 14.37 ms\n",
            "Step 678/1000, Loss: 5.3752, Time: 15.06 ms\n",
            "Step 679/1000, Loss: 5.4659, Time: 15.09 ms\n",
            "Step 680/1000, Loss: 5.6401, Time: 16.40 ms\n",
            "Step 681/1000, Loss: 5.2515, Time: 15.61 ms\n",
            "Step 682/1000, Loss: 5.7831, Time: 14.84 ms\n",
            "Step 683/1000, Loss: 5.4005, Time: 14.89 ms\n",
            "Step 684/1000, Loss: 5.1532, Time: 15.66 ms\n",
            "Step 685/1000, Loss: 5.5670, Time: 17.70 ms\n",
            "Step 686/1000, Loss: 5.2831, Time: 14.81 ms\n",
            "Step 687/1000, Loss: 5.5334, Time: 14.80 ms\n",
            "Step 688/1000, Loss: 5.3235, Time: 20.58 ms\n",
            "Step 689/1000, Loss: 5.5399, Time: 15.02 ms\n",
            "Step 690/1000, Loss: 5.3608, Time: 14.83 ms\n",
            "Step 691/1000, Loss: 5.5836, Time: 15.73 ms\n",
            "Step 692/1000, Loss: 5.2465, Time: 16.66 ms\n",
            "Step 693/1000, Loss: 5.7241, Time: 15.09 ms\n",
            "Step 694/1000, Loss: 5.5071, Time: 16.96 ms\n",
            "Step 695/1000, Loss: 5.6897, Time: 15.35 ms\n",
            "Step 696/1000, Loss: 5.3601, Time: 16.60 ms\n",
            "Step 697/1000, Loss: 5.4280, Time: 15.21 ms\n",
            "Step 698/1000, Loss: 5.5986, Time: 14.71 ms\n",
            "Step 699/1000, Loss: 5.1881, Time: 14.86 ms\n",
            "Step 700/1000, Loss: 5.7099, Time: 16.94 ms\n",
            "Step 701/1000, Loss: 5.7638, Time: 15.24 ms\n",
            "Step 702/1000, Loss: 5.6371, Time: 15.92 ms\n",
            "Step 703/1000, Loss: 5.7139, Time: 14.81 ms\n",
            "Step 704/1000, Loss: 5.6653, Time: 16.48 ms\n",
            "Step 705/1000, Loss: 5.7186, Time: 15.77 ms\n",
            "Step 706/1000, Loss: 5.4993, Time: 15.68 ms\n",
            "Step 707/1000, Loss: 5.9654, Time: 21.01 ms\n",
            "Step 708/1000, Loss: 5.3958, Time: 16.28 ms\n",
            "Step 709/1000, Loss: 5.9842, Time: 26.36 ms\n",
            "Step 710/1000, Loss: 5.3012, Time: 23.31 ms\n",
            "Step 711/1000, Loss: 5.4041, Time: 21.81 ms\n",
            "Step 712/1000, Loss: 5.5741, Time: 21.57 ms\n",
            "Step 713/1000, Loss: 5.7329, Time: 19.78 ms\n",
            "Step 714/1000, Loss: 5.3415, Time: 19.80 ms\n",
            "Step 715/1000, Loss: 5.7993, Time: 22.65 ms\n",
            "Step 716/1000, Loss: 5.4688, Time: 20.80 ms\n",
            "Step 717/1000, Loss: 5.5415, Time: 19.94 ms\n",
            "Step 718/1000, Loss: 5.7102, Time: 19.85 ms\n",
            "Step 719/1000, Loss: 5.4523, Time: 25.76 ms\n",
            "Step 720/1000, Loss: 5.3355, Time: 20.77 ms\n",
            "Step 721/1000, Loss: 5.4159, Time: 22.56 ms\n",
            "Step 722/1000, Loss: 5.4756, Time: 20.59 ms\n",
            "Step 723/1000, Loss: 5.5287, Time: 18.88 ms\n",
            "Step 724/1000, Loss: 5.6101, Time: 21.60 ms\n",
            "Step 725/1000, Loss: 5.3992, Time: 17.76 ms\n",
            "Step 726/1000, Loss: 5.3290, Time: 20.17 ms\n",
            "Step 727/1000, Loss: 5.5765, Time: 26.42 ms\n",
            "Step 728/1000, Loss: 4.9423, Time: 22.16 ms\n",
            "Step 729/1000, Loss: 5.3584, Time: 18.78 ms\n",
            "Step 730/1000, Loss: 5.3007, Time: 19.48 ms\n",
            "Step 731/1000, Loss: 5.2964, Time: 21.45 ms\n",
            "Step 732/1000, Loss: 5.3071, Time: 26.97 ms\n",
            "Step 733/1000, Loss: 5.6132, Time: 19.43 ms\n",
            "Step 734/1000, Loss: 5.6354, Time: 22.64 ms\n",
            "Step 735/1000, Loss: 5.6428, Time: 25.18 ms\n",
            "Step 736/1000, Loss: 5.6795, Time: 24.38 ms\n",
            "Step 737/1000, Loss: 5.5533, Time: 24.47 ms\n",
            "Step 738/1000, Loss: 5.4359, Time: 24.47 ms\n",
            "Step 739/1000, Loss: 5.5640, Time: 29.73 ms\n",
            "Step 740/1000, Loss: 5.2639, Time: 23.69 ms\n",
            "Step 741/1000, Loss: 6.0239, Time: 23.91 ms\n",
            "Step 742/1000, Loss: 5.6782, Time: 14.50 ms\n",
            "Step 743/1000, Loss: 5.6363, Time: 14.36 ms\n",
            "Step 744/1000, Loss: 5.5450, Time: 16.04 ms\n",
            "Step 745/1000, Loss: 5.3471, Time: 15.27 ms\n",
            "Step 746/1000, Loss: 5.4938, Time: 15.40 ms\n",
            "Step 747/1000, Loss: 5.5874, Time: 15.20 ms\n",
            "Step 748/1000, Loss: 5.2636, Time: 15.25 ms\n",
            "Step 749/1000, Loss: 5.9523, Time: 14.52 ms\n",
            "Step 750/1000, Loss: 5.6434, Time: 14.51 ms\n",
            "Step 751/1000, Loss: 5.7132, Time: 14.94 ms\n",
            "Step 752/1000, Loss: 5.6777, Time: 16.99 ms\n",
            "Step 753/1000, Loss: 5.6843, Time: 15.06 ms\n",
            "Step 754/1000, Loss: 5.7724, Time: 16.34 ms\n",
            "Step 755/1000, Loss: 5.9636, Time: 15.45 ms\n",
            "Step 756/1000, Loss: 5.8843, Time: 15.85 ms\n",
            "Step 757/1000, Loss: 6.0022, Time: 17.37 ms\n",
            "Step 758/1000, Loss: 5.7143, Time: 14.61 ms\n",
            "Step 759/1000, Loss: 6.1834, Time: 18.76 ms\n",
            "Step 760/1000, Loss: 5.7437, Time: 16.98 ms\n",
            "Step 761/1000, Loss: 5.8183, Time: 14.72 ms\n",
            "Step 762/1000, Loss: 5.8259, Time: 14.81 ms\n",
            "Step 763/1000, Loss: 6.1415, Time: 14.79 ms\n",
            "Step 764/1000, Loss: 5.7263, Time: 15.42 ms\n",
            "Step 765/1000, Loss: 5.7689, Time: 14.86 ms\n",
            "Step 766/1000, Loss: 5.9178, Time: 17.70 ms\n",
            "Step 767/1000, Loss: 5.6205, Time: 14.15 ms\n",
            "Step 768/1000, Loss: 5.4811, Time: 15.65 ms\n",
            "Step 769/1000, Loss: 5.6592, Time: 14.24 ms\n",
            "Step 770/1000, Loss: 5.6199, Time: 14.68 ms\n",
            "Step 771/1000, Loss: 5.8911, Time: 15.82 ms\n",
            "Step 772/1000, Loss: 5.9932, Time: 16.38 ms\n",
            "Step 773/1000, Loss: 6.0848, Time: 16.17 ms\n",
            "Step 774/1000, Loss: 5.6297, Time: 15.63 ms\n",
            "Step 775/1000, Loss: 5.9810, Time: 15.72 ms\n",
            "Step 776/1000, Loss: 5.5549, Time: 15.75 ms\n",
            "Step 777/1000, Loss: 5.8546, Time: 14.86 ms\n",
            "Step 778/1000, Loss: 5.5247, Time: 14.65 ms\n",
            "Step 779/1000, Loss: 5.9453, Time: 14.94 ms\n",
            "Step 780/1000, Loss: 5.5696, Time: 16.08 ms\n",
            "Step 781/1000, Loss: 5.7093, Time: 14.72 ms\n",
            "Step 782/1000, Loss: 5.5087, Time: 15.40 ms\n",
            "Step 783/1000, Loss: 5.7556, Time: 15.02 ms\n",
            "Step 784/1000, Loss: 5.4733, Time: 15.74 ms\n",
            "Step 785/1000, Loss: 5.1991, Time: 15.55 ms\n",
            "Step 786/1000, Loss: 5.4444, Time: 15.12 ms\n",
            "Step 787/1000, Loss: 5.5576, Time: 15.83 ms\n",
            "Step 788/1000, Loss: 5.7551, Time: 15.92 ms\n",
            "Step 789/1000, Loss: 5.8197, Time: 14.77 ms\n",
            "Step 790/1000, Loss: 5.4421, Time: 14.33 ms\n",
            "Step 791/1000, Loss: 5.9052, Time: 15.18 ms\n",
            "Step 792/1000, Loss: 5.2122, Time: 15.37 ms\n",
            "Step 793/1000, Loss: 5.9162, Time: 15.32 ms\n",
            "Step 794/1000, Loss: 5.4433, Time: 14.59 ms\n",
            "Step 795/1000, Loss: 5.7536, Time: 14.51 ms\n",
            "Step 796/1000, Loss: 5.3882, Time: 15.39 ms\n",
            "Step 797/1000, Loss: 6.3556, Time: 14.53 ms\n",
            "Step 798/1000, Loss: 5.4451, Time: 14.13 ms\n",
            "Step 799/1000, Loss: 5.2227, Time: 14.45 ms\n",
            "Step 800/1000, Loss: 5.6693, Time: 15.09 ms\n",
            "Step 801/1000, Loss: 5.4340, Time: 14.56 ms\n",
            "Step 802/1000, Loss: 5.1623, Time: 20.80 ms\n",
            "Step 803/1000, Loss: 6.1487, Time: 17.29 ms\n",
            "Step 804/1000, Loss: 5.7095, Time: 19.02 ms\n",
            "Step 805/1000, Loss: 5.4709, Time: 15.18 ms\n",
            "Step 806/1000, Loss: 5.3194, Time: 18.43 ms\n",
            "Step 807/1000, Loss: 5.5028, Time: 14.91 ms\n",
            "Step 808/1000, Loss: 5.5019, Time: 15.19 ms\n",
            "Step 809/1000, Loss: 5.7925, Time: 14.53 ms\n",
            "Step 810/1000, Loss: 5.4869, Time: 14.92 ms\n",
            "Step 811/1000, Loss: 6.0443, Time: 15.14 ms\n",
            "Step 812/1000, Loss: 5.5789, Time: 15.69 ms\n",
            "Step 813/1000, Loss: 5.7663, Time: 14.74 ms\n",
            "Step 814/1000, Loss: 5.4945, Time: 14.51 ms\n",
            "Step 815/1000, Loss: 5.5885, Time: 14.82 ms\n",
            "Step 816/1000, Loss: 5.3078, Time: 16.50 ms\n",
            "Step 817/1000, Loss: 5.2680, Time: 15.54 ms\n",
            "Step 818/1000, Loss: 5.4051, Time: 15.54 ms\n",
            "Step 819/1000, Loss: 5.2162, Time: 14.44 ms\n",
            "Step 820/1000, Loss: 5.4993, Time: 19.35 ms\n",
            "Step 821/1000, Loss: 5.5278, Time: 23.19 ms\n",
            "Step 822/1000, Loss: 5.4128, Time: 14.80 ms\n",
            "Step 823/1000, Loss: 5.5336, Time: 14.84 ms\n",
            "Step 824/1000, Loss: 5.6225, Time: 15.82 ms\n",
            "Step 825/1000, Loss: 5.9617, Time: 14.82 ms\n",
            "Step 826/1000, Loss: 5.6944, Time: 15.75 ms\n",
            "Step 827/1000, Loss: 5.7816, Time: 14.63 ms\n",
            "Step 828/1000, Loss: 5.3764, Time: 15.22 ms\n",
            "Step 829/1000, Loss: 5.7087, Time: 14.60 ms\n",
            "Step 830/1000, Loss: 5.4110, Time: 14.99 ms\n",
            "Step 831/1000, Loss: 5.4058, Time: 15.39 ms\n",
            "Step 832/1000, Loss: 5.2044, Time: 18.09 ms\n",
            "Step 833/1000, Loss: 5.4598, Time: 15.13 ms\n",
            "Step 834/1000, Loss: 5.2091, Time: 15.03 ms\n",
            "Step 835/1000, Loss: 5.4219, Time: 14.58 ms\n",
            "Step 836/1000, Loss: 5.7389, Time: 15.68 ms\n",
            "Step 837/1000, Loss: 5.8307, Time: 14.56 ms\n",
            "Step 838/1000, Loss: 5.6174, Time: 22.94 ms\n",
            "Step 839/1000, Loss: 5.9033, Time: 14.69 ms\n",
            "Step 840/1000, Loss: 5.6464, Time: 16.22 ms\n",
            "Step 841/1000, Loss: 5.7334, Time: 14.86 ms\n",
            "Step 842/1000, Loss: 5.2675, Time: 16.39 ms\n",
            "Step 843/1000, Loss: 5.7381, Time: 14.54 ms\n",
            "Step 844/1000, Loss: 5.7499, Time: 15.40 ms\n",
            "Step 845/1000, Loss: 5.6301, Time: 14.42 ms\n",
            "Step 846/1000, Loss: 5.6876, Time: 14.95 ms\n",
            "Step 847/1000, Loss: 5.6479, Time: 14.66 ms\n",
            "Step 848/1000, Loss: 5.0196, Time: 15.21 ms\n",
            "Step 849/1000, Loss: 4.8127, Time: 14.31 ms\n",
            "Step 850/1000, Loss: 5.2701, Time: 15.03 ms\n",
            "Step 851/1000, Loss: 5.8531, Time: 15.22 ms\n",
            "Step 852/1000, Loss: 5.6346, Time: 15.22 ms\n",
            "Step 853/1000, Loss: 5.6586, Time: 16.30 ms\n",
            "Step 854/1000, Loss: 5.3072, Time: 16.69 ms\n",
            "Step 855/1000, Loss: 5.4878, Time: 20.62 ms\n",
            "Step 856/1000, Loss: 5.4243, Time: 15.42 ms\n",
            "Step 857/1000, Loss: 5.3512, Time: 14.36 ms\n",
            "Step 858/1000, Loss: 5.4641, Time: 14.82 ms\n",
            "Step 859/1000, Loss: 5.5629, Time: 16.68 ms\n",
            "Step 860/1000, Loss: 5.2691, Time: 15.85 ms\n",
            "Step 861/1000, Loss: 5.8066, Time: 14.40 ms\n",
            "Step 862/1000, Loss: 5.2874, Time: 14.73 ms\n",
            "Step 863/1000, Loss: 5.6009, Time: 14.76 ms\n",
            "Step 864/1000, Loss: 5.7026, Time: 15.50 ms\n",
            "Step 865/1000, Loss: 5.5345, Time: 14.75 ms\n",
            "Step 866/1000, Loss: 5.0195, Time: 14.60 ms\n",
            "Step 867/1000, Loss: 5.6744, Time: 15.30 ms\n",
            "Step 868/1000, Loss: 5.6784, Time: 16.47 ms\n",
            "Step 869/1000, Loss: 5.9037, Time: 15.00 ms\n",
            "Step 870/1000, Loss: 5.6372, Time: 15.52 ms\n",
            "Step 871/1000, Loss: 5.8259, Time: 16.18 ms\n",
            "Step 872/1000, Loss: 5.7008, Time: 16.20 ms\n",
            "Step 873/1000, Loss: 6.0043, Time: 15.65 ms\n",
            "Step 874/1000, Loss: 5.5584, Time: 17.84 ms\n",
            "Step 875/1000, Loss: 5.6829, Time: 16.18 ms\n",
            "Step 876/1000, Loss: 5.6398, Time: 16.42 ms\n",
            "Step 877/1000, Loss: 5.6795, Time: 14.65 ms\n",
            "Step 878/1000, Loss: 5.6362, Time: 14.43 ms\n",
            "Step 879/1000, Loss: 5.4891, Time: 15.46 ms\n",
            "Step 880/1000, Loss: 5.4255, Time: 16.46 ms\n",
            "Step 881/1000, Loss: 5.6896, Time: 15.65 ms\n",
            "Step 882/1000, Loss: 5.7911, Time: 15.49 ms\n",
            "Step 883/1000, Loss: 5.6315, Time: 15.84 ms\n",
            "Step 884/1000, Loss: 5.8319, Time: 17.13 ms\n",
            "Step 885/1000, Loss: 5.9809, Time: 14.78 ms\n",
            "Step 886/1000, Loss: 6.0384, Time: 18.78 ms\n",
            "Step 887/1000, Loss: 6.2928, Time: 14.44 ms\n",
            "Step 888/1000, Loss: 5.9939, Time: 15.70 ms\n",
            "Step 889/1000, Loss: 5.7752, Time: 18.33 ms\n",
            "Step 890/1000, Loss: 5.7718, Time: 15.37 ms\n",
            "Step 891/1000, Loss: 5.8443, Time: 15.43 ms\n",
            "Step 892/1000, Loss: 6.0818, Time: 17.61 ms\n",
            "Step 893/1000, Loss: 5.7340, Time: 14.39 ms\n",
            "Step 894/1000, Loss: 5.5302, Time: 14.58 ms\n",
            "Step 895/1000, Loss: 5.6413, Time: 14.55 ms\n",
            "Step 896/1000, Loss: 5.4897, Time: 19.38 ms\n",
            "Step 897/1000, Loss: 6.0770, Time: 14.79 ms\n",
            "Step 898/1000, Loss: 5.6120, Time: 14.48 ms\n",
            "Step 899/1000, Loss: 5.6170, Time: 14.17 ms\n",
            "Step 900/1000, Loss: 5.8415, Time: 14.76 ms\n",
            "Step 901/1000, Loss: 5.5222, Time: 14.74 ms\n",
            "Step 902/1000, Loss: 5.5748, Time: 14.25 ms\n",
            "Step 903/1000, Loss: 5.6968, Time: 26.03 ms\n",
            "Step 904/1000, Loss: 6.1209, Time: 16.43 ms\n",
            "Step 905/1000, Loss: 6.0219, Time: 19.80 ms\n",
            "Step 906/1000, Loss: 5.4522, Time: 16.24 ms\n",
            "Step 907/1000, Loss: 5.6328, Time: 24.49 ms\n",
            "Step 908/1000, Loss: 5.6303, Time: 24.22 ms\n",
            "Step 909/1000, Loss: 5.5375, Time: 20.77 ms\n",
            "Step 910/1000, Loss: 5.5750, Time: 25.74 ms\n",
            "Step 911/1000, Loss: 5.8241, Time: 22.95 ms\n",
            "Step 912/1000, Loss: 5.5778, Time: 18.79 ms\n",
            "Step 913/1000, Loss: 5.7713, Time: 18.80 ms\n",
            "Step 914/1000, Loss: 5.4267, Time: 19.04 ms\n",
            "Step 915/1000, Loss: 5.3611, Time: 21.34 ms\n",
            "Step 916/1000, Loss: 5.5749, Time: 19.78 ms\n",
            "Step 917/1000, Loss: 5.3565, Time: 20.02 ms\n",
            "Step 918/1000, Loss: 5.1002, Time: 20.81 ms\n",
            "Step 919/1000, Loss: 5.7504, Time: 25.21 ms\n",
            "Step 920/1000, Loss: 5.2698, Time: 18.27 ms\n",
            "Step 921/1000, Loss: 5.4936, Time: 17.57 ms\n",
            "Step 922/1000, Loss: 5.5091, Time: 21.09 ms\n",
            "Step 923/1000, Loss: 5.7398, Time: 21.14 ms\n",
            "Step 924/1000, Loss: 5.6470, Time: 20.17 ms\n",
            "Step 925/1000, Loss: 5.9160, Time: 22.46 ms\n",
            "Step 926/1000, Loss: 5.9678, Time: 20.65 ms\n",
            "Step 927/1000, Loss: 5.5809, Time: 24.29 ms\n",
            "Step 928/1000, Loss: 5.5045, Time: 20.71 ms\n",
            "Step 929/1000, Loss: 5.8948, Time: 21.35 ms\n",
            "Step 930/1000, Loss: 5.8112, Time: 23.68 ms\n",
            "Step 931/1000, Loss: 5.5978, Time: 24.11 ms\n",
            "Step 932/1000, Loss: 5.4316, Time: 25.15 ms\n",
            "Step 933/1000, Loss: 5.7884, Time: 24.16 ms\n",
            "Step 934/1000, Loss: 5.6932, Time: 21.41 ms\n",
            "Step 935/1000, Loss: 5.5008, Time: 26.99 ms\n",
            "Step 936/1000, Loss: 5.6071, Time: 24.92 ms\n",
            "Step 937/1000, Loss: 5.5748, Time: 23.49 ms\n",
            "Step 938/1000, Loss: 5.3623, Time: 23.67 ms\n",
            "Step 939/1000, Loss: 5.4040, Time: 24.88 ms\n",
            "Step 940/1000, Loss: 5.4090, Time: 15.92 ms\n",
            "Step 941/1000, Loss: 5.2477, Time: 14.47 ms\n",
            "Step 942/1000, Loss: 5.5549, Time: 14.52 ms\n",
            "Step 943/1000, Loss: 5.2445, Time: 14.30 ms\n",
            "Step 944/1000, Loss: 5.5483, Time: 15.10 ms\n",
            "Step 945/1000, Loss: 5.7311, Time: 15.76 ms\n",
            "Step 946/1000, Loss: 6.2062, Time: 14.65 ms\n",
            "Step 947/1000, Loss: 5.4183, Time: 15.09 ms\n",
            "Step 948/1000, Loss: 5.6900, Time: 15.81 ms\n",
            "Step 949/1000, Loss: 5.7797, Time: 14.87 ms\n",
            "Step 950/1000, Loss: 5.5149, Time: 14.44 ms\n",
            "Step 951/1000, Loss: 5.5571, Time: 14.46 ms\n",
            "Step 952/1000, Loss: 5.7974, Time: 17.17 ms\n",
            "Step 953/1000, Loss: 5.6360, Time: 14.81 ms\n",
            "Step 954/1000, Loss: 5.1364, Time: 14.12 ms\n",
            "Step 955/1000, Loss: 5.5429, Time: 15.35 ms\n",
            "Step 956/1000, Loss: 5.3887, Time: 16.94 ms\n",
            "Step 957/1000, Loss: 5.6598, Time: 14.12 ms\n",
            "Step 958/1000, Loss: 5.0682, Time: 14.19 ms\n",
            "Step 959/1000, Loss: 5.5266, Time: 13.89 ms\n",
            "Step 960/1000, Loss: 5.5988, Time: 14.86 ms\n",
            "Step 961/1000, Loss: 6.0032, Time: 14.33 ms\n",
            "Step 962/1000, Loss: 5.5926, Time: 14.99 ms\n",
            "Step 963/1000, Loss: 5.7533, Time: 15.02 ms\n",
            "Step 964/1000, Loss: 5.4573, Time: 15.82 ms\n",
            "Step 965/1000, Loss: 5.4079, Time: 15.10 ms\n",
            "Step 966/1000, Loss: 5.6369, Time: 15.74 ms\n",
            "Step 967/1000, Loss: 5.5730, Time: 14.66 ms\n",
            "Step 968/1000, Loss: 5.3975, Time: 15.56 ms\n",
            "Step 969/1000, Loss: 5.3267, Time: 15.16 ms\n",
            "Step 970/1000, Loss: 5.2262, Time: 17.04 ms\n",
            "Step 971/1000, Loss: 5.3935, Time: 14.57 ms\n",
            "Step 972/1000, Loss: 5.1798, Time: 16.11 ms\n",
            "Step 973/1000, Loss: 5.4006, Time: 15.50 ms\n",
            "Step 974/1000, Loss: 5.3669, Time: 16.76 ms\n",
            "Step 975/1000, Loss: 5.3567, Time: 15.02 ms\n",
            "Step 976/1000, Loss: 5.3124, Time: 15.47 ms\n",
            "Step 977/1000, Loss: 5.3040, Time: 16.42 ms\n",
            "Step 978/1000, Loss: 5.0734, Time: 14.19 ms\n",
            "Step 979/1000, Loss: 5.5700, Time: 14.88 ms\n",
            "Step 980/1000, Loss: 5.6498, Time: 15.31 ms\n",
            "Step 981/1000, Loss: 5.8084, Time: 14.97 ms\n",
            "Step 982/1000, Loss: 5.9125, Time: 14.85 ms\n",
            "Step 983/1000, Loss: 6.0969, Time: 14.69 ms\n",
            "Step 984/1000, Loss: 5.6385, Time: 17.88 ms\n",
            "Step 985/1000, Loss: 6.0574, Time: 14.74 ms\n",
            "Step 986/1000, Loss: 5.9128, Time: 14.78 ms\n",
            "Step 987/1000, Loss: 5.6564, Time: 14.16 ms\n",
            "Step 988/1000, Loss: 5.3825, Time: 15.58 ms\n",
            "Step 989/1000, Loss: 5.6687, Time: 16.43 ms\n",
            "Step 990/1000, Loss: 5.5398, Time: 20.04 ms\n",
            "Step 991/1000, Loss: 5.8239, Time: 14.99 ms\n",
            "Step 992/1000, Loss: 5.3368, Time: 18.79 ms\n",
            "Step 993/1000, Loss: 5.8850, Time: 14.36 ms\n",
            "Step 994/1000, Loss: 5.3875, Time: 15.72 ms\n",
            "Step 995/1000, Loss: 5.9211, Time: 14.85 ms\n",
            "Step 996/1000, Loss: 5.3462, Time: 15.15 ms\n",
            "Step 997/1000, Loss: 5.7090, Time: 15.00 ms\n",
            "Step 998/1000, Loss: 5.3424, Time: 14.76 ms\n",
            "Step 999/1000, Loss: 5.7216, Time: 17.27 ms\n",
            "Step 1000/1000, Loss: 5.3645, Time: 15.81 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import time\n",
        "import tiktoken  # Assuming this is a utility for tokenization\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 256  # reduced max sequence length to fit into 4GB GPU\n",
        "    vocab_size: int = 50304  # number of tokens\n",
        "    n_layer: int = 6  # increased number of layers for better learning\n",
        "    n_head: int = 8  # increased number of heads for better learning\n",
        "    n_embd: int = 256  # increased embedding dimension for better learning\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "# Initialize model and prepare for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(42)\n",
        "model = GPT(GPTConfig()).to(device)\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "train_loader = DataLoaderLite(B=4, T=256)  # Adjust batch size and sequence length as needed\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "max_steps = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for step in range(max_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = model(x, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Step {step + 1}/{max_steps} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edXGH90tdW2W",
        "outputId": "62778ff7-b2cc-46db-fa46-3ecbc05f5bf9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Step 6/1000 | Loss: 1.8202\n",
            "Step 7/1000 | Loss: 1.5345\n",
            "Step 8/1000 | Loss: 1.9093\n",
            "Step 9/1000 | Loss: 1.6938\n",
            "Step 10/1000 | Loss: 1.9894\n",
            "Step 11/1000 | Loss: 1.7140\n",
            "Step 12/1000 | Loss: 1.8872\n",
            "Step 13/1000 | Loss: 1.7128\n",
            "Step 14/1000 | Loss: 2.1265\n",
            "Step 15/1000 | Loss: 2.2721\n",
            "Step 16/1000 | Loss: 2.1492\n",
            "Step 17/1000 | Loss: 2.0144\n",
            "Step 18/1000 | Loss: 1.7331\n",
            "Step 19/1000 | Loss: 2.0080\n",
            "Step 20/1000 | Loss: 1.8211\n",
            "Step 21/1000 | Loss: 1.7549\n",
            "Step 22/1000 | Loss: 1.7236\n",
            "Step 23/1000 | Loss: 1.7665\n",
            "Step 24/1000 | Loss: 1.8104\n",
            "Step 25/1000 | Loss: 1.9676\n",
            "Step 26/1000 | Loss: 2.2003\n",
            "Step 27/1000 | Loss: 2.1039\n",
            "Step 28/1000 | Loss: 2.0842\n",
            "Step 29/1000 | Loss: 2.1093\n",
            "Step 30/1000 | Loss: 2.0263\n",
            "Step 31/1000 | Loss: 2.0068\n",
            "Step 32/1000 | Loss: 1.8059\n",
            "Step 33/1000 | Loss: 2.0369\n",
            "Step 34/1000 | Loss: 2.0976\n",
            "Step 35/1000 | Loss: 1.9823\n",
            "Step 36/1000 | Loss: 1.9558\n",
            "Step 37/1000 | Loss: 1.9057\n",
            "Step 38/1000 | Loss: 1.6564\n",
            "Step 39/1000 | Loss: 1.4387\n",
            "Step 40/1000 | Loss: 1.8131\n",
            "Step 41/1000 | Loss: 2.0855\n",
            "Step 42/1000 | Loss: 2.0814\n",
            "Step 43/1000 | Loss: 2.0023\n",
            "Step 44/1000 | Loss: 1.7482\n",
            "Step 45/1000 | Loss: 1.7074\n",
            "Step 46/1000 | Loss: 1.9728\n",
            "Step 47/1000 | Loss: 1.6571\n",
            "Step 48/1000 | Loss: 1.8891\n",
            "Step 49/1000 | Loss: 1.8926\n",
            "Step 50/1000 | Loss: 1.7388\n",
            "Step 51/1000 | Loss: 1.8971\n",
            "Step 52/1000 | Loss: 1.6311\n",
            "Step 53/1000 | Loss: 1.8346\n",
            "Step 54/1000 | Loss: 1.9631\n",
            "Step 55/1000 | Loss: 1.7279\n",
            "Step 56/1000 | Loss: 1.4335\n",
            "Step 57/1000 | Loss: 1.8171\n",
            "Step 58/1000 | Loss: 1.8872\n",
            "Step 59/1000 | Loss: 2.1176\n",
            "Step 60/1000 | Loss: 1.8151\n",
            "Step 61/1000 | Loss: 2.1717\n",
            "Step 62/1000 | Loss: 2.0125\n",
            "Step 63/1000 | Loss: 2.1020\n",
            "Step 64/1000 | Loss: 1.8550\n",
            "Step 65/1000 | Loss: 1.8364\n",
            "Step 66/1000 | Loss: 1.9038\n",
            "Step 67/1000 | Loss: 1.9833\n",
            "Step 68/1000 | Loss: 2.0938\n",
            "Step 69/1000 | Loss: 1.8457\n",
            "Step 70/1000 | Loss: 1.8959\n",
            "Step 71/1000 | Loss: 1.9113\n",
            "Step 72/1000 | Loss: 2.2475\n",
            "Step 73/1000 | Loss: 2.0584\n",
            "Step 74/1000 | Loss: 2.1131\n",
            "Step 75/1000 | Loss: 2.1006\n",
            "Step 76/1000 | Loss: 2.2815\n",
            "Step 77/1000 | Loss: 2.3801\n",
            "Step 78/1000 | Loss: 2.2726\n",
            "Step 79/1000 | Loss: 1.9965\n",
            "Step 80/1000 | Loss: 2.1708\n",
            "Step 81/1000 | Loss: 2.1483\n",
            "Step 82/1000 | Loss: 2.2739\n",
            "Step 83/1000 | Loss: 1.8681\n",
            "Step 84/1000 | Loss: 1.9782\n",
            "Step 85/1000 | Loss: 1.9535\n",
            "Step 86/1000 | Loss: 1.9858\n",
            "Step 87/1000 | Loss: 2.1858\n",
            "Step 88/1000 | Loss: 1.9778\n",
            "Step 89/1000 | Loss: 1.9940\n",
            "Step 90/1000 | Loss: 2.3331\n",
            "Step 91/1000 | Loss: 2.1696\n",
            "Step 92/1000 | Loss: 2.1291\n",
            "Step 93/1000 | Loss: 2.0052\n",
            "Step 94/1000 | Loss: 2.3500\n",
            "Step 95/1000 | Loss: 2.1112\n",
            "Step 96/1000 | Loss: 1.8344\n",
            "Step 97/1000 | Loss: 1.8940\n",
            "Step 98/1000 | Loss: 2.1023\n",
            "Step 99/1000 | Loss: 1.9244\n",
            "Step 100/1000 | Loss: 1.7853\n",
            "Step 101/1000 | Loss: 2.1239\n",
            "Step 102/1000 | Loss: 2.1606\n",
            "Step 103/1000 | Loss: 2.0081\n",
            "Step 104/1000 | Loss: 1.8506\n",
            "Step 105/1000 | Loss: 1.7395\n",
            "Step 106/1000 | Loss: 1.8218\n",
            "Step 107/1000 | Loss: 1.8340\n",
            "Step 108/1000 | Loss: 1.6746\n",
            "Step 109/1000 | Loss: 1.8247\n",
            "Step 110/1000 | Loss: 1.7936\n",
            "Step 111/1000 | Loss: 1.7579\n",
            "Step 112/1000 | Loss: 1.8753\n",
            "Step 113/1000 | Loss: 1.8158\n",
            "Step 114/1000 | Loss: 1.9460\n",
            "Step 115/1000 | Loss: 2.0539\n",
            "Step 116/1000 | Loss: 2.1514\n",
            "Step 117/1000 | Loss: 1.9131\n",
            "Step 118/1000 | Loss: 1.8709\n",
            "Step 119/1000 | Loss: 2.0030\n",
            "Step 120/1000 | Loss: 2.1734\n",
            "Step 121/1000 | Loss: 2.0007\n",
            "Step 122/1000 | Loss: 1.6922\n",
            "Step 123/1000 | Loss: 2.0754\n",
            "Step 124/1000 | Loss: 1.9736\n",
            "Step 125/1000 | Loss: 1.8622\n",
            "Step 126/1000 | Loss: 1.9188\n",
            "Step 127/1000 | Loss: 1.8776\n",
            "Step 128/1000 | Loss: 1.8517\n",
            "Step 129/1000 | Loss: 1.8525\n",
            "Step 130/1000 | Loss: 1.9139\n",
            "Step 131/1000 | Loss: 1.7577\n",
            "Step 132/1000 | Loss: 1.9912\n",
            "Step 133/1000 | Loss: 1.6793\n",
            "Step 134/1000 | Loss: 2.0387\n",
            "Step 135/1000 | Loss: 2.0007\n",
            "Step 136/1000 | Loss: 2.1764\n",
            "Step 137/1000 | Loss: 1.8165\n",
            "Step 138/1000 | Loss: 2.1924\n",
            "Step 139/1000 | Loss: 2.0231\n",
            "Step 140/1000 | Loss: 1.8990\n",
            "Step 141/1000 | Loss: 1.8205\n",
            "Step 142/1000 | Loss: 2.0060\n",
            "Step 143/1000 | Loss: 1.8987\n",
            "Step 144/1000 | Loss: 1.5911\n",
            "Step 145/1000 | Loss: 1.7735\n",
            "Step 146/1000 | Loss: 1.8178\n",
            "Step 147/1000 | Loss: 1.8601\n",
            "Step 148/1000 | Loss: 1.2900\n",
            "Step 149/1000 | Loss: 1.7799\n",
            "Step 150/1000 | Loss: 1.8841\n",
            "Step 151/1000 | Loss: 2.1176\n",
            "Step 152/1000 | Loss: 1.8373\n",
            "Step 153/1000 | Loss: 1.8834\n",
            "Step 154/1000 | Loss: 1.8547\n",
            "Step 155/1000 | Loss: 1.8305\n",
            "Step 156/1000 | Loss: 1.9379\n",
            "Step 157/1000 | Loss: 1.9892\n",
            "Step 158/1000 | Loss: 1.9255\n",
            "Step 159/1000 | Loss: 1.5809\n",
            "Step 160/1000 | Loss: 1.6659\n",
            "Step 161/1000 | Loss: 1.8646\n",
            "Step 162/1000 | Loss: 1.6663\n",
            "Step 163/1000 | Loss: 1.7064\n",
            "Step 164/1000 | Loss: 1.5463\n",
            "Step 165/1000 | Loss: 1.6324\n",
            "Step 166/1000 | Loss: 1.6326\n",
            "Step 167/1000 | Loss: 1.5585\n",
            "Step 168/1000 | Loss: 1.5187\n",
            "Step 169/1000 | Loss: 1.6349\n",
            "Step 170/1000 | Loss: 1.9710\n",
            "Step 171/1000 | Loss: 1.9321\n",
            "Step 172/1000 | Loss: 1.9116\n",
            "Step 173/1000 | Loss: 2.0893\n",
            "Step 174/1000 | Loss: 1.8466\n",
            "Step 175/1000 | Loss: 2.0356\n",
            "Step 176/1000 | Loss: 2.0274\n",
            "Step 177/1000 | Loss: 1.7936\n",
            "Step 178/1000 | Loss: 1.7598\n",
            "Step 179/1000 | Loss: 1.7019\n",
            "Step 180/1000 | Loss: 1.7635\n",
            "Step 181/1000 | Loss: 2.1241\n",
            "Step 182/1000 | Loss: 2.0790\n",
            "Step 183/1000 | Loss: 2.1989\n",
            "Step 184/1000 | Loss: 1.9712\n",
            "Step 185/1000 | Loss: 1.9037\n",
            "Step 186/1000 | Loss: 1.7213\n",
            "Step 187/1000 | Loss: 2.0934\n",
            "Step 188/1000 | Loss: 1.8722\n",
            "Step 189/1000 | Loss: 1.9609\n",
            "Step 190/1000 | Loss: 1.9779\n",
            "Step 191/1000 | Loss: 2.1089\n",
            "Step 192/1000 | Loss: 2.3690\n",
            "Step 193/1000 | Loss: 1.9112\n",
            "Step 194/1000 | Loss: 2.1337\n",
            "Step 195/1000 | Loss: 2.1748\n",
            "Step 196/1000 | Loss: 2.1663\n",
            "Step 197/1000 | Loss: 2.1026\n",
            "Step 198/1000 | Loss: 1.9868\n",
            "Step 199/1000 | Loss: 1.8432\n",
            "Step 200/1000 | Loss: 2.0519\n",
            "Step 201/1000 | Loss: 1.7043\n",
            "Step 202/1000 | Loss: 2.1143\n",
            "Step 203/1000 | Loss: 1.7559\n",
            "Step 204/1000 | Loss: 1.6239\n",
            "Step 205/1000 | Loss: 1.9713\n",
            "Step 206/1000 | Loss: 1.8452\n",
            "Step 207/1000 | Loss: 1.8531\n",
            "Step 208/1000 | Loss: 1.7663\n",
            "Step 209/1000 | Loss: 1.8539\n",
            "Step 210/1000 | Loss: 1.7268\n",
            "Step 211/1000 | Loss: 1.8596\n",
            "Step 212/1000 | Loss: 1.7422\n",
            "Step 213/1000 | Loss: 1.7786\n",
            "Step 214/1000 | Loss: 1.6547\n",
            "Step 215/1000 | Loss: 1.8286\n",
            "Step 216/1000 | Loss: 1.7147\n",
            "Step 217/1000 | Loss: 1.8087\n",
            "Step 218/1000 | Loss: 1.8461\n",
            "Step 219/1000 | Loss: 1.6162\n",
            "Step 220/1000 | Loss: 1.9520\n",
            "Step 221/1000 | Loss: 1.8724\n",
            "Step 222/1000 | Loss: 1.7672\n",
            "Step 223/1000 | Loss: 1.7531\n",
            "Step 224/1000 | Loss: 2.0184\n",
            "Step 225/1000 | Loss: 1.9301\n",
            "Step 226/1000 | Loss: 1.9300\n",
            "Step 227/1000 | Loss: 2.1068\n",
            "Step 228/1000 | Loss: 1.7275\n",
            "Step 229/1000 | Loss: 1.9540\n",
            "Step 230/1000 | Loss: 1.3536\n",
            "Step 231/1000 | Loss: 1.2556\n",
            "Step 232/1000 | Loss: 1.6300\n",
            "Step 233/1000 | Loss: 1.8626\n",
            "Step 234/1000 | Loss: 1.5575\n",
            "Step 235/1000 | Loss: 1.6585\n",
            "Step 236/1000 | Loss: 1.7406\n",
            "Step 237/1000 | Loss: 1.6926\n",
            "Step 238/1000 | Loss: 1.8863\n",
            "Step 239/1000 | Loss: 1.5289\n",
            "Step 240/1000 | Loss: 1.7524\n",
            "Step 241/1000 | Loss: 1.7019\n",
            "Step 242/1000 | Loss: 1.7383\n",
            "Step 243/1000 | Loss: 1.7004\n",
            "Step 244/1000 | Loss: 1.7917\n",
            "Step 245/1000 | Loss: 1.6980\n",
            "Step 246/1000 | Loss: 1.4986\n",
            "Step 247/1000 | Loss: 1.7202\n",
            "Step 248/1000 | Loss: 1.4120\n",
            "Step 249/1000 | Loss: 1.5226\n",
            "Step 250/1000 | Loss: 1.6083\n",
            "Step 251/1000 | Loss: 1.7170\n",
            "Step 252/1000 | Loss: 1.6691\n",
            "Step 253/1000 | Loss: 1.8393\n",
            "Step 254/1000 | Loss: 2.0032\n",
            "Step 255/1000 | Loss: 1.8034\n",
            "Step 256/1000 | Loss: 2.0823\n",
            "Step 257/1000 | Loss: 1.8428\n",
            "Step 258/1000 | Loss: 1.8016\n",
            "Step 259/1000 | Loss: 1.7774\n",
            "Step 260/1000 | Loss: 1.6296\n",
            "Step 261/1000 | Loss: 1.9973\n",
            "Step 262/1000 | Loss: 1.6994\n",
            "Step 263/1000 | Loss: 1.4973\n",
            "Step 264/1000 | Loss: 1.6521\n",
            "Step 265/1000 | Loss: 1.3333\n",
            "Step 266/1000 | Loss: 1.6228\n",
            "Step 267/1000 | Loss: 1.5457\n",
            "Step 268/1000 | Loss: 1.7216\n",
            "Step 269/1000 | Loss: 2.0532\n",
            "Step 270/1000 | Loss: 2.0106\n",
            "Step 271/1000 | Loss: 1.8209\n",
            "Step 272/1000 | Loss: 1.8557\n",
            "Step 273/1000 | Loss: 1.7456\n",
            "Step 274/1000 | Loss: 2.0577\n",
            "Step 275/1000 | Loss: 2.1234\n",
            "Step 276/1000 | Loss: 2.0202\n",
            "Step 277/1000 | Loss: 2.0150\n",
            "Step 278/1000 | Loss: 1.9116\n",
            "Step 279/1000 | Loss: 1.9412\n",
            "Step 280/1000 | Loss: 1.8454\n",
            "Step 281/1000 | Loss: 1.7239\n",
            "Step 282/1000 | Loss: 1.9765\n",
            "Step 283/1000 | Loss: 1.9459\n",
            "Step 284/1000 | Loss: 1.8149\n",
            "Step 285/1000 | Loss: 1.8943\n",
            "Step 286/1000 | Loss: 1.9812\n",
            "Step 287/1000 | Loss: 1.8487\n",
            "Step 288/1000 | Loss: 1.8805\n",
            "Step 289/1000 | Loss: 1.7588\n",
            "Step 290/1000 | Loss: 1.8225\n",
            "Step 291/1000 | Loss: 1.9484\n",
            "Step 292/1000 | Loss: 2.1221\n",
            "Step 293/1000 | Loss: 2.0473\n",
            "Step 294/1000 | Loss: 1.7564\n",
            "Step 295/1000 | Loss: 1.9170\n",
            "Step 296/1000 | Loss: 1.7098\n",
            "Step 297/1000 | Loss: 2.0677\n",
            "Step 298/1000 | Loss: 1.7066\n",
            "Step 299/1000 | Loss: 1.8798\n",
            "Step 300/1000 | Loss: 1.9385\n",
            "Step 301/1000 | Loss: 1.8380\n",
            "Step 302/1000 | Loss: 1.8088\n",
            "Step 303/1000 | Loss: 1.9800\n",
            "Step 304/1000 | Loss: 1.5695\n",
            "Step 305/1000 | Loss: 1.4088\n",
            "Step 306/1000 | Loss: 1.4607\n",
            "Step 307/1000 | Loss: 1.5451\n",
            "Step 308/1000 | Loss: 2.1498\n",
            "Step 309/1000 | Loss: 1.9301\n",
            "Step 310/1000 | Loss: 1.7108\n",
            "Step 311/1000 | Loss: 2.0727\n",
            "Step 312/1000 | Loss: 1.7310\n",
            "Step 313/1000 | Loss: 1.9946\n",
            "Step 314/1000 | Loss: 1.8840\n",
            "Step 315/1000 | Loss: 1.9219\n",
            "Step 316/1000 | Loss: 1.9464\n",
            "Step 317/1000 | Loss: 2.2606\n",
            "Step 318/1000 | Loss: 1.8925\n",
            "Step 319/1000 | Loss: 1.7336\n",
            "Step 320/1000 | Loss: 2.0467\n",
            "Step 321/1000 | Loss: 1.7930\n",
            "Step 322/1000 | Loss: 1.6549\n",
            "Step 323/1000 | Loss: 2.0837\n",
            "Step 324/1000 | Loss: 1.8345\n",
            "Step 325/1000 | Loss: 1.7152\n",
            "Step 326/1000 | Loss: 1.6194\n",
            "Step 327/1000 | Loss: 1.8151\n",
            "Step 328/1000 | Loss: 1.7307\n",
            "Step 329/1000 | Loss: 1.7757\n",
            "Step 330/1000 | Loss: 1.7313\n",
            "Step 331/1000 | Loss: 2.0694\n",
            "Step 332/1000 | Loss: 1.7796\n",
            "Step 333/1000 | Loss: 1.7483\n",
            "Step 334/1000 | Loss: 1.8489\n",
            "Step 335/1000 | Loss: 1.7735\n",
            "Step 336/1000 | Loss: 1.7949\n",
            "Step 337/1000 | Loss: 1.4450\n",
            "Step 338/1000 | Loss: 1.8316\n",
            "Step 339/1000 | Loss: 1.6559\n",
            "Step 340/1000 | Loss: 2.0324\n",
            "Step 341/1000 | Loss: 1.6831\n",
            "Step 342/1000 | Loss: 1.8656\n",
            "Step 343/1000 | Loss: 1.6425\n",
            "Step 344/1000 | Loss: 1.9656\n",
            "Step 345/1000 | Loss: 2.1421\n",
            "Step 346/1000 | Loss: 2.0657\n",
            "Step 347/1000 | Loss: 2.0284\n",
            "Step 348/1000 | Loss: 1.7331\n",
            "Step 349/1000 | Loss: 1.9471\n",
            "Step 350/1000 | Loss: 1.8519\n",
            "Step 351/1000 | Loss: 1.9135\n",
            "Step 352/1000 | Loss: 1.6881\n",
            "Step 353/1000 | Loss: 1.7043\n",
            "Step 354/1000 | Loss: 1.6961\n",
            "Step 355/1000 | Loss: 1.8897\n",
            "Step 356/1000 | Loss: 2.1247\n",
            "Step 357/1000 | Loss: 2.0766\n",
            "Step 358/1000 | Loss: 2.0991\n",
            "Step 359/1000 | Loss: 2.0502\n",
            "Step 360/1000 | Loss: 2.1364\n",
            "Step 361/1000 | Loss: 2.0054\n",
            "Step 362/1000 | Loss: 1.8867\n",
            "Step 363/1000 | Loss: 2.0497\n",
            "Step 364/1000 | Loss: 2.1465\n",
            "Step 365/1000 | Loss: 2.0654\n",
            "Step 366/1000 | Loss: 2.1627\n",
            "Step 367/1000 | Loss: 1.9996\n",
            "Step 368/1000 | Loss: 1.5999\n",
            "Step 369/1000 | Loss: 1.3372\n",
            "Step 370/1000 | Loss: 1.7992\n",
            "Step 371/1000 | Loss: 2.0772\n",
            "Step 372/1000 | Loss: 2.0350\n",
            "Step 373/1000 | Loss: 1.9441\n",
            "Step 374/1000 | Loss: 1.6842\n",
            "Step 375/1000 | Loss: 1.6949\n",
            "Step 376/1000 | Loss: 1.9149\n",
            "Step 377/1000 | Loss: 1.6380\n",
            "Step 378/1000 | Loss: 1.9489\n",
            "Step 379/1000 | Loss: 1.8604\n",
            "Step 380/1000 | Loss: 1.7580\n",
            "Step 381/1000 | Loss: 1.8128\n",
            "Step 382/1000 | Loss: 1.6842\n",
            "Step 383/1000 | Loss: 1.7978\n",
            "Step 384/1000 | Loss: 1.9367\n",
            "Step 385/1000 | Loss: 1.6235\n",
            "Step 386/1000 | Loss: 1.4208\n",
            "Step 387/1000 | Loss: 1.7891\n",
            "Step 388/1000 | Loss: 1.8925\n",
            "Step 389/1000 | Loss: 2.1073\n",
            "Step 390/1000 | Loss: 1.8237\n",
            "Step 391/1000 | Loss: 1.9547\n",
            "Step 392/1000 | Loss: 1.8645\n",
            "Step 393/1000 | Loss: 2.0417\n",
            "Step 394/1000 | Loss: 1.7723\n",
            "Step 395/1000 | Loss: 1.8872\n",
            "Step 396/1000 | Loss: 1.9281\n",
            "Step 397/1000 | Loss: 1.9809\n",
            "Step 398/1000 | Loss: 2.0409\n",
            "Step 399/1000 | Loss: 1.8079\n",
            "Step 400/1000 | Loss: 1.8037\n",
            "Step 401/1000 | Loss: 1.8686\n",
            "Step 402/1000 | Loss: 2.1618\n",
            "Step 403/1000 | Loss: 2.0262\n",
            "Step 404/1000 | Loss: 2.1901\n",
            "Step 405/1000 | Loss: 2.1778\n",
            "Step 406/1000 | Loss: 2.2987\n",
            "Step 407/1000 | Loss: 2.3733\n",
            "Step 408/1000 | Loss: 2.2949\n",
            "Step 409/1000 | Loss: 2.0559\n",
            "Step 410/1000 | Loss: 2.2217\n",
            "Step 411/1000 | Loss: 2.1639\n",
            "Step 412/1000 | Loss: 2.3614\n",
            "Step 413/1000 | Loss: 1.7356\n",
            "Step 414/1000 | Loss: 1.9496\n",
            "Step 415/1000 | Loss: 1.9613\n",
            "Step 416/1000 | Loss: 1.9470\n",
            "Step 417/1000 | Loss: 2.1762\n",
            "Step 418/1000 | Loss: 1.9066\n",
            "Step 419/1000 | Loss: 2.0137\n",
            "Step 420/1000 | Loss: 2.1746\n",
            "Step 421/1000 | Loss: 2.0826\n",
            "Step 422/1000 | Loss: 2.0767\n",
            "Step 423/1000 | Loss: 2.0996\n",
            "Step 424/1000 | Loss: 2.3659\n",
            "Step 425/1000 | Loss: 2.2159\n",
            "Step 426/1000 | Loss: 1.9094\n",
            "Step 427/1000 | Loss: 1.9264\n",
            "Step 428/1000 | Loss: 2.1098\n",
            "Step 429/1000 | Loss: 1.9759\n",
            "Step 430/1000 | Loss: 1.8375\n",
            "Step 431/1000 | Loss: 2.0519\n",
            "Step 432/1000 | Loss: 2.0712\n",
            "Step 433/1000 | Loss: 1.9003\n",
            "Step 434/1000 | Loss: 1.8489\n",
            "Step 435/1000 | Loss: 1.7365\n",
            "Step 436/1000 | Loss: 1.7590\n",
            "Step 437/1000 | Loss: 1.8255\n",
            "Step 438/1000 | Loss: 1.7637\n",
            "Step 439/1000 | Loss: 1.8836\n",
            "Step 440/1000 | Loss: 1.8660\n",
            "Step 441/1000 | Loss: 1.7068\n",
            "Step 442/1000 | Loss: 1.8629\n",
            "Step 443/1000 | Loss: 1.8098\n",
            "Step 444/1000 | Loss: 1.9063\n",
            "Step 445/1000 | Loss: 2.1133\n",
            "Step 446/1000 | Loss: 2.0861\n",
            "Step 447/1000 | Loss: 1.8304\n",
            "Step 448/1000 | Loss: 1.8465\n",
            "Step 449/1000 | Loss: 1.9381\n",
            "Step 450/1000 | Loss: 2.2102\n",
            "Step 451/1000 | Loss: 1.9090\n",
            "Step 452/1000 | Loss: 1.5957\n",
            "Step 453/1000 | Loss: 1.9983\n",
            "Step 454/1000 | Loss: 2.0697\n",
            "Step 455/1000 | Loss: 1.8475\n",
            "Step 456/1000 | Loss: 1.9027\n",
            "Step 457/1000 | Loss: 1.9599\n",
            "Step 458/1000 | Loss: 1.8020\n",
            "Step 459/1000 | Loss: 1.8339\n",
            "Step 460/1000 | Loss: 1.9401\n",
            "Step 461/1000 | Loss: 1.7605\n",
            "Step 462/1000 | Loss: 1.9445\n",
            "Step 463/1000 | Loss: 1.6169\n",
            "Step 464/1000 | Loss: 2.0221\n",
            "Step 465/1000 | Loss: 1.9610\n",
            "Step 466/1000 | Loss: 2.1815\n",
            "Step 467/1000 | Loss: 1.9100\n",
            "Step 468/1000 | Loss: 2.3833\n",
            "Step 469/1000 | Loss: 2.0340\n",
            "Step 470/1000 | Loss: 2.0316\n",
            "Step 471/1000 | Loss: 1.8238\n",
            "Step 472/1000 | Loss: 1.9481\n",
            "Step 473/1000 | Loss: 1.9248\n",
            "Step 474/1000 | Loss: 1.5815\n",
            "Step 475/1000 | Loss: 1.7271\n",
            "Step 476/1000 | Loss: 1.8642\n",
            "Step 477/1000 | Loss: 1.7908\n",
            "Step 478/1000 | Loss: 1.3029\n",
            "Step 479/1000 | Loss: 1.7772\n",
            "Step 480/1000 | Loss: 1.9662\n",
            "Step 481/1000 | Loss: 2.1578\n",
            "Step 482/1000 | Loss: 1.7504\n",
            "Step 483/1000 | Loss: 1.9035\n",
            "Step 484/1000 | Loss: 1.9122\n",
            "Step 485/1000 | Loss: 1.7085\n",
            "Step 486/1000 | Loss: 1.8911\n",
            "Step 487/1000 | Loss: 1.8605\n",
            "Step 488/1000 | Loss: 1.8493\n",
            "Step 489/1000 | Loss: 1.6018\n",
            "Step 490/1000 | Loss: 1.6252\n",
            "Step 491/1000 | Loss: 1.7530\n",
            "Step 492/1000 | Loss: 1.6508\n",
            "Step 493/1000 | Loss: 1.5906\n",
            "Step 494/1000 | Loss: 1.6366\n",
            "Step 495/1000 | Loss: 1.6542\n",
            "Step 496/1000 | Loss: 1.6351\n",
            "Step 497/1000 | Loss: 1.5656\n",
            "Step 498/1000 | Loss: 1.4909\n",
            "Step 499/1000 | Loss: 1.6312\n",
            "Step 500/1000 | Loss: 1.9688\n",
            "Step 501/1000 | Loss: 1.8922\n",
            "Step 502/1000 | Loss: 1.9241\n",
            "Step 503/1000 | Loss: 2.1347\n",
            "Step 504/1000 | Loss: 1.7301\n",
            "Step 505/1000 | Loss: 1.9905\n",
            "Step 506/1000 | Loss: 1.9968\n",
            "Step 507/1000 | Loss: 1.6781\n",
            "Step 508/1000 | Loss: 1.6696\n",
            "Step 509/1000 | Loss: 1.6834\n",
            "Step 510/1000 | Loss: 1.6832\n",
            "Step 511/1000 | Loss: 2.2227\n",
            "Step 512/1000 | Loss: 2.0427\n",
            "Step 513/1000 | Loss: 2.1746\n",
            "Step 514/1000 | Loss: 1.9998\n",
            "Step 515/1000 | Loss: 1.9685\n",
            "Step 516/1000 | Loss: 1.8394\n",
            "Step 517/1000 | Loss: 2.1064\n",
            "Step 518/1000 | Loss: 1.8591\n",
            "Step 519/1000 | Loss: 1.9625\n",
            "Step 520/1000 | Loss: 1.8484\n",
            "Step 521/1000 | Loss: 1.9062\n",
            "Step 522/1000 | Loss: 2.2860\n",
            "Step 523/1000 | Loss: 1.8490\n",
            "Step 524/1000 | Loss: 2.0764\n",
            "Step 525/1000 | Loss: 2.0405\n",
            "Step 526/1000 | Loss: 2.0425\n",
            "Step 527/1000 | Loss: 2.0374\n",
            "Step 528/1000 | Loss: 1.9527\n",
            "Step 529/1000 | Loss: 1.8090\n",
            "Step 530/1000 | Loss: 2.0843\n",
            "Step 531/1000 | Loss: 1.7208\n",
            "Step 532/1000 | Loss: 2.1599\n",
            "Step 533/1000 | Loss: 1.7463\n",
            "Step 534/1000 | Loss: 1.5546\n",
            "Step 535/1000 | Loss: 1.9143\n",
            "Step 536/1000 | Loss: 1.7448\n",
            "Step 537/1000 | Loss: 1.8901\n",
            "Step 538/1000 | Loss: 1.8205\n",
            "Step 539/1000 | Loss: 1.7232\n",
            "Step 540/1000 | Loss: 1.6997\n",
            "Step 541/1000 | Loss: 1.7651\n",
            "Step 542/1000 | Loss: 1.7789\n",
            "Step 543/1000 | Loss: 1.8122\n",
            "Step 544/1000 | Loss: 1.6546\n",
            "Step 545/1000 | Loss: 1.7848\n",
            "Step 546/1000 | Loss: 1.6606\n",
            "Step 547/1000 | Loss: 1.8432\n",
            "Step 548/1000 | Loss: 1.7898\n",
            "Step 549/1000 | Loss: 1.4757\n",
            "Step 550/1000 | Loss: 1.9234\n",
            "Step 551/1000 | Loss: 1.9124\n",
            "Step 552/1000 | Loss: 1.7591\n",
            "Step 553/1000 | Loss: 1.7913\n",
            "Step 554/1000 | Loss: 1.9334\n",
            "Step 555/1000 | Loss: 1.8834\n",
            "Step 556/1000 | Loss: 1.7406\n",
            "Step 557/1000 | Loss: 2.0225\n",
            "Step 558/1000 | Loss: 1.6390\n",
            "Step 559/1000 | Loss: 1.8498\n",
            "Step 560/1000 | Loss: 1.3243\n",
            "Step 561/1000 | Loss: 1.3100\n",
            "Step 562/1000 | Loss: 1.5723\n",
            "Step 563/1000 | Loss: 1.8465\n",
            "Step 564/1000 | Loss: 1.5441\n",
            "Step 565/1000 | Loss: 1.5886\n",
            "Step 566/1000 | Loss: 1.6050\n",
            "Step 567/1000 | Loss: 1.6710\n",
            "Step 568/1000 | Loss: 1.7842\n",
            "Step 569/1000 | Loss: 1.6516\n",
            "Step 570/1000 | Loss: 1.6525\n",
            "Step 571/1000 | Loss: 1.6417\n",
            "Step 572/1000 | Loss: 1.7219\n",
            "Step 573/1000 | Loss: 1.7016\n",
            "Step 574/1000 | Loss: 1.7398\n",
            "Step 575/1000 | Loss: 1.6935\n",
            "Step 576/1000 | Loss: 1.4226\n",
            "Step 577/1000 | Loss: 1.6650\n",
            "Step 578/1000 | Loss: 1.3499\n",
            "Step 579/1000 | Loss: 1.4935\n",
            "Step 580/1000 | Loss: 1.4905\n",
            "Step 581/1000 | Loss: 1.5467\n",
            "Step 582/1000 | Loss: 1.5673\n",
            "Step 583/1000 | Loss: 1.7570\n",
            "Step 584/1000 | Loss: 1.9033\n",
            "Step 585/1000 | Loss: 1.6580\n",
            "Step 586/1000 | Loss: 1.8929\n",
            "Step 587/1000 | Loss: 1.8558\n",
            "Step 588/1000 | Loss: 1.8510\n",
            "Step 589/1000 | Loss: 1.7907\n",
            "Step 590/1000 | Loss: 1.6382\n",
            "Step 591/1000 | Loss: 1.9943\n",
            "Step 592/1000 | Loss: 1.8064\n",
            "Step 593/1000 | Loss: 1.5003\n",
            "Step 594/1000 | Loss: 1.6465\n",
            "Step 595/1000 | Loss: 1.3159\n",
            "Step 596/1000 | Loss: 1.5807\n",
            "Step 597/1000 | Loss: 1.5497\n",
            "Step 598/1000 | Loss: 1.6291\n",
            "Step 599/1000 | Loss: 2.0353\n",
            "Step 600/1000 | Loss: 1.9762\n",
            "Step 601/1000 | Loss: 1.7814\n",
            "Step 602/1000 | Loss: 1.7270\n",
            "Step 603/1000 | Loss: 1.7506\n",
            "Step 604/1000 | Loss: 1.9673\n",
            "Step 605/1000 | Loss: 2.0171\n",
            "Step 606/1000 | Loss: 1.9720\n",
            "Step 607/1000 | Loss: 1.9711\n",
            "Step 608/1000 | Loss: 1.8008\n",
            "Step 609/1000 | Loss: 2.0027\n",
            "Step 610/1000 | Loss: 1.8468\n",
            "Step 611/1000 | Loss: 1.7627\n",
            "Step 612/1000 | Loss: 1.9290\n",
            "Step 613/1000 | Loss: 1.9504\n",
            "Step 614/1000 | Loss: 1.8758\n",
            "Step 615/1000 | Loss: 1.9249\n",
            "Step 616/1000 | Loss: 1.9385\n",
            "Step 617/1000 | Loss: 1.8003\n",
            "Step 618/1000 | Loss: 1.7851\n",
            "Step 619/1000 | Loss: 1.7287\n",
            "Step 620/1000 | Loss: 1.8087\n",
            "Step 621/1000 | Loss: 1.9852\n",
            "Step 622/1000 | Loss: 2.1259\n",
            "Step 623/1000 | Loss: 2.0400\n",
            "Step 624/1000 | Loss: 1.7734\n",
            "Step 625/1000 | Loss: 1.8436\n",
            "Step 626/1000 | Loss: 1.7089\n",
            "Step 627/1000 | Loss: 1.9054\n",
            "Step 628/1000 | Loss: 1.6263\n",
            "Step 629/1000 | Loss: 1.7471\n",
            "Step 630/1000 | Loss: 1.8008\n",
            "Step 631/1000 | Loss: 1.7733\n",
            "Step 632/1000 | Loss: 1.7429\n",
            "Step 633/1000 | Loss: 1.8965\n",
            "Step 634/1000 | Loss: 1.5316\n",
            "Step 635/1000 | Loss: 1.4081\n",
            "Step 636/1000 | Loss: 1.4441\n",
            "Step 637/1000 | Loss: 1.4812\n",
            "Step 638/1000 | Loss: 2.1055\n",
            "Step 639/1000 | Loss: 1.9051\n",
            "Step 640/1000 | Loss: 1.5924\n",
            "Step 641/1000 | Loss: 2.0054\n",
            "Step 642/1000 | Loss: 1.7340\n",
            "Step 643/1000 | Loss: 1.9850\n",
            "Step 644/1000 | Loss: 1.8586\n",
            "Step 645/1000 | Loss: 1.9000\n",
            "Step 646/1000 | Loss: 1.8210\n",
            "Step 647/1000 | Loss: 2.2400\n",
            "Step 648/1000 | Loss: 1.9113\n",
            "Step 649/1000 | Loss: 1.7649\n",
            "Step 650/1000 | Loss: 2.0422\n",
            "Step 651/1000 | Loss: 1.8116\n",
            "Step 652/1000 | Loss: 1.6301\n",
            "Step 653/1000 | Loss: 2.1346\n",
            "Step 654/1000 | Loss: 1.8303\n",
            "Step 655/1000 | Loss: 1.7012\n",
            "Step 656/1000 | Loss: 1.6629\n",
            "Step 657/1000 | Loss: 1.7984\n",
            "Step 658/1000 | Loss: 1.7647\n",
            "Step 659/1000 | Loss: 1.7673\n",
            "Step 660/1000 | Loss: 1.7040\n",
            "Step 661/1000 | Loss: 2.0390\n",
            "Step 662/1000 | Loss: 1.8250\n",
            "Step 663/1000 | Loss: 1.6154\n",
            "Step 664/1000 | Loss: 1.7523\n",
            "Step 665/1000 | Loss: 1.8024\n",
            "Step 666/1000 | Loss: 1.7187\n",
            "Step 667/1000 | Loss: 1.4568\n",
            "Step 668/1000 | Loss: 1.7155\n",
            "Step 669/1000 | Loss: 1.5537\n",
            "Step 670/1000 | Loss: 1.9281\n",
            "Step 671/1000 | Loss: 1.6569\n",
            "Step 672/1000 | Loss: 1.8819\n",
            "Step 673/1000 | Loss: 1.6844\n",
            "Step 674/1000 | Loss: 1.9953\n",
            "Step 675/1000 | Loss: 2.0939\n",
            "Step 676/1000 | Loss: 1.9324\n",
            "Step 677/1000 | Loss: 1.8958\n",
            "Step 678/1000 | Loss: 1.5868\n",
            "Step 679/1000 | Loss: 1.8612\n",
            "Step 680/1000 | Loss: 1.8465\n",
            "Step 681/1000 | Loss: 1.7718\n",
            "Step 682/1000 | Loss: 1.6712\n",
            "Step 683/1000 | Loss: 1.7200\n",
            "Step 684/1000 | Loss: 1.7457\n",
            "Step 685/1000 | Loss: 1.8173\n",
            "Step 686/1000 | Loss: 2.0620\n",
            "Step 687/1000 | Loss: 1.9966\n",
            "Step 688/1000 | Loss: 1.8911\n",
            "Step 689/1000 | Loss: 1.9068\n",
            "Step 690/1000 | Loss: 1.9870\n",
            "Step 691/1000 | Loss: 1.8637\n",
            "Step 692/1000 | Loss: 1.7312\n",
            "Step 693/1000 | Loss: 2.0123\n",
            "Step 694/1000 | Loss: 2.1344\n",
            "Step 695/1000 | Loss: 1.9991\n",
            "Step 696/1000 | Loss: 2.1107\n",
            "Step 697/1000 | Loss: 2.0976\n",
            "Step 698/1000 | Loss: 1.6899\n",
            "Step 699/1000 | Loss: 1.3882\n",
            "Step 700/1000 | Loss: 1.8041\n",
            "Step 701/1000 | Loss: 2.0724\n",
            "Step 702/1000 | Loss: 1.9894\n",
            "Step 703/1000 | Loss: 1.8614\n",
            "Step 704/1000 | Loss: 1.7789\n",
            "Step 705/1000 | Loss: 1.6997\n",
            "Step 706/1000 | Loss: 1.9167\n",
            "Step 707/1000 | Loss: 1.6453\n",
            "Step 708/1000 | Loss: 1.8501\n",
            "Step 709/1000 | Loss: 1.8159\n",
            "Step 710/1000 | Loss: 1.7457\n",
            "Step 711/1000 | Loss: 1.8294\n",
            "Step 712/1000 | Loss: 1.6306\n",
            "Step 713/1000 | Loss: 1.8297\n",
            "Step 714/1000 | Loss: 1.9035\n",
            "Step 715/1000 | Loss: 1.6411\n",
            "Step 716/1000 | Loss: 1.3868\n",
            "Step 717/1000 | Loss: 1.6904\n",
            "Step 718/1000 | Loss: 1.8793\n",
            "Step 719/1000 | Loss: 2.0749\n",
            "Step 720/1000 | Loss: 1.8300\n",
            "Step 721/1000 | Loss: 2.1540\n",
            "Step 722/1000 | Loss: 1.8988\n",
            "Step 723/1000 | Loss: 1.9616\n",
            "Step 724/1000 | Loss: 1.7238\n",
            "Step 725/1000 | Loss: 1.7831\n",
            "Step 726/1000 | Loss: 1.7783\n",
            "Step 727/1000 | Loss: 1.8504\n",
            "Step 728/1000 | Loss: 2.0394\n",
            "Step 729/1000 | Loss: 1.8248\n",
            "Step 730/1000 | Loss: 1.8198\n",
            "Step 731/1000 | Loss: 1.8105\n",
            "Step 732/1000 | Loss: 2.1969\n",
            "Step 733/1000 | Loss: 1.9094\n",
            "Step 734/1000 | Loss: 2.0168\n",
            "Step 735/1000 | Loss: 2.0603\n",
            "Step 736/1000 | Loss: 2.2092\n",
            "Step 737/1000 | Loss: 2.3164\n",
            "Step 738/1000 | Loss: 2.1749\n",
            "Step 739/1000 | Loss: 2.0482\n",
            "Step 740/1000 | Loss: 2.1639\n",
            "Step 741/1000 | Loss: 2.1240\n",
            "Step 742/1000 | Loss: 2.3796\n",
            "Step 743/1000 | Loss: 1.8167\n",
            "Step 744/1000 | Loss: 1.9762\n",
            "Step 745/1000 | Loss: 1.9643\n",
            "Step 746/1000 | Loss: 1.9139\n",
            "Step 747/1000 | Loss: 2.1653\n",
            "Step 748/1000 | Loss: 1.8497\n",
            "Step 749/1000 | Loss: 1.8745\n",
            "Step 750/1000 | Loss: 2.0387\n",
            "Step 751/1000 | Loss: 2.0107\n",
            "Step 752/1000 | Loss: 2.0107\n",
            "Step 753/1000 | Loss: 2.0136\n",
            "Step 754/1000 | Loss: 2.3340\n",
            "Step 755/1000 | Loss: 2.1642\n",
            "Step 756/1000 | Loss: 1.7769\n",
            "Step 757/1000 | Loss: 1.8276\n",
            "Step 758/1000 | Loss: 2.0256\n",
            "Step 759/1000 | Loss: 1.9410\n",
            "Step 760/1000 | Loss: 1.8742\n",
            "Step 761/1000 | Loss: 2.0677\n",
            "Step 762/1000 | Loss: 2.0124\n",
            "Step 763/1000 | Loss: 1.8774\n",
            "Step 764/1000 | Loss: 1.8490\n",
            "Step 765/1000 | Loss: 1.6577\n",
            "Step 766/1000 | Loss: 1.7182\n",
            "Step 767/1000 | Loss: 1.7159\n",
            "Step 768/1000 | Loss: 1.7301\n",
            "Step 769/1000 | Loss: 1.7932\n",
            "Step 770/1000 | Loss: 1.7820\n",
            "Step 771/1000 | Loss: 1.7094\n",
            "Step 772/1000 | Loss: 1.8018\n",
            "Step 773/1000 | Loss: 1.8092\n",
            "Step 774/1000 | Loss: 1.8077\n",
            "Step 775/1000 | Loss: 1.9600\n",
            "Step 776/1000 | Loss: 2.1169\n",
            "Step 777/1000 | Loss: 1.8821\n",
            "Step 778/1000 | Loss: 1.7649\n",
            "Step 779/1000 | Loss: 1.8245\n",
            "Step 780/1000 | Loss: 2.0991\n",
            "Step 781/1000 | Loss: 1.8355\n",
            "Step 782/1000 | Loss: 1.5561\n",
            "Step 783/1000 | Loss: 1.8785\n",
            "Step 784/1000 | Loss: 1.9110\n",
            "Step 785/1000 | Loss: 1.7341\n",
            "Step 786/1000 | Loss: 1.8195\n",
            "Step 787/1000 | Loss: 1.8617\n",
            "Step 788/1000 | Loss: 1.6913\n",
            "Step 789/1000 | Loss: 1.8234\n",
            "Step 790/1000 | Loss: 1.8628\n",
            "Step 791/1000 | Loss: 1.6763\n",
            "Step 792/1000 | Loss: 2.0010\n",
            "Step 793/1000 | Loss: 1.5995\n",
            "Step 794/1000 | Loss: 2.1031\n",
            "Step 795/1000 | Loss: 1.9879\n",
            "Step 796/1000 | Loss: 2.1702\n",
            "Step 797/1000 | Loss: 1.7877\n",
            "Step 798/1000 | Loss: 2.2572\n",
            "Step 799/1000 | Loss: 2.0550\n",
            "Step 800/1000 | Loss: 1.8878\n",
            "Step 801/1000 | Loss: 1.8695\n",
            "Step 802/1000 | Loss: 1.9750\n",
            "Step 803/1000 | Loss: 1.9278\n",
            "Step 804/1000 | Loss: 1.5680\n",
            "Step 805/1000 | Loss: 1.7662\n",
            "Step 806/1000 | Loss: 1.9043\n",
            "Step 807/1000 | Loss: 1.8362\n",
            "Step 808/1000 | Loss: 1.2919\n",
            "Step 809/1000 | Loss: 1.7637\n",
            "Step 810/1000 | Loss: 1.9946\n",
            "Step 811/1000 | Loss: 2.0828\n",
            "Step 812/1000 | Loss: 1.6824\n",
            "Step 813/1000 | Loss: 1.8575\n",
            "Step 814/1000 | Loss: 1.8464\n",
            "Step 815/1000 | Loss: 1.6923\n",
            "Step 816/1000 | Loss: 1.8249\n",
            "Step 817/1000 | Loss: 1.7820\n",
            "Step 818/1000 | Loss: 1.8473\n",
            "Step 819/1000 | Loss: 1.5258\n",
            "Step 820/1000 | Loss: 1.6051\n",
            "Step 821/1000 | Loss: 1.7257\n",
            "Step 822/1000 | Loss: 1.6213\n",
            "Step 823/1000 | Loss: 1.5478\n",
            "Step 824/1000 | Loss: 1.4909\n",
            "Step 825/1000 | Loss: 1.5507\n",
            "Step 826/1000 | Loss: 1.5263\n",
            "Step 827/1000 | Loss: 1.4063\n",
            "Step 828/1000 | Loss: 1.4079\n",
            "Step 829/1000 | Loss: 1.5483\n",
            "Step 830/1000 | Loss: 1.8654\n",
            "Step 831/1000 | Loss: 1.9359\n",
            "Step 832/1000 | Loss: 1.9312\n",
            "Step 833/1000 | Loss: 2.1403\n",
            "Step 834/1000 | Loss: 1.9078\n",
            "Step 835/1000 | Loss: 1.9921\n",
            "Step 836/1000 | Loss: 2.0103\n",
            "Step 837/1000 | Loss: 1.6625\n",
            "Step 838/1000 | Loss: 1.5512\n",
            "Step 839/1000 | Loss: 1.6808\n",
            "Step 840/1000 | Loss: 1.7164\n",
            "Step 841/1000 | Loss: 2.1541\n",
            "Step 842/1000 | Loss: 2.0243\n",
            "Step 843/1000 | Loss: 2.1306\n",
            "Step 844/1000 | Loss: 1.8890\n",
            "Step 845/1000 | Loss: 1.8880\n",
            "Step 846/1000 | Loss: 1.8086\n",
            "Step 847/1000 | Loss: 2.1426\n",
            "Step 848/1000 | Loss: 1.8313\n",
            "Step 849/1000 | Loss: 1.9543\n",
            "Step 850/1000 | Loss: 1.9321\n",
            "Step 851/1000 | Loss: 1.9284\n",
            "Step 852/1000 | Loss: 2.2761\n",
            "Step 853/1000 | Loss: 1.8064\n",
            "Step 854/1000 | Loss: 2.0580\n",
            "Step 855/1000 | Loss: 1.9983\n",
            "Step 856/1000 | Loss: 1.9552\n",
            "Step 857/1000 | Loss: 1.9271\n",
            "Step 858/1000 | Loss: 1.8184\n",
            "Step 859/1000 | Loss: 1.8407\n",
            "Step 860/1000 | Loss: 1.9718\n",
            "Step 861/1000 | Loss: 1.6180\n",
            "Step 862/1000 | Loss: 2.0346\n",
            "Step 863/1000 | Loss: 1.6729\n",
            "Step 864/1000 | Loss: 1.5276\n",
            "Step 865/1000 | Loss: 1.8925\n",
            "Step 866/1000 | Loss: 1.7180\n",
            "Step 867/1000 | Loss: 1.7658\n",
            "Step 868/1000 | Loss: 1.7402\n",
            "Step 869/1000 | Loss: 1.7129\n",
            "Step 870/1000 | Loss: 1.6506\n",
            "Step 871/1000 | Loss: 1.8265\n",
            "Step 872/1000 | Loss: 1.6183\n",
            "Step 873/1000 | Loss: 1.7451\n",
            "Step 874/1000 | Loss: 1.5733\n",
            "Step 875/1000 | Loss: 1.7491\n",
            "Step 876/1000 | Loss: 1.6970\n",
            "Step 877/1000 | Loss: 1.7421\n",
            "Step 878/1000 | Loss: 1.7476\n",
            "Step 879/1000 | Loss: 1.5180\n",
            "Step 880/1000 | Loss: 1.9367\n",
            "Step 881/1000 | Loss: 1.8046\n",
            "Step 882/1000 | Loss: 1.6641\n",
            "Step 883/1000 | Loss: 1.6975\n",
            "Step 884/1000 | Loss: 1.9025\n",
            "Step 885/1000 | Loss: 1.8348\n",
            "Step 886/1000 | Loss: 1.6746\n",
            "Step 887/1000 | Loss: 1.9313\n",
            "Step 888/1000 | Loss: 1.6339\n",
            "Step 889/1000 | Loss: 1.7518\n",
            "Step 890/1000 | Loss: 1.3899\n",
            "Step 891/1000 | Loss: 1.2489\n",
            "Step 892/1000 | Loss: 1.5744\n",
            "Step 893/1000 | Loss: 1.7986\n",
            "Step 894/1000 | Loss: 1.5264\n",
            "Step 895/1000 | Loss: 1.5074\n",
            "Step 896/1000 | Loss: 1.5413\n",
            "Step 897/1000 | Loss: 1.5802\n",
            "Step 898/1000 | Loss: 1.7572\n",
            "Step 899/1000 | Loss: 1.5845\n",
            "Step 900/1000 | Loss: 1.6172\n",
            "Step 901/1000 | Loss: 1.5887\n",
            "Step 902/1000 | Loss: 1.5876\n",
            "Step 903/1000 | Loss: 1.6480\n",
            "Step 904/1000 | Loss: 1.6757\n",
            "Step 905/1000 | Loss: 1.6149\n",
            "Step 906/1000 | Loss: 1.4293\n",
            "Step 907/1000 | Loss: 1.6299\n",
            "Step 908/1000 | Loss: 1.2958\n",
            "Step 909/1000 | Loss: 1.4614\n",
            "Step 910/1000 | Loss: 1.4550\n",
            "Step 911/1000 | Loss: 1.5103\n",
            "Step 912/1000 | Loss: 1.4659\n",
            "Step 913/1000 | Loss: 1.7014\n",
            "Step 914/1000 | Loss: 1.9183\n",
            "Step 915/1000 | Loss: 1.5814\n",
            "Step 916/1000 | Loss: 1.8754\n",
            "Step 917/1000 | Loss: 1.7002\n",
            "Step 918/1000 | Loss: 1.6368\n",
            "Step 919/1000 | Loss: 1.6584\n",
            "Step 920/1000 | Loss: 1.4916\n",
            "Step 921/1000 | Loss: 1.9525\n",
            "Step 922/1000 | Loss: 1.7028\n",
            "Step 923/1000 | Loss: 1.5113\n",
            "Step 924/1000 | Loss: 1.6805\n",
            "Step 925/1000 | Loss: 1.2979\n",
            "Step 926/1000 | Loss: 1.5039\n",
            "Step 927/1000 | Loss: 1.5267\n",
            "Step 928/1000 | Loss: 1.6400\n",
            "Step 929/1000 | Loss: 2.0277\n",
            "Step 930/1000 | Loss: 1.9345\n",
            "Step 931/1000 | Loss: 1.7937\n",
            "Step 932/1000 | Loss: 1.7049\n",
            "Step 933/1000 | Loss: 1.6324\n",
            "Step 934/1000 | Loss: 1.9238\n",
            "Step 935/1000 | Loss: 1.9787\n",
            "Step 936/1000 | Loss: 1.9582\n",
            "Step 937/1000 | Loss: 1.9519\n",
            "Step 938/1000 | Loss: 1.7953\n",
            "Step 939/1000 | Loss: 2.0451\n",
            "Step 940/1000 | Loss: 1.8340\n",
            "Step 941/1000 | Loss: 1.7308\n",
            "Step 942/1000 | Loss: 2.0067\n",
            "Step 943/1000 | Loss: 1.9834\n",
            "Step 944/1000 | Loss: 1.8191\n",
            "Step 945/1000 | Loss: 1.9076\n",
            "Step 946/1000 | Loss: 2.0082\n",
            "Step 947/1000 | Loss: 1.7379\n",
            "Step 948/1000 | Loss: 1.8307\n",
            "Step 949/1000 | Loss: 1.6617\n",
            "Step 950/1000 | Loss: 1.8189\n",
            "Step 951/1000 | Loss: 1.9387\n",
            "Step 952/1000 | Loss: 2.0732\n",
            "Step 953/1000 | Loss: 2.0759\n",
            "Step 954/1000 | Loss: 1.7414\n",
            "Step 955/1000 | Loss: 1.8870\n",
            "Step 956/1000 | Loss: 1.6559\n",
            "Step 957/1000 | Loss: 1.9576\n",
            "Step 958/1000 | Loss: 1.6534\n",
            "Step 959/1000 | Loss: 1.8018\n",
            "Step 960/1000 | Loss: 1.8228\n",
            "Step 961/1000 | Loss: 1.7203\n",
            "Step 962/1000 | Loss: 1.6788\n",
            "Step 963/1000 | Loss: 1.7757\n",
            "Step 964/1000 | Loss: 1.4796\n",
            "Step 965/1000 | Loss: 1.3242\n",
            "Step 966/1000 | Loss: 1.4198\n",
            "Step 967/1000 | Loss: 1.4053\n",
            "Step 968/1000 | Loss: 2.0822\n",
            "Step 969/1000 | Loss: 1.8511\n",
            "Step 970/1000 | Loss: 1.5870\n",
            "Step 971/1000 | Loss: 1.9791\n",
            "Step 972/1000 | Loss: 1.6694\n",
            "Step 973/1000 | Loss: 1.8573\n",
            "Step 974/1000 | Loss: 1.8508\n",
            "Step 975/1000 | Loss: 1.7832\n",
            "Step 976/1000 | Loss: 1.7738\n",
            "Step 977/1000 | Loss: 2.1910\n",
            "Step 978/1000 | Loss: 1.7347\n",
            "Step 979/1000 | Loss: 1.6808\n",
            "Step 980/1000 | Loss: 1.9638\n",
            "Step 981/1000 | Loss: 1.7500\n",
            "Step 982/1000 | Loss: 1.5723\n",
            "Step 983/1000 | Loss: 2.0997\n",
            "Step 984/1000 | Loss: 1.8074\n",
            "Step 985/1000 | Loss: 1.7923\n",
            "Step 986/1000 | Loss: 1.5982\n",
            "Step 987/1000 | Loss: 1.7425\n",
            "Step 988/1000 | Loss: 1.7898\n",
            "Step 989/1000 | Loss: 1.7471\n",
            "Step 990/1000 | Loss: 1.6450\n",
            "Step 991/1000 | Loss: 2.0440\n",
            "Step 992/1000 | Loss: 1.7796\n",
            "Step 993/1000 | Loss: 1.6459\n",
            "Step 994/1000 | Loss: 1.6845\n",
            "Step 995/1000 | Loss: 1.6952\n",
            "Step 996/1000 | Loss: 1.6993\n",
            "Step 997/1000 | Loss: 1.4307\n",
            "Step 998/1000 | Loss: 1.7857\n",
            "Step 999/1000 | Loss: 1.5412\n",
            "Step 1000/1000 | Loss: 1.8502\n",
            "Epoch 17/20\n",
            "Step 1/1000 | Loss: 1.6278\n",
            "Step 2/1000 | Loss: 1.8285\n",
            "Step 3/1000 | Loss: 1.5447\n",
            "Step 4/1000 | Loss: 1.9166\n",
            "Step 5/1000 | Loss: 2.0112\n",
            "Step 6/1000 | Loss: 1.9768\n",
            "Step 7/1000 | Loss: 1.9670\n",
            "Step 8/1000 | Loss: 1.6653\n",
            "Step 9/1000 | Loss: 1.8179\n",
            "Step 10/1000 | Loss: 1.7743\n",
            "Step 11/1000 | Loss: 1.7299\n",
            "Step 12/1000 | Loss: 1.6095\n",
            "Step 13/1000 | Loss: 1.6678\n",
            "Step 14/1000 | Loss: 1.6803\n",
            "Step 15/1000 | Loss: 1.7942\n",
            "Step 16/1000 | Loss: 1.9632\n",
            "Step 17/1000 | Loss: 1.8701\n",
            "Step 18/1000 | Loss: 1.8683\n",
            "Step 19/1000 | Loss: 1.9858\n",
            "Step 20/1000 | Loss: 1.9754\n",
            "Step 21/1000 | Loss: 1.7638\n",
            "Step 22/1000 | Loss: 1.6803\n",
            "Step 23/1000 | Loss: 1.8961\n",
            "Step 24/1000 | Loss: 2.0808\n",
            "Step 25/1000 | Loss: 1.8983\n",
            "Step 26/1000 | Loss: 2.0267\n",
            "Step 27/1000 | Loss: 2.0127\n",
            "Step 28/1000 | Loss: 1.6321\n",
            "Step 29/1000 | Loss: 1.2700\n",
            "Step 30/1000 | Loss: 1.8914\n",
            "Step 31/1000 | Loss: 2.0636\n",
            "Step 32/1000 | Loss: 2.0415\n",
            "Step 33/1000 | Loss: 1.9373\n",
            "Step 34/1000 | Loss: 1.7049\n",
            "Step 35/1000 | Loss: 1.7457\n",
            "Step 36/1000 | Loss: 1.9578\n",
            "Step 37/1000 | Loss: 1.7005\n",
            "Step 38/1000 | Loss: 1.8841\n",
            "Step 39/1000 | Loss: 1.8285\n",
            "Step 40/1000 | Loss: 1.7227\n",
            "Step 41/1000 | Loss: 1.8607\n",
            "Step 42/1000 | Loss: 1.6330\n",
            "Step 43/1000 | Loss: 1.7524\n",
            "Step 44/1000 | Loss: 1.8994\n",
            "Step 45/1000 | Loss: 1.6621\n",
            "Step 46/1000 | Loss: 1.3565\n",
            "Step 47/1000 | Loss: 1.7069\n",
            "Step 48/1000 | Loss: 1.8495\n",
            "Step 49/1000 | Loss: 2.0444\n",
            "Step 50/1000 | Loss: 1.7571\n",
            "Step 51/1000 | Loss: 1.9914\n",
            "Step 52/1000 | Loss: 1.9534\n",
            "Step 53/1000 | Loss: 1.9353\n",
            "Step 54/1000 | Loss: 1.7334\n",
            "Step 55/1000 | Loss: 1.7476\n",
            "Step 56/1000 | Loss: 1.8454\n",
            "Step 57/1000 | Loss: 1.8575\n",
            "Step 58/1000 | Loss: 1.9463\n",
            "Step 59/1000 | Loss: 1.7193\n",
            "Step 60/1000 | Loss: 1.7206\n",
            "Step 61/1000 | Loss: 1.7300\n",
            "Step 62/1000 | Loss: 2.0795\n",
            "Step 63/1000 | Loss: 1.8931\n",
            "Step 64/1000 | Loss: 1.9999\n",
            "Step 65/1000 | Loss: 1.9531\n",
            "Step 66/1000 | Loss: 2.1490\n",
            "Step 67/1000 | Loss: 2.2367\n",
            "Step 68/1000 | Loss: 2.1249\n",
            "Step 69/1000 | Loss: 1.9975\n",
            "Step 70/1000 | Loss: 2.1284\n",
            "Step 71/1000 | Loss: 2.0814\n",
            "Step 72/1000 | Loss: 2.3063\n",
            "Step 73/1000 | Loss: 1.7366\n",
            "Step 74/1000 | Loss: 1.8321\n",
            "Step 75/1000 | Loss: 1.8569\n",
            "Step 76/1000 | Loss: 1.9013\n",
            "Step 77/1000 | Loss: 2.1307\n",
            "Step 78/1000 | Loss: 1.8863\n",
            "Step 79/1000 | Loss: 1.8556\n",
            "Step 80/1000 | Loss: 2.0701\n",
            "Step 81/1000 | Loss: 1.9507\n",
            "Step 82/1000 | Loss: 1.9364\n",
            "Step 83/1000 | Loss: 1.8985\n",
            "Step 84/1000 | Loss: 2.2569\n",
            "Step 85/1000 | Loss: 1.9539\n",
            "Step 86/1000 | Loss: 1.7230\n",
            "Step 87/1000 | Loss: 1.8070\n",
            "Step 88/1000 | Loss: 2.0268\n",
            "Step 89/1000 | Loss: 1.9217\n",
            "Step 90/1000 | Loss: 1.8331\n",
            "Step 91/1000 | Loss: 2.0181\n",
            "Step 92/1000 | Loss: 2.0282\n",
            "Step 93/1000 | Loss: 1.8663\n",
            "Step 94/1000 | Loss: 1.7680\n",
            "Step 95/1000 | Loss: 1.5705\n",
            "Step 96/1000 | Loss: 1.7299\n",
            "Step 97/1000 | Loss: 1.6647\n",
            "Step 98/1000 | Loss: 1.5976\n",
            "Step 99/1000 | Loss: 1.8005\n",
            "Step 100/1000 | Loss: 1.6811\n",
            "Step 101/1000 | Loss: 1.6701\n",
            "Step 102/1000 | Loss: 1.8006\n",
            "Step 103/1000 | Loss: 1.7520\n",
            "Step 104/1000 | Loss: 1.8053\n",
            "Step 105/1000 | Loss: 1.9312\n",
            "Step 106/1000 | Loss: 1.9925\n",
            "Step 107/1000 | Loss: 1.8169\n",
            "Step 108/1000 | Loss: 1.6914\n",
            "Step 109/1000 | Loss: 1.7712\n",
            "Step 110/1000 | Loss: 2.0401\n",
            "Step 111/1000 | Loss: 1.7136\n",
            "Step 112/1000 | Loss: 1.4986\n",
            "Step 113/1000 | Loss: 1.8256\n",
            "Step 114/1000 | Loss: 1.9324\n",
            "Step 115/1000 | Loss: 1.6955\n",
            "Step 116/1000 | Loss: 1.8199\n",
            "Step 117/1000 | Loss: 1.7577\n",
            "Step 118/1000 | Loss: 1.6598\n",
            "Step 119/1000 | Loss: 1.7407\n",
            "Step 120/1000 | Loss: 1.8025\n",
            "Step 121/1000 | Loss: 1.6816\n",
            "Step 122/1000 | Loss: 1.9020\n",
            "Step 123/1000 | Loss: 1.5380\n",
            "Step 124/1000 | Loss: 1.9372\n",
            "Step 125/1000 | Loss: 1.8461\n",
            "Step 126/1000 | Loss: 2.0704\n",
            "Step 127/1000 | Loss: 1.8504\n",
            "Step 128/1000 | Loss: 2.2252\n",
            "Step 129/1000 | Loss: 2.0350\n",
            "Step 130/1000 | Loss: 1.9566\n",
            "Step 131/1000 | Loss: 1.8314\n",
            "Step 132/1000 | Loss: 1.8831\n",
            "Step 133/1000 | Loss: 1.8859\n",
            "Step 134/1000 | Loss: 1.5386\n",
            "Step 135/1000 | Loss: 1.7137\n",
            "Step 136/1000 | Loss: 1.8558\n",
            "Step 137/1000 | Loss: 1.8160\n",
            "Step 138/1000 | Loss: 1.3285\n",
            "Step 139/1000 | Loss: 1.8528\n",
            "Step 140/1000 | Loss: 2.0140\n",
            "Step 141/1000 | Loss: 2.0949\n",
            "Step 142/1000 | Loss: 1.7103\n",
            "Step 143/1000 | Loss: 1.9447\n",
            "Step 144/1000 | Loss: 1.8967\n",
            "Step 145/1000 | Loss: 1.7092\n",
            "Step 146/1000 | Loss: 1.8956\n",
            "Step 147/1000 | Loss: 1.7465\n",
            "Step 148/1000 | Loss: 1.8278\n",
            "Step 149/1000 | Loss: 1.5189\n",
            "Step 150/1000 | Loss: 1.6388\n",
            "Step 151/1000 | Loss: 1.7151\n",
            "Step 152/1000 | Loss: 1.5338\n",
            "Step 153/1000 | Loss: 1.5904\n",
            "Step 154/1000 | Loss: 1.5390\n",
            "Step 155/1000 | Loss: 1.5506\n",
            "Step 156/1000 | Loss: 1.4802\n",
            "Step 157/1000 | Loss: 1.4009\n",
            "Step 158/1000 | Loss: 1.3555\n",
            "Step 159/1000 | Loss: 1.5726\n",
            "Step 160/1000 | Loss: 1.8906\n",
            "Step 161/1000 | Loss: 1.9111\n",
            "Step 162/1000 | Loss: 1.9649\n",
            "Step 163/1000 | Loss: 2.0975\n",
            "Step 164/1000 | Loss: 1.8222\n",
            "Step 165/1000 | Loss: 2.0050\n",
            "Step 166/1000 | Loss: 1.9432\n",
            "Step 167/1000 | Loss: 1.6447\n",
            "Step 168/1000 | Loss: 1.5826\n",
            "Step 169/1000 | Loss: 1.5710\n",
            "Step 170/1000 | Loss: 1.6138\n",
            "Step 171/1000 | Loss: 2.1197\n",
            "Step 172/1000 | Loss: 2.0458\n",
            "Step 173/1000 | Loss: 2.0902\n",
            "Step 174/1000 | Loss: 1.8558\n",
            "Step 175/1000 | Loss: 1.9050\n",
            "Step 176/1000 | Loss: 1.7265\n",
            "Step 177/1000 | Loss: 2.0225\n",
            "Step 178/1000 | Loss: 1.7729\n",
            "Step 179/1000 | Loss: 1.8732\n",
            "Step 180/1000 | Loss: 1.9081\n",
            "Step 181/1000 | Loss: 1.9063\n",
            "Step 182/1000 | Loss: 2.2695\n",
            "Step 183/1000 | Loss: 1.8117\n",
            "Step 184/1000 | Loss: 1.9508\n",
            "Step 185/1000 | Loss: 1.9453\n",
            "Step 186/1000 | Loss: 1.9194\n",
            "Step 187/1000 | Loss: 1.9795\n",
            "Step 188/1000 | Loss: 1.8441\n",
            "Step 189/1000 | Loss: 1.7398\n",
            "Step 190/1000 | Loss: 1.9528\n",
            "Step 191/1000 | Loss: 1.6222\n",
            "Step 192/1000 | Loss: 2.0395\n",
            "Step 193/1000 | Loss: 1.6314\n",
            "Step 194/1000 | Loss: 1.5418\n",
            "Step 195/1000 | Loss: 1.9357\n",
            "Step 196/1000 | Loss: 1.6517\n",
            "Step 197/1000 | Loss: 1.7270\n",
            "Step 198/1000 | Loss: 1.6344\n",
            "Step 199/1000 | Loss: 1.6863\n",
            "Step 200/1000 | Loss: 1.6058\n",
            "Step 201/1000 | Loss: 1.7451\n",
            "Step 202/1000 | Loss: 1.6448\n",
            "Step 203/1000 | Loss: 1.6604\n",
            "Step 204/1000 | Loss: 1.5310\n",
            "Step 205/1000 | Loss: 1.6663\n",
            "Step 206/1000 | Loss: 1.6047\n",
            "Step 207/1000 | Loss: 1.6548\n",
            "Step 208/1000 | Loss: 1.6147\n",
            "Step 209/1000 | Loss: 1.4465\n",
            "Step 210/1000 | Loss: 1.7947\n",
            "Step 211/1000 | Loss: 1.7017\n",
            "Step 212/1000 | Loss: 1.6700\n",
            "Step 213/1000 | Loss: 1.6109\n",
            "Step 214/1000 | Loss: 1.8780\n",
            "Step 215/1000 | Loss: 1.7249\n",
            "Step 216/1000 | Loss: 1.6626\n",
            "Step 217/1000 | Loss: 1.8093\n",
            "Step 218/1000 | Loss: 1.5578\n",
            "Step 219/1000 | Loss: 1.6609\n",
            "Step 220/1000 | Loss: 1.2797\n",
            "Step 221/1000 | Loss: 1.2640\n",
            "Step 222/1000 | Loss: 1.5451\n",
            "Step 223/1000 | Loss: 1.8289\n",
            "Step 224/1000 | Loss: 1.4942\n",
            "Step 225/1000 | Loss: 1.4946\n",
            "Step 226/1000 | Loss: 1.5484\n",
            "Step 227/1000 | Loss: 1.5897\n",
            "Step 228/1000 | Loss: 1.7517\n",
            "Step 229/1000 | Loss: 1.5729\n",
            "Step 230/1000 | Loss: 1.5414\n",
            "Step 231/1000 | Loss: 1.5608\n",
            "Step 232/1000 | Loss: 1.5752\n",
            "Step 233/1000 | Loss: 1.5667\n",
            "Step 234/1000 | Loss: 1.7014\n",
            "Step 235/1000 | Loss: 1.5893\n",
            "Step 236/1000 | Loss: 1.4503\n",
            "Step 237/1000 | Loss: 1.6001\n",
            "Step 238/1000 | Loss: 1.3350\n",
            "Step 239/1000 | Loss: 1.4893\n",
            "Step 240/1000 | Loss: 1.5135\n",
            "Step 241/1000 | Loss: 1.4757\n",
            "Step 242/1000 | Loss: 1.4669\n",
            "Step 243/1000 | Loss: 1.6130\n",
            "Step 244/1000 | Loss: 1.7885\n",
            "Step 245/1000 | Loss: 1.5432\n",
            "Step 246/1000 | Loss: 1.8102\n",
            "Step 247/1000 | Loss: 1.6773\n",
            "Step 248/1000 | Loss: 1.6404\n",
            "Step 249/1000 | Loss: 1.6011\n",
            "Step 250/1000 | Loss: 1.4839\n",
            "Step 251/1000 | Loss: 1.7809\n",
            "Step 252/1000 | Loss: 1.6362\n",
            "Step 253/1000 | Loss: 1.4334\n",
            "Step 254/1000 | Loss: 1.5939\n",
            "Step 255/1000 | Loss: 1.2871\n",
            "Step 256/1000 | Loss: 1.5152\n",
            "Step 257/1000 | Loss: 1.5009\n",
            "Step 258/1000 | Loss: 1.5623\n",
            "Step 259/1000 | Loss: 1.9742\n",
            "Step 260/1000 | Loss: 1.8974\n",
            "Step 261/1000 | Loss: 1.7501\n",
            "Step 262/1000 | Loss: 1.6507\n",
            "Step 263/1000 | Loss: 1.6601\n",
            "Step 264/1000 | Loss: 1.8823\n",
            "Step 265/1000 | Loss: 1.8368\n",
            "Step 266/1000 | Loss: 1.8034\n",
            "Step 267/1000 | Loss: 1.8283\n",
            "Step 268/1000 | Loss: 1.7348\n",
            "Step 269/1000 | Loss: 1.9428\n",
            "Step 270/1000 | Loss: 1.7396\n",
            "Step 271/1000 | Loss: 1.7049\n",
            "Step 272/1000 | Loss: 1.8845\n",
            "Step 273/1000 | Loss: 1.8748\n",
            "Step 274/1000 | Loss: 1.7951\n",
            "Step 275/1000 | Loss: 1.9245\n",
            "Step 276/1000 | Loss: 1.9204\n",
            "Step 277/1000 | Loss: 1.7033\n",
            "Step 278/1000 | Loss: 1.7602\n",
            "Step 279/1000 | Loss: 1.6010\n",
            "Step 280/1000 | Loss: 1.7924\n",
            "Step 281/1000 | Loss: 1.8507\n",
            "Step 282/1000 | Loss: 2.0555\n",
            "Step 283/1000 | Loss: 1.9897\n",
            "Step 284/1000 | Loss: 1.6937\n",
            "Step 285/1000 | Loss: 1.8493\n",
            "Step 286/1000 | Loss: 1.6670\n",
            "Step 287/1000 | Loss: 1.9198\n",
            "Step 288/1000 | Loss: 1.6111\n",
            "Step 289/1000 | Loss: 1.7437\n",
            "Step 290/1000 | Loss: 1.7800\n",
            "Step 291/1000 | Loss: 1.6648\n",
            "Step 292/1000 | Loss: 1.6225\n",
            "Step 293/1000 | Loss: 1.7524\n",
            "Step 294/1000 | Loss: 1.4917\n",
            "Step 295/1000 | Loss: 1.3271\n",
            "Step 296/1000 | Loss: 1.3638\n",
            "Step 297/1000 | Loss: 1.3614\n",
            "Step 298/1000 | Loss: 1.9291\n",
            "Step 299/1000 | Loss: 1.6977\n",
            "Step 300/1000 | Loss: 1.5058\n",
            "Step 301/1000 | Loss: 1.9186\n",
            "Step 302/1000 | Loss: 1.6485\n",
            "Step 303/1000 | Loss: 1.8567\n",
            "Step 304/1000 | Loss: 1.7938\n",
            "Step 305/1000 | Loss: 1.7440\n",
            "Step 306/1000 | Loss: 1.7069\n",
            "Step 307/1000 | Loss: 2.2931\n",
            "Step 308/1000 | Loss: 1.6553\n",
            "Step 309/1000 | Loss: 1.6515\n",
            "Step 310/1000 | Loss: 1.8845\n",
            "Step 311/1000 | Loss: 1.6816\n",
            "Step 312/1000 | Loss: 1.4994\n",
            "Step 313/1000 | Loss: 2.0113\n",
            "Step 314/1000 | Loss: 1.6901\n",
            "Step 315/1000 | Loss: 1.6310\n",
            "Step 316/1000 | Loss: 1.5742\n",
            "Step 317/1000 | Loss: 1.8208\n",
            "Step 318/1000 | Loss: 1.7305\n",
            "Step 319/1000 | Loss: 1.6479\n",
            "Step 320/1000 | Loss: 1.6203\n",
            "Step 321/1000 | Loss: 1.9959\n",
            "Step 322/1000 | Loss: 1.7407\n",
            "Step 323/1000 | Loss: 1.5869\n",
            "Step 324/1000 | Loss: 1.6652\n",
            "Step 325/1000 | Loss: 1.6171\n",
            "Step 326/1000 | Loss: 1.5897\n",
            "Step 327/1000 | Loss: 1.4389\n",
            "Step 328/1000 | Loss: 1.6772\n",
            "Step 329/1000 | Loss: 1.5545\n",
            "Step 330/1000 | Loss: 1.8584\n",
            "Step 331/1000 | Loss: 1.6389\n",
            "Step 332/1000 | Loss: 1.8385\n",
            "Step 333/1000 | Loss: 1.5880\n",
            "Step 334/1000 | Loss: 1.9820\n",
            "Step 335/1000 | Loss: 2.0346\n",
            "Step 336/1000 | Loss: 1.9740\n",
            "Step 337/1000 | Loss: 1.8755\n",
            "Step 338/1000 | Loss: 1.5651\n",
            "Step 339/1000 | Loss: 1.7841\n",
            "Step 340/1000 | Loss: 1.6867\n",
            "Step 341/1000 | Loss: 1.6976\n",
            "Step 342/1000 | Loss: 1.5820\n",
            "Step 343/1000 | Loss: 1.6059\n",
            "Step 344/1000 | Loss: 1.5869\n",
            "Step 345/1000 | Loss: 1.7082\n",
            "Step 346/1000 | Loss: 1.9446\n",
            "Step 347/1000 | Loss: 1.8578\n",
            "Step 348/1000 | Loss: 1.8477\n",
            "Step 349/1000 | Loss: 1.9164\n",
            "Step 350/1000 | Loss: 1.8525\n",
            "Step 351/1000 | Loss: 1.7536\n",
            "Step 352/1000 | Loss: 1.6297\n",
            "Step 353/1000 | Loss: 1.8121\n",
            "Step 354/1000 | Loss: 1.9471\n",
            "Step 355/1000 | Loss: 1.7322\n",
            "Step 356/1000 | Loss: 1.8668\n",
            "Step 357/1000 | Loss: 1.8344\n",
            "Step 358/1000 | Loss: 1.5351\n",
            "Step 359/1000 | Loss: 1.2863\n",
            "Step 360/1000 | Loss: 1.8923\n",
            "Step 361/1000 | Loss: 2.0549\n",
            "Step 362/1000 | Loss: 2.0155\n",
            "Step 363/1000 | Loss: 1.8947\n",
            "Step 364/1000 | Loss: 1.6254\n",
            "Step 365/1000 | Loss: 1.6668\n",
            "Step 366/1000 | Loss: 1.8600\n",
            "Step 367/1000 | Loss: 1.5587\n",
            "Step 368/1000 | Loss: 1.8231\n",
            "Step 369/1000 | Loss: 1.7735\n",
            "Step 370/1000 | Loss: 1.7193\n",
            "Step 371/1000 | Loss: 1.8020\n",
            "Step 372/1000 | Loss: 1.6618\n",
            "Step 373/1000 | Loss: 1.7479\n",
            "Step 374/1000 | Loss: 1.8981\n",
            "Step 375/1000 | Loss: 1.5567\n",
            "Step 376/1000 | Loss: 1.3714\n",
            "Step 377/1000 | Loss: 1.7202\n",
            "Step 378/1000 | Loss: 1.7768\n",
            "Step 379/1000 | Loss: 1.8985\n",
            "Step 380/1000 | Loss: 1.7490\n",
            "Step 381/1000 | Loss: 1.9595\n",
            "Step 382/1000 | Loss: 1.9417\n",
            "Step 383/1000 | Loss: 1.9227\n",
            "Step 384/1000 | Loss: 1.7789\n",
            "Step 385/1000 | Loss: 1.6286\n",
            "Step 386/1000 | Loss: 1.8516\n",
            "Step 387/1000 | Loss: 1.8940\n",
            "Step 388/1000 | Loss: 2.0512\n",
            "Step 389/1000 | Loss: 1.7356\n",
            "Step 390/1000 | Loss: 1.7366\n",
            "Step 391/1000 | Loss: 1.7662\n",
            "Step 392/1000 | Loss: 2.0922\n",
            "Step 393/1000 | Loss: 1.8133\n",
            "Step 394/1000 | Loss: 1.9697\n",
            "Step 395/1000 | Loss: 1.9068\n",
            "Step 396/1000 | Loss: 2.1261\n",
            "Step 397/1000 | Loss: 2.2037\n",
            "Step 398/1000 | Loss: 2.1415\n",
            "Step 399/1000 | Loss: 1.9830\n",
            "Step 400/1000 | Loss: 2.0626\n",
            "Step 401/1000 | Loss: 2.0388\n",
            "Step 402/1000 | Loss: 2.2284\n",
            "Step 403/1000 | Loss: 1.7407\n",
            "Step 404/1000 | Loss: 1.9367\n",
            "Step 405/1000 | Loss: 1.8474\n",
            "Step 406/1000 | Loss: 1.8900\n",
            "Step 407/1000 | Loss: 2.0965\n",
            "Step 408/1000 | Loss: 1.8547\n",
            "Step 409/1000 | Loss: 1.8766\n",
            "Step 410/1000 | Loss: 2.1215\n",
            "Step 411/1000 | Loss: 2.0690\n",
            "Step 412/1000 | Loss: 1.9715\n",
            "Step 413/1000 | Loss: 1.9037\n",
            "Step 414/1000 | Loss: 2.4052\n",
            "Step 415/1000 | Loss: 2.1223\n",
            "Step 416/1000 | Loss: 1.7865\n",
            "Step 417/1000 | Loss: 1.7621\n",
            "Step 418/1000 | Loss: 1.9815\n",
            "Step 419/1000 | Loss: 1.8477\n",
            "Step 420/1000 | Loss: 1.7996\n",
            "Step 421/1000 | Loss: 2.0355\n",
            "Step 422/1000 | Loss: 1.9907\n",
            "Step 423/1000 | Loss: 1.8546\n",
            "Step 424/1000 | Loss: 1.8323\n",
            "Step 425/1000 | Loss: 1.6380\n",
            "Step 426/1000 | Loss: 1.6833\n",
            "Step 427/1000 | Loss: 1.7051\n",
            "Step 428/1000 | Loss: 1.6088\n",
            "Step 429/1000 | Loss: 1.8049\n",
            "Step 430/1000 | Loss: 1.6286\n",
            "Step 431/1000 | Loss: 1.6270\n",
            "Step 432/1000 | Loss: 1.7620\n",
            "Step 433/1000 | Loss: 1.7623\n",
            "Step 434/1000 | Loss: 1.8137\n",
            "Step 435/1000 | Loss: 1.9110\n",
            "Step 436/1000 | Loss: 2.0574\n",
            "Step 437/1000 | Loss: 1.7633\n",
            "Step 438/1000 | Loss: 1.7020\n",
            "Step 439/1000 | Loss: 1.7620\n",
            "Step 440/1000 | Loss: 2.0139\n",
            "Step 441/1000 | Loss: 1.7888\n",
            "Step 442/1000 | Loss: 1.5183\n",
            "Step 443/1000 | Loss: 1.7842\n",
            "Step 444/1000 | Loss: 1.9409\n",
            "Step 445/1000 | Loss: 1.6746\n",
            "Step 446/1000 | Loss: 1.6988\n",
            "Step 447/1000 | Loss: 1.7440\n",
            "Step 448/1000 | Loss: 1.6635\n",
            "Step 449/1000 | Loss: 1.7081\n",
            "Step 450/1000 | Loss: 1.7373\n",
            "Step 451/1000 | Loss: 1.7059\n",
            "Step 452/1000 | Loss: 1.9078\n",
            "Step 453/1000 | Loss: 1.5539\n",
            "Step 454/1000 | Loss: 1.9670\n",
            "Step 455/1000 | Loss: 1.8727\n",
            "Step 456/1000 | Loss: 1.9970\n",
            "Step 457/1000 | Loss: 1.7217\n",
            "Step 458/1000 | Loss: 2.1201\n",
            "Step 459/1000 | Loss: 1.9302\n",
            "Step 460/1000 | Loss: 1.8163\n",
            "Step 461/1000 | Loss: 1.8321\n",
            "Step 462/1000 | Loss: 1.9479\n",
            "Step 463/1000 | Loss: 1.8797\n",
            "Step 464/1000 | Loss: 1.5906\n",
            "Step 465/1000 | Loss: 1.7195\n",
            "Step 466/1000 | Loss: 1.8003\n",
            "Step 467/1000 | Loss: 1.7302\n",
            "Step 468/1000 | Loss: 1.2156\n",
            "Step 469/1000 | Loss: 1.7028\n",
            "Step 470/1000 | Loss: 1.9774\n",
            "Step 471/1000 | Loss: 1.9727\n",
            "Step 472/1000 | Loss: 1.6926\n",
            "Step 473/1000 | Loss: 1.8386\n",
            "Step 474/1000 | Loss: 1.8168\n",
            "Step 475/1000 | Loss: 1.7229\n",
            "Step 476/1000 | Loss: 1.9299\n",
            "Step 477/1000 | Loss: 1.8375\n",
            "Step 478/1000 | Loss: 1.7793\n",
            "Step 479/1000 | Loss: 1.4802\n",
            "Step 480/1000 | Loss: 1.6665\n",
            "Step 481/1000 | Loss: 1.7163\n",
            "Step 482/1000 | Loss: 1.4889\n",
            "Step 483/1000 | Loss: 1.5690\n",
            "Step 484/1000 | Loss: 1.4336\n",
            "Step 485/1000 | Loss: 1.5209\n",
            "Step 486/1000 | Loss: 1.4970\n",
            "Step 487/1000 | Loss: 1.4816\n",
            "Step 488/1000 | Loss: 1.3481\n",
            "Step 489/1000 | Loss: 1.5325\n",
            "Step 490/1000 | Loss: 1.7878\n",
            "Step 491/1000 | Loss: 1.7406\n",
            "Step 492/1000 | Loss: 1.8789\n",
            "Step 493/1000 | Loss: 1.9942\n",
            "Step 494/1000 | Loss: 1.6985\n",
            "Step 495/1000 | Loss: 1.8831\n",
            "Step 496/1000 | Loss: 1.8645\n",
            "Step 497/1000 | Loss: 1.5973\n",
            "Step 498/1000 | Loss: 1.5135\n",
            "Step 499/1000 | Loss: 1.6131\n",
            "Step 500/1000 | Loss: 1.6651\n",
            "Step 501/1000 | Loss: 2.0944\n",
            "Step 502/1000 | Loss: 2.0295\n",
            "Step 503/1000 | Loss: 2.0229\n",
            "Step 504/1000 | Loss: 1.7948\n",
            "Step 505/1000 | Loss: 1.9717\n",
            "Step 506/1000 | Loss: 1.8154\n",
            "Step 507/1000 | Loss: 2.0382\n",
            "Step 508/1000 | Loss: 1.7433\n",
            "Step 509/1000 | Loss: 1.8533\n",
            "Step 510/1000 | Loss: 1.8174\n",
            "Step 511/1000 | Loss: 1.8468\n",
            "Step 512/1000 | Loss: 2.2064\n",
            "Step 513/1000 | Loss: 1.8207\n",
            "Step 514/1000 | Loss: 2.0036\n",
            "Step 515/1000 | Loss: 1.9572\n",
            "Step 516/1000 | Loss: 1.8386\n",
            "Step 517/1000 | Loss: 1.8140\n",
            "Step 518/1000 | Loss: 1.7791\n",
            "Step 519/1000 | Loss: 1.6578\n",
            "Step 520/1000 | Loss: 1.8479\n",
            "Step 521/1000 | Loss: 1.5644\n",
            "Step 522/1000 | Loss: 2.0621\n",
            "Step 523/1000 | Loss: 1.7075\n",
            "Step 524/1000 | Loss: 1.5565\n",
            "Step 525/1000 | Loss: 1.9397\n",
            "Step 526/1000 | Loss: 1.6968\n",
            "Step 527/1000 | Loss: 1.7734\n",
            "Step 528/1000 | Loss: 1.6795\n",
            "Step 529/1000 | Loss: 1.6624\n",
            "Step 530/1000 | Loss: 1.5528\n",
            "Step 531/1000 | Loss: 1.7096\n",
            "Step 532/1000 | Loss: 1.6028\n",
            "Step 533/1000 | Loss: 1.6183\n",
            "Step 534/1000 | Loss: 1.4588\n",
            "Step 535/1000 | Loss: 1.6788\n",
            "Step 536/1000 | Loss: 1.6479\n",
            "Step 537/1000 | Loss: 1.6333\n",
            "Step 538/1000 | Loss: 1.6806\n",
            "Step 539/1000 | Loss: 1.4311\n",
            "Step 540/1000 | Loss: 1.8531\n",
            "Step 541/1000 | Loss: 1.7897\n",
            "Step 542/1000 | Loss: 1.6737\n",
            "Step 543/1000 | Loss: 1.6534\n",
            "Step 544/1000 | Loss: 1.7449\n",
            "Step 545/1000 | Loss: 1.6726\n",
            "Step 546/1000 | Loss: 1.5611\n",
            "Step 547/1000 | Loss: 1.7950\n",
            "Step 548/1000 | Loss: 1.4978\n",
            "Step 549/1000 | Loss: 1.6363\n",
            "Step 550/1000 | Loss: 1.2508\n",
            "Step 551/1000 | Loss: 1.1652\n",
            "Step 552/1000 | Loss: 1.4779\n",
            "Step 553/1000 | Loss: 1.7198\n",
            "Step 554/1000 | Loss: 1.4520\n",
            "Step 555/1000 | Loss: 1.5289\n",
            "Step 556/1000 | Loss: 1.4194\n",
            "Step 557/1000 | Loss: 1.4700\n",
            "Step 558/1000 | Loss: 1.7690\n",
            "Step 559/1000 | Loss: 1.4373\n",
            "Step 560/1000 | Loss: 1.5346\n",
            "Step 561/1000 | Loss: 1.5183\n",
            "Step 562/1000 | Loss: 1.5314\n",
            "Step 563/1000 | Loss: 1.5168\n",
            "Step 564/1000 | Loss: 1.6770\n",
            "Step 565/1000 | Loss: 1.6445\n",
            "Step 566/1000 | Loss: 1.4646\n",
            "Step 567/1000 | Loss: 1.6085\n",
            "Step 568/1000 | Loss: 1.3956\n",
            "Step 569/1000 | Loss: 1.4435\n",
            "Step 570/1000 | Loss: 1.4567\n",
            "Step 571/1000 | Loss: 1.4542\n",
            "Step 572/1000 | Loss: 1.5282\n",
            "Step 573/1000 | Loss: 1.6756\n",
            "Step 574/1000 | Loss: 1.8722\n",
            "Step 575/1000 | Loss: 1.5525\n",
            "Step 576/1000 | Loss: 1.8111\n",
            "Step 577/1000 | Loss: 1.5517\n",
            "Step 578/1000 | Loss: 1.5431\n",
            "Step 579/1000 | Loss: 1.5874\n",
            "Step 580/1000 | Loss: 1.5114\n",
            "Step 581/1000 | Loss: 1.7438\n",
            "Step 582/1000 | Loss: 1.6563\n",
            "Step 583/1000 | Loss: 1.3578\n",
            "Step 584/1000 | Loss: 1.4970\n",
            "Step 585/1000 | Loss: 1.2750\n",
            "Step 586/1000 | Loss: 1.4693\n",
            "Step 587/1000 | Loss: 1.4159\n",
            "Step 588/1000 | Loss: 1.5146\n",
            "Step 589/1000 | Loss: 1.8501\n",
            "Step 590/1000 | Loss: 1.8701\n",
            "Step 591/1000 | Loss: 1.7462\n",
            "Step 592/1000 | Loss: 1.5738\n",
            "Step 593/1000 | Loss: 1.6113\n",
            "Step 594/1000 | Loss: 1.8826\n",
            "Step 595/1000 | Loss: 1.7834\n",
            "Step 596/1000 | Loss: 1.8358\n",
            "Step 597/1000 | Loss: 1.8383\n",
            "Step 598/1000 | Loss: 1.6760\n",
            "Step 599/1000 | Loss: 1.7905\n",
            "Step 600/1000 | Loss: 1.7494\n",
            "Step 601/1000 | Loss: 1.6319\n",
            "Step 602/1000 | Loss: 1.8249\n",
            "Step 603/1000 | Loss: 1.7755\n",
            "Step 604/1000 | Loss: 1.7651\n",
            "Step 605/1000 | Loss: 1.7648\n",
            "Step 606/1000 | Loss: 1.8307\n",
            "Step 607/1000 | Loss: 1.7144\n",
            "Step 608/1000 | Loss: 1.7250\n",
            "Step 609/1000 | Loss: 1.5380\n",
            "Step 610/1000 | Loss: 1.5526\n",
            "Step 611/1000 | Loss: 1.7576\n",
            "Step 612/1000 | Loss: 1.8905\n",
            "Step 613/1000 | Loss: 1.8282\n",
            "Step 614/1000 | Loss: 1.5801\n",
            "Step 615/1000 | Loss: 1.7447\n",
            "Step 616/1000 | Loss: 1.6099\n",
            "Step 617/1000 | Loss: 1.8152\n",
            "Step 618/1000 | Loss: 1.5479\n",
            "Step 619/1000 | Loss: 1.6816\n",
            "Step 620/1000 | Loss: 1.7476\n",
            "Step 621/1000 | Loss: 1.6867\n",
            "Step 622/1000 | Loss: 1.5729\n",
            "Step 623/1000 | Loss: 1.7576\n",
            "Step 624/1000 | Loss: 1.4329\n",
            "Step 625/1000 | Loss: 1.3114\n",
            "Step 626/1000 | Loss: 1.3644\n",
            "Step 627/1000 | Loss: 1.4036\n",
            "Step 628/1000 | Loss: 1.9577\n",
            "Step 629/1000 | Loss: 1.7467\n",
            "Step 630/1000 | Loss: 1.6317\n",
            "Step 631/1000 | Loss: 1.9298\n",
            "Step 632/1000 | Loss: 1.5266\n",
            "Step 633/1000 | Loss: 1.8013\n",
            "Step 634/1000 | Loss: 1.7492\n",
            "Step 635/1000 | Loss: 1.7245\n",
            "Step 636/1000 | Loss: 1.7080\n",
            "Step 637/1000 | Loss: 2.0998\n",
            "Step 638/1000 | Loss: 1.5827\n",
            "Step 639/1000 | Loss: 1.5919\n",
            "Step 640/1000 | Loss: 1.8421\n",
            "Step 641/1000 | Loss: 1.5999\n",
            "Step 642/1000 | Loss: 1.4344\n",
            "Step 643/1000 | Loss: 1.9166\n",
            "Step 644/1000 | Loss: 1.7013\n",
            "Step 645/1000 | Loss: 1.6262\n",
            "Step 646/1000 | Loss: 1.4711\n",
            "Step 647/1000 | Loss: 1.8079\n",
            "Step 648/1000 | Loss: 1.6547\n",
            "Step 649/1000 | Loss: 1.6090\n",
            "Step 650/1000 | Loss: 1.6644\n",
            "Step 651/1000 | Loss: 2.0078\n",
            "Step 652/1000 | Loss: 1.6715\n",
            "Step 653/1000 | Loss: 1.5349\n",
            "Step 654/1000 | Loss: 1.7095\n",
            "Step 655/1000 | Loss: 1.6681\n",
            "Step 656/1000 | Loss: 1.6429\n",
            "Step 657/1000 | Loss: 1.4084\n",
            "Step 658/1000 | Loss: 1.6795\n",
            "Step 659/1000 | Loss: 1.4793\n",
            "Step 660/1000 | Loss: 1.7958\n",
            "Step 661/1000 | Loss: 1.5848\n",
            "Step 662/1000 | Loss: 1.7221\n",
            "Step 663/1000 | Loss: 1.5270\n",
            "Step 664/1000 | Loss: 1.8937\n",
            "Step 665/1000 | Loss: 2.1091\n",
            "Step 666/1000 | Loss: 1.9322\n",
            "Step 667/1000 | Loss: 1.8696\n",
            "Step 668/1000 | Loss: 1.6154\n",
            "Step 669/1000 | Loss: 1.8402\n",
            "Step 670/1000 | Loss: 1.6647\n",
            "Step 671/1000 | Loss: 1.6790\n",
            "Step 672/1000 | Loss: 1.5092\n",
            "Step 673/1000 | Loss: 1.5224\n",
            "Step 674/1000 | Loss: 1.5422\n",
            "Step 675/1000 | Loss: 1.6623\n",
            "Step 676/1000 | Loss: 1.9257\n",
            "Step 677/1000 | Loss: 1.8114\n",
            "Step 678/1000 | Loss: 1.7563\n",
            "Step 679/1000 | Loss: 1.8537\n",
            "Step 680/1000 | Loss: 1.8664\n",
            "Step 681/1000 | Loss: 1.7761\n",
            "Step 682/1000 | Loss: 1.6164\n",
            "Step 683/1000 | Loss: 1.7772\n",
            "Step 684/1000 | Loss: 1.9332\n",
            "Step 685/1000 | Loss: 1.6613\n",
            "Step 686/1000 | Loss: 1.8372\n",
            "Step 687/1000 | Loss: 1.8025\n",
            "Step 688/1000 | Loss: 1.4023\n",
            "Step 689/1000 | Loss: 1.1344\n",
            "Step 690/1000 | Loss: 1.7498\n",
            "Step 691/1000 | Loss: 1.9160\n",
            "Step 692/1000 | Loss: 1.9065\n",
            "Step 693/1000 | Loss: 1.7738\n",
            "Step 694/1000 | Loss: 1.5064\n",
            "Step 695/1000 | Loss: 1.6969\n",
            "Step 696/1000 | Loss: 1.8122\n",
            "Step 697/1000 | Loss: 1.5414\n",
            "Step 698/1000 | Loss: 1.7928\n",
            "Step 699/1000 | Loss: 1.7589\n",
            "Step 700/1000 | Loss: 1.6434\n",
            "Step 701/1000 | Loss: 1.7198\n",
            "Step 702/1000 | Loss: 1.5522\n",
            "Step 703/1000 | Loss: 1.7238\n",
            "Step 704/1000 | Loss: 1.8794\n",
            "Step 705/1000 | Loss: 1.5602\n",
            "Step 706/1000 | Loss: 1.2917\n",
            "Step 707/1000 | Loss: 1.6305\n",
            "Step 708/1000 | Loss: 1.7642\n",
            "Step 709/1000 | Loss: 1.9267\n",
            "Step 710/1000 | Loss: 1.7142\n",
            "Step 711/1000 | Loss: 1.8770\n",
            "Step 712/1000 | Loss: 1.8400\n",
            "Step 713/1000 | Loss: 1.8332\n",
            "Step 714/1000 | Loss: 1.6904\n",
            "Step 715/1000 | Loss: 1.5997\n",
            "Step 716/1000 | Loss: 1.7459\n",
            "Step 717/1000 | Loss: 1.8196\n",
            "Step 718/1000 | Loss: 1.9473\n",
            "Step 719/1000 | Loss: 1.6631\n",
            "Step 720/1000 | Loss: 1.6835\n",
            "Step 721/1000 | Loss: 1.6847\n",
            "Step 722/1000 | Loss: 2.0595\n",
            "Step 723/1000 | Loss: 1.7657\n",
            "Step 724/1000 | Loss: 1.8991\n",
            "Step 725/1000 | Loss: 1.7673\n",
            "Step 726/1000 | Loss: 2.0056\n",
            "Step 727/1000 | Loss: 2.1119\n",
            "Step 728/1000 | Loss: 2.0708\n",
            "Step 729/1000 | Loss: 1.8878\n",
            "Step 730/1000 | Loss: 2.1232\n",
            "Step 731/1000 | Loss: 2.0797\n",
            "Step 732/1000 | Loss: 2.2661\n",
            "Step 733/1000 | Loss: 1.6439\n",
            "Step 734/1000 | Loss: 1.8414\n",
            "Step 735/1000 | Loss: 1.8199\n",
            "Step 736/1000 | Loss: 1.8728\n",
            "Step 737/1000 | Loss: 2.0798\n",
            "Step 738/1000 | Loss: 1.8089\n",
            "Step 739/1000 | Loss: 1.7803\n",
            "Step 740/1000 | Loss: 2.0753\n",
            "Step 741/1000 | Loss: 1.8962\n",
            "Step 742/1000 | Loss: 1.9258\n",
            "Step 743/1000 | Loss: 1.9015\n",
            "Step 744/1000 | Loss: 2.3457\n",
            "Step 745/1000 | Loss: 1.9676\n",
            "Step 746/1000 | Loss: 1.7347\n",
            "Step 747/1000 | Loss: 1.7682\n",
            "Step 748/1000 | Loss: 2.0475\n",
            "Step 749/1000 | Loss: 1.8534\n",
            "Step 750/1000 | Loss: 1.7558\n",
            "Step 751/1000 | Loss: 1.9559\n",
            "Step 752/1000 | Loss: 1.9383\n",
            "Step 753/1000 | Loss: 1.8233\n",
            "Step 754/1000 | Loss: 1.7077\n",
            "Step 755/1000 | Loss: 1.6522\n",
            "Step 756/1000 | Loss: 1.6596\n",
            "Step 757/1000 | Loss: 1.7398\n",
            "Step 758/1000 | Loss: 1.5829\n",
            "Step 759/1000 | Loss: 1.6831\n",
            "Step 760/1000 | Loss: 1.6807\n",
            "Step 761/1000 | Loss: 1.6276\n",
            "Step 762/1000 | Loss: 1.7730\n",
            "Step 763/1000 | Loss: 1.7668\n",
            "Step 764/1000 | Loss: 1.7695\n",
            "Step 765/1000 | Loss: 1.8945\n",
            "Step 766/1000 | Loss: 2.1322\n",
            "Step 767/1000 | Loss: 1.7692\n",
            "Step 768/1000 | Loss: 1.6836\n",
            "Step 769/1000 | Loss: 1.8024\n",
            "Step 770/1000 | Loss: 1.9753\n",
            "Step 771/1000 | Loss: 1.7763\n",
            "Step 772/1000 | Loss: 1.5104\n",
            "Step 773/1000 | Loss: 1.8336\n",
            "Step 774/1000 | Loss: 1.8134\n",
            "Step 775/1000 | Loss: 1.7356\n",
            "Step 776/1000 | Loss: 1.7640\n",
            "Step 777/1000 | Loss: 1.6984\n",
            "Step 778/1000 | Loss: 1.5609\n",
            "Step 779/1000 | Loss: 1.6061\n",
            "Step 780/1000 | Loss: 1.6682\n",
            "Step 781/1000 | Loss: 1.5736\n",
            "Step 782/1000 | Loss: 1.8634\n",
            "Step 783/1000 | Loss: 1.5623\n",
            "Step 784/1000 | Loss: 1.8932\n",
            "Step 785/1000 | Loss: 1.8099\n",
            "Step 786/1000 | Loss: 2.0818\n",
            "Step 787/1000 | Loss: 1.7359\n",
            "Step 788/1000 | Loss: 2.1868\n",
            "Step 789/1000 | Loss: 1.9730\n",
            "Step 790/1000 | Loss: 1.8222\n",
            "Step 791/1000 | Loss: 1.7698\n",
            "Step 792/1000 | Loss: 1.8845\n",
            "Step 793/1000 | Loss: 1.7096\n",
            "Step 794/1000 | Loss: 1.4881\n",
            "Step 795/1000 | Loss: 1.6778\n",
            "Step 796/1000 | Loss: 1.7023\n",
            "Step 797/1000 | Loss: 1.7212\n",
            "Step 798/1000 | Loss: 1.2839\n",
            "Step 799/1000 | Loss: 1.6802\n",
            "Step 800/1000 | Loss: 1.9130\n",
            "Step 801/1000 | Loss: 1.9598\n",
            "Step 802/1000 | Loss: 1.6147\n",
            "Step 803/1000 | Loss: 1.7435\n",
            "Step 804/1000 | Loss: 1.7120\n",
            "Step 805/1000 | Loss: 1.5847\n",
            "Step 806/1000 | Loss: 1.7907\n",
            "Step 807/1000 | Loss: 1.7536\n",
            "Step 808/1000 | Loss: 1.6944\n",
            "Step 809/1000 | Loss: 1.3531\n",
            "Step 810/1000 | Loss: 1.5488\n",
            "Step 811/1000 | Loss: 1.6060\n",
            "Step 812/1000 | Loss: 1.5055\n",
            "Step 813/1000 | Loss: 1.5175\n",
            "Step 814/1000 | Loss: 1.4638\n",
            "Step 815/1000 | Loss: 1.5288\n",
            "Step 816/1000 | Loss: 1.4837\n",
            "Step 817/1000 | Loss: 1.3538\n",
            "Step 818/1000 | Loss: 1.3234\n",
            "Step 819/1000 | Loss: 1.4757\n",
            "Step 820/1000 | Loss: 1.7623\n",
            "Step 821/1000 | Loss: 1.7519\n",
            "Step 822/1000 | Loss: 1.7591\n",
            "Step 823/1000 | Loss: 1.8253\n",
            "Step 824/1000 | Loss: 1.7363\n",
            "Step 825/1000 | Loss: 1.8403\n",
            "Step 826/1000 | Loss: 1.8237\n",
            "Step 827/1000 | Loss: 1.5180\n",
            "Step 828/1000 | Loss: 1.4606\n",
            "Step 829/1000 | Loss: 1.4680\n",
            "Step 830/1000 | Loss: 1.5731\n",
            "Step 831/1000 | Loss: 1.9908\n",
            "Step 832/1000 | Loss: 1.8869\n",
            "Step 833/1000 | Loss: 1.9394\n",
            "Step 834/1000 | Loss: 1.7581\n",
            "Step 835/1000 | Loss: 1.8197\n",
            "Step 836/1000 | Loss: 1.6760\n",
            "Step 837/1000 | Loss: 1.9138\n",
            "Step 838/1000 | Loss: 1.7303\n",
            "Step 839/1000 | Loss: 1.7885\n",
            "Step 840/1000 | Loss: 1.7652\n",
            "Step 841/1000 | Loss: 1.8360\n",
            "Step 842/1000 | Loss: 2.2566\n",
            "Step 843/1000 | Loss: 1.8494\n",
            "Step 844/1000 | Loss: 1.9089\n",
            "Step 845/1000 | Loss: 1.9808\n",
            "Step 846/1000 | Loss: 1.8844\n",
            "Step 847/1000 | Loss: 1.7937\n",
            "Step 848/1000 | Loss: 1.8181\n",
            "Step 849/1000 | Loss: 1.7226\n",
            "Step 850/1000 | Loss: 1.8085\n",
            "Step 851/1000 | Loss: 1.4996\n",
            "Step 852/1000 | Loss: 1.9697\n",
            "Step 853/1000 | Loss: 1.5998\n",
            "Step 854/1000 | Loss: 1.5116\n",
            "Step 855/1000 | Loss: 1.8338\n",
            "Step 856/1000 | Loss: 1.6344\n",
            "Step 857/1000 | Loss: 1.6624\n",
            "Step 858/1000 | Loss: 1.6729\n",
            "Step 859/1000 | Loss: 1.6707\n",
            "Step 860/1000 | Loss: 1.5163\n",
            "Step 861/1000 | Loss: 1.6079\n",
            "Step 862/1000 | Loss: 1.5053\n",
            "Step 863/1000 | Loss: 1.5686\n",
            "Step 864/1000 | Loss: 1.4668\n",
            "Step 865/1000 | Loss: 1.5796\n",
            "Step 866/1000 | Loss: 1.6052\n",
            "Step 867/1000 | Loss: 1.6478\n",
            "Step 868/1000 | Loss: 1.6026\n",
            "Step 869/1000 | Loss: 1.4016\n",
            "Step 870/1000 | Loss: 1.8686\n",
            "Step 871/1000 | Loss: 1.7418\n",
            "Step 872/1000 | Loss: 1.6011\n",
            "Step 873/1000 | Loss: 1.6902\n",
            "Step 874/1000 | Loss: 1.8251\n",
            "Step 875/1000 | Loss: 1.7991\n",
            "Step 876/1000 | Loss: 1.6602\n",
            "Step 877/1000 | Loss: 1.8491\n",
            "Step 878/1000 | Loss: 1.4985\n",
            "Step 879/1000 | Loss: 1.6323\n",
            "Step 880/1000 | Loss: 1.2331\n",
            "Step 881/1000 | Loss: 1.1650\n",
            "Step 882/1000 | Loss: 1.4314\n",
            "Step 883/1000 | Loss: 1.7348\n",
            "Step 884/1000 | Loss: 1.4031\n",
            "Step 885/1000 | Loss: 1.4719\n",
            "Step 886/1000 | Loss: 1.4915\n",
            "Step 887/1000 | Loss: 1.5087\n",
            "Step 888/1000 | Loss: 1.7399\n",
            "Step 889/1000 | Loss: 1.4452\n",
            "Step 890/1000 | Loss: 1.5607\n",
            "Step 891/1000 | Loss: 1.5962\n",
            "Step 892/1000 | Loss: 1.5419\n",
            "Step 893/1000 | Loss: 1.5617\n",
            "Step 894/1000 | Loss: 1.6625\n",
            "Step 895/1000 | Loss: 1.6046\n",
            "Step 896/1000 | Loss: 1.4300\n",
            "Step 897/1000 | Loss: 1.5091\n",
            "Step 898/1000 | Loss: 1.2785\n",
            "Step 899/1000 | Loss: 1.4930\n",
            "Step 900/1000 | Loss: 1.5441\n",
            "Step 901/1000 | Loss: 1.5134\n",
            "Step 902/1000 | Loss: 1.4803\n",
            "Step 903/1000 | Loss: 1.6634\n",
            "Step 904/1000 | Loss: 1.8465\n",
            "Step 905/1000 | Loss: 1.4815\n",
            "Step 906/1000 | Loss: 1.8041\n",
            "Step 907/1000 | Loss: 1.5678\n",
            "Step 908/1000 | Loss: 1.5692\n",
            "Step 909/1000 | Loss: 1.5439\n",
            "Step 910/1000 | Loss: 1.4273\n",
            "Step 911/1000 | Loss: 1.7376\n",
            "Step 912/1000 | Loss: 1.5726\n",
            "Step 913/1000 | Loss: 1.4155\n",
            "Step 914/1000 | Loss: 1.5090\n",
            "Step 915/1000 | Loss: 1.2690\n",
            "Step 916/1000 | Loss: 1.5252\n",
            "Step 917/1000 | Loss: 1.3802\n",
            "Step 918/1000 | Loss: 1.5599\n",
            "Step 919/1000 | Loss: 1.7952\n",
            "Step 920/1000 | Loss: 1.7848\n",
            "Step 921/1000 | Loss: 1.6935\n",
            "Step 922/1000 | Loss: 1.5646\n",
            "Step 923/1000 | Loss: 1.5726\n",
            "Step 924/1000 | Loss: 1.7241\n",
            "Step 925/1000 | Loss: 1.8366\n",
            "Step 926/1000 | Loss: 1.8058\n",
            "Step 927/1000 | Loss: 1.7483\n",
            "Step 928/1000 | Loss: 1.6530\n",
            "Step 929/1000 | Loss: 1.8559\n",
            "Step 930/1000 | Loss: 1.7097\n",
            "Step 931/1000 | Loss: 1.6070\n",
            "Step 932/1000 | Loss: 1.8222\n",
            "Step 933/1000 | Loss: 1.7514\n",
            "Step 934/1000 | Loss: 1.6848\n",
            "Step 935/1000 | Loss: 1.7003\n",
            "Step 936/1000 | Loss: 1.8103\n",
            "Step 937/1000 | Loss: 1.5996\n",
            "Step 938/1000 | Loss: 1.7448\n",
            "Step 939/1000 | Loss: 1.5082\n",
            "Step 940/1000 | Loss: 1.6136\n",
            "Step 941/1000 | Loss: 1.7561\n",
            "Step 942/1000 | Loss: 1.8456\n",
            "Step 943/1000 | Loss: 1.7677\n",
            "Step 944/1000 | Loss: 1.5482\n",
            "Step 945/1000 | Loss: 1.6975\n",
            "Step 946/1000 | Loss: 1.4639\n",
            "Step 947/1000 | Loss: 1.7444\n",
            "Step 948/1000 | Loss: 1.4265\n",
            "Step 949/1000 | Loss: 1.7049\n",
            "Step 950/1000 | Loss: 1.6621\n",
            "Step 951/1000 | Loss: 1.5340\n",
            "Step 952/1000 | Loss: 1.5635\n",
            "Step 953/1000 | Loss: 1.6850\n",
            "Step 954/1000 | Loss: 1.4121\n",
            "Step 955/1000 | Loss: 1.2320\n",
            "Step 956/1000 | Loss: 1.3633\n",
            "Step 957/1000 | Loss: 1.2953\n",
            "Step 958/1000 | Loss: 1.8367\n",
            "Step 959/1000 | Loss: 1.7991\n",
            "Step 960/1000 | Loss: 1.4835\n",
            "Step 961/1000 | Loss: 1.9921\n",
            "Step 962/1000 | Loss: 1.6386\n",
            "Step 963/1000 | Loss: 1.7759\n",
            "Step 964/1000 | Loss: 1.7495\n",
            "Step 965/1000 | Loss: 1.5739\n",
            "Step 966/1000 | Loss: 1.6834\n",
            "Step 967/1000 | Loss: 2.1517\n",
            "Step 968/1000 | Loss: 1.5555\n",
            "Step 969/1000 | Loss: 1.5188\n",
            "Step 970/1000 | Loss: 1.7551\n",
            "Step 971/1000 | Loss: 1.5674\n",
            "Step 972/1000 | Loss: 1.3510\n",
            "Step 973/1000 | Loss: 1.8541\n",
            "Step 974/1000 | Loss: 1.6565\n",
            "Step 975/1000 | Loss: 1.4644\n",
            "Step 976/1000 | Loss: 1.4238\n",
            "Step 977/1000 | Loss: 1.6732\n",
            "Step 978/1000 | Loss: 1.5977\n",
            "Step 979/1000 | Loss: 1.6695\n",
            "Step 980/1000 | Loss: 1.5478\n",
            "Step 981/1000 | Loss: 1.9257\n",
            "Step 982/1000 | Loss: 1.6683\n",
            "Step 983/1000 | Loss: 1.5956\n",
            "Step 984/1000 | Loss: 1.7066\n",
            "Step 985/1000 | Loss: 1.5882\n",
            "Step 986/1000 | Loss: 1.6244\n",
            "Step 987/1000 | Loss: 1.3277\n",
            "Step 988/1000 | Loss: 1.6593\n",
            "Step 989/1000 | Loss: 1.4525\n",
            "Step 990/1000 | Loss: 1.7016\n",
            "Step 991/1000 | Loss: 1.4927\n",
            "Step 992/1000 | Loss: 1.6245\n",
            "Step 993/1000 | Loss: 1.4911\n",
            "Step 994/1000 | Loss: 1.7806\n",
            "Step 995/1000 | Loss: 1.9279\n",
            "Step 996/1000 | Loss: 1.8691\n",
            "Step 997/1000 | Loss: 1.8116\n",
            "Step 998/1000 | Loss: 1.6062\n",
            "Step 999/1000 | Loss: 1.8103\n",
            "Step 1000/1000 | Loss: 1.6650\n",
            "Epoch 18/20\n",
            "Step 1/1000 | Loss: 1.6782\n",
            "Step 2/1000 | Loss: 1.5933\n",
            "Step 3/1000 | Loss: 1.5312\n",
            "Step 4/1000 | Loss: 1.5518\n",
            "Step 5/1000 | Loss: 1.6446\n",
            "Step 6/1000 | Loss: 1.8720\n",
            "Step 7/1000 | Loss: 1.7772\n",
            "Step 8/1000 | Loss: 1.7004\n",
            "Step 9/1000 | Loss: 1.7389\n",
            "Step 10/1000 | Loss: 1.7650\n",
            "Step 11/1000 | Loss: 1.6901\n",
            "Step 12/1000 | Loss: 1.4536\n",
            "Step 13/1000 | Loss: 1.7222\n",
            "Step 14/1000 | Loss: 1.8749\n",
            "Step 15/1000 | Loss: 1.7204\n",
            "Step 16/1000 | Loss: 1.8371\n",
            "Step 17/1000 | Loss: 1.7656\n",
            "Step 18/1000 | Loss: 1.4348\n",
            "Step 19/1000 | Loss: 1.1973\n",
            "Step 20/1000 | Loss: 1.7323\n",
            "Step 21/1000 | Loss: 1.8318\n",
            "Step 22/1000 | Loss: 1.8219\n",
            "Step 23/1000 | Loss: 1.7949\n",
            "Step 24/1000 | Loss: 1.4803\n",
            "Step 25/1000 | Loss: 1.5623\n",
            "Step 26/1000 | Loss: 1.7286\n",
            "Step 27/1000 | Loss: 1.4358\n",
            "Step 28/1000 | Loss: 1.7180\n",
            "Step 29/1000 | Loss: 1.7574\n",
            "Step 30/1000 | Loss: 1.6969\n",
            "Step 31/1000 | Loss: 1.7647\n",
            "Step 32/1000 | Loss: 1.4707\n",
            "Step 33/1000 | Loss: 1.5918\n",
            "Step 34/1000 | Loss: 1.7829\n",
            "Step 35/1000 | Loss: 1.4769\n",
            "Step 36/1000 | Loss: 1.2527\n",
            "Step 37/1000 | Loss: 1.5530\n",
            "Step 38/1000 | Loss: 1.7957\n",
            "Step 39/1000 | Loss: 1.9280\n",
            "Step 40/1000 | Loss: 1.6907\n",
            "Step 41/1000 | Loss: 1.9422\n",
            "Step 42/1000 | Loss: 1.8066\n",
            "Step 43/1000 | Loss: 1.8927\n",
            "Step 44/1000 | Loss: 1.6874\n",
            "Step 45/1000 | Loss: 1.6098\n",
            "Step 46/1000 | Loss: 1.6628\n",
            "Step 47/1000 | Loss: 1.6566\n",
            "Step 48/1000 | Loss: 1.7893\n",
            "Step 49/1000 | Loss: 1.5940\n",
            "Step 50/1000 | Loss: 1.6296\n",
            "Step 51/1000 | Loss: 1.6370\n",
            "Step 52/1000 | Loss: 2.0239\n",
            "Step 53/1000 | Loss: 1.7108\n",
            "Step 54/1000 | Loss: 1.8823\n",
            "Step 55/1000 | Loss: 1.7482\n",
            "Step 56/1000 | Loss: 1.9586\n",
            "Step 57/1000 | Loss: 2.1008\n",
            "Step 58/1000 | Loss: 2.0276\n",
            "Step 59/1000 | Loss: 1.8665\n",
            "Step 60/1000 | Loss: 1.9762\n",
            "Step 61/1000 | Loss: 1.8621\n",
            "Step 62/1000 | Loss: 2.1700\n",
            "Step 63/1000 | Loss: 1.6765\n",
            "Step 64/1000 | Loss: 1.8099\n",
            "Step 65/1000 | Loss: 1.7659\n",
            "Step 66/1000 | Loss: 1.8197\n",
            "Step 67/1000 | Loss: 2.0654\n",
            "Step 68/1000 | Loss: 1.7587\n",
            "Step 69/1000 | Loss: 1.9097\n",
            "Step 70/1000 | Loss: 2.0725\n",
            "Step 71/1000 | Loss: 1.9187\n",
            "Step 72/1000 | Loss: 1.9026\n",
            "Step 73/1000 | Loss: 1.9610\n",
            "Step 74/1000 | Loss: 2.2546\n",
            "Step 75/1000 | Loss: 1.9536\n",
            "Step 76/1000 | Loss: 1.7155\n",
            "Step 77/1000 | Loss: 1.7332\n",
            "Step 78/1000 | Loss: 1.9930\n",
            "Step 79/1000 | Loss: 1.8066\n",
            "Step 80/1000 | Loss: 1.7195\n",
            "Step 81/1000 | Loss: 1.9688\n",
            "Step 82/1000 | Loss: 2.0185\n",
            "Step 83/1000 | Loss: 1.8278\n",
            "Step 84/1000 | Loss: 1.5944\n",
            "Step 85/1000 | Loss: 1.6305\n",
            "Step 86/1000 | Loss: 1.6989\n",
            "Step 87/1000 | Loss: 1.6780\n",
            "Step 88/1000 | Loss: 1.6270\n",
            "Step 89/1000 | Loss: 1.7190\n",
            "Step 90/1000 | Loss: 1.6829\n",
            "Step 91/1000 | Loss: 1.6611\n",
            "Step 92/1000 | Loss: 1.7626\n",
            "Step 93/1000 | Loss: 1.6909\n",
            "Step 94/1000 | Loss: 1.7683\n",
            "Step 95/1000 | Loss: 1.9217\n",
            "Step 96/1000 | Loss: 2.0777\n",
            "Step 97/1000 | Loss: 1.8285\n",
            "Step 98/1000 | Loss: 1.7656\n",
            "Step 99/1000 | Loss: 1.7847\n",
            "Step 100/1000 | Loss: 1.9787\n",
            "Step 101/1000 | Loss: 1.7130\n",
            "Step 102/1000 | Loss: 1.5606\n",
            "Step 103/1000 | Loss: 1.8651\n",
            "Step 104/1000 | Loss: 1.8515\n",
            "Step 105/1000 | Loss: 1.6587\n",
            "Step 106/1000 | Loss: 1.7437\n",
            "Step 107/1000 | Loss: 1.6619\n",
            "Step 108/1000 | Loss: 1.5481\n",
            "Step 109/1000 | Loss: 1.5914\n",
            "Step 110/1000 | Loss: 1.6517\n",
            "Step 111/1000 | Loss: 1.5396\n",
            "Step 112/1000 | Loss: 1.7571\n",
            "Step 113/1000 | Loss: 1.5564\n",
            "Step 114/1000 | Loss: 1.9135\n",
            "Step 115/1000 | Loss: 1.8124\n",
            "Step 116/1000 | Loss: 2.0194\n",
            "Step 117/1000 | Loss: 1.7705\n",
            "Step 118/1000 | Loss: 2.1936\n",
            "Step 119/1000 | Loss: 1.8631\n",
            "Step 120/1000 | Loss: 1.8761\n",
            "Step 121/1000 | Loss: 1.8121\n",
            "Step 122/1000 | Loss: 1.8241\n",
            "Step 123/1000 | Loss: 1.6969\n",
            "Step 124/1000 | Loss: 1.4813\n",
            "Step 125/1000 | Loss: 1.6181\n",
            "Step 126/1000 | Loss: 1.7194\n",
            "Step 127/1000 | Loss: 1.7123\n",
            "Step 128/1000 | Loss: 1.2094\n",
            "Step 129/1000 | Loss: 1.6964\n",
            "Step 130/1000 | Loss: 1.8360\n",
            "Step 131/1000 | Loss: 1.8887\n",
            "Step 132/1000 | Loss: 1.6741\n",
            "Step 133/1000 | Loss: 1.6570\n",
            "Step 134/1000 | Loss: 1.6523\n",
            "Step 135/1000 | Loss: 1.5822\n",
            "Step 136/1000 | Loss: 1.7655\n",
            "Step 137/1000 | Loss: 1.7688\n",
            "Step 138/1000 | Loss: 1.7650\n",
            "Step 139/1000 | Loss: 1.5074\n",
            "Step 140/1000 | Loss: 1.5413\n",
            "Step 141/1000 | Loss: 1.6388\n",
            "Step 142/1000 | Loss: 1.5800\n",
            "Step 143/1000 | Loss: 1.5543\n",
            "Step 144/1000 | Loss: 1.4542\n",
            "Step 145/1000 | Loss: 1.5378\n",
            "Step 146/1000 | Loss: 1.4273\n",
            "Step 147/1000 | Loss: 1.3611\n",
            "Step 148/1000 | Loss: 1.3505\n",
            "Step 149/1000 | Loss: 1.4749\n",
            "Step 150/1000 | Loss: 1.6881\n",
            "Step 151/1000 | Loss: 1.7226\n",
            "Step 152/1000 | Loss: 1.8009\n",
            "Step 153/1000 | Loss: 1.8900\n",
            "Step 154/1000 | Loss: 1.5795\n",
            "Step 155/1000 | Loss: 1.8764\n",
            "Step 156/1000 | Loss: 1.7536\n",
            "Step 157/1000 | Loss: 1.4869\n",
            "Step 158/1000 | Loss: 1.4431\n",
            "Step 159/1000 | Loss: 1.4699\n",
            "Step 160/1000 | Loss: 1.5109\n",
            "Step 161/1000 | Loss: 1.9133\n",
            "Step 162/1000 | Loss: 1.8004\n",
            "Step 163/1000 | Loss: 1.9545\n",
            "Step 164/1000 | Loss: 1.6824\n",
            "Step 165/1000 | Loss: 1.7144\n",
            "Step 166/1000 | Loss: 1.5831\n",
            "Step 167/1000 | Loss: 1.8259\n",
            "Step 168/1000 | Loss: 1.6389\n",
            "Step 169/1000 | Loss: 1.6749\n",
            "Step 170/1000 | Loss: 1.7345\n",
            "Step 171/1000 | Loss: 1.7281\n",
            "Step 172/1000 | Loss: 2.0622\n",
            "Step 173/1000 | Loss: 1.6278\n",
            "Step 174/1000 | Loss: 1.9465\n",
            "Step 175/1000 | Loss: 1.9839\n",
            "Step 176/1000 | Loss: 1.8275\n",
            "Step 177/1000 | Loss: 1.8086\n",
            "Step 178/1000 | Loss: 1.7358\n",
            "Step 179/1000 | Loss: 1.6110\n",
            "Step 180/1000 | Loss: 1.8702\n",
            "Step 181/1000 | Loss: 1.4538\n",
            "Step 182/1000 | Loss: 1.9557\n",
            "Step 183/1000 | Loss: 1.5535\n",
            "Step 184/1000 | Loss: 1.4643\n",
            "Step 185/1000 | Loss: 1.7811\n",
            "Step 186/1000 | Loss: 1.5591\n",
            "Step 187/1000 | Loss: 1.5672\n",
            "Step 188/1000 | Loss: 1.5962\n",
            "Step 189/1000 | Loss: 1.6182\n",
            "Step 190/1000 | Loss: 1.5893\n",
            "Step 191/1000 | Loss: 1.7440\n",
            "Step 192/1000 | Loss: 1.6196\n",
            "Step 193/1000 | Loss: 1.6090\n",
            "Step 194/1000 | Loss: 1.5077\n",
            "Step 195/1000 | Loss: 1.5571\n",
            "Step 196/1000 | Loss: 1.4941\n",
            "Step 197/1000 | Loss: 1.5607\n",
            "Step 198/1000 | Loss: 1.5877\n",
            "Step 199/1000 | Loss: 1.3731\n",
            "Step 200/1000 | Loss: 1.8448\n",
            "Step 201/1000 | Loss: 1.6826\n",
            "Step 202/1000 | Loss: 1.5615\n",
            "Step 203/1000 | Loss: 1.6222\n",
            "Step 204/1000 | Loss: 1.7792\n",
            "Step 205/1000 | Loss: 1.6359\n",
            "Step 206/1000 | Loss: 1.6210\n",
            "Step 207/1000 | Loss: 1.7857\n",
            "Step 208/1000 | Loss: 1.4750\n",
            "Step 209/1000 | Loss: 1.6768\n",
            "Step 210/1000 | Loss: 1.1900\n",
            "Step 211/1000 | Loss: 1.1247\n",
            "Step 212/1000 | Loss: 1.4529\n",
            "Step 213/1000 | Loss: 1.7547\n",
            "Step 214/1000 | Loss: 1.4426\n",
            "Step 215/1000 | Loss: 1.5152\n",
            "Step 216/1000 | Loss: 1.4559\n",
            "Step 217/1000 | Loss: 1.5035\n",
            "Step 218/1000 | Loss: 1.6315\n",
            "Step 219/1000 | Loss: 1.3793\n",
            "Step 220/1000 | Loss: 1.4404\n",
            "Step 221/1000 | Loss: 1.5267\n",
            "Step 222/1000 | Loss: 1.5419\n",
            "Step 223/1000 | Loss: 1.4859\n",
            "Step 224/1000 | Loss: 1.5789\n",
            "Step 225/1000 | Loss: 1.5219\n",
            "Step 226/1000 | Loss: 1.3885\n",
            "Step 227/1000 | Loss: 1.4456\n",
            "Step 228/1000 | Loss: 1.2768\n",
            "Step 229/1000 | Loss: 1.4232\n",
            "Step 230/1000 | Loss: 1.4697\n",
            "Step 231/1000 | Loss: 1.4856\n",
            "Step 232/1000 | Loss: 1.4933\n",
            "Step 233/1000 | Loss: 1.6510\n",
            "Step 234/1000 | Loss: 1.8862\n",
            "Step 235/1000 | Loss: 1.5885\n",
            "Step 236/1000 | Loss: 1.9198\n",
            "Step 237/1000 | Loss: 1.6955\n",
            "Step 238/1000 | Loss: 1.5787\n",
            "Step 239/1000 | Loss: 1.5512\n",
            "Step 240/1000 | Loss: 1.3946\n",
            "Step 241/1000 | Loss: 1.6783\n",
            "Step 242/1000 | Loss: 1.5754\n",
            "Step 243/1000 | Loss: 1.3674\n",
            "Step 244/1000 | Loss: 1.5136\n",
            "Step 245/1000 | Loss: 1.2778\n",
            "Step 246/1000 | Loss: 1.5013\n",
            "Step 247/1000 | Loss: 1.4428\n",
            "Step 248/1000 | Loss: 1.5041\n",
            "Step 249/1000 | Loss: 1.8939\n",
            "Step 250/1000 | Loss: 1.9054\n",
            "Step 251/1000 | Loss: 1.7125\n",
            "Step 252/1000 | Loss: 1.5565\n",
            "Step 253/1000 | Loss: 1.6274\n",
            "Step 254/1000 | Loss: 1.7815\n",
            "Step 255/1000 | Loss: 1.8441\n",
            "Step 256/1000 | Loss: 1.7309\n",
            "Step 257/1000 | Loss: 1.7300\n",
            "Step 258/1000 | Loss: 1.6047\n",
            "Step 259/1000 | Loss: 1.8545\n",
            "Step 260/1000 | Loss: 1.6394\n",
            "Step 261/1000 | Loss: 1.6279\n",
            "Step 262/1000 | Loss: 1.7586\n",
            "Step 263/1000 | Loss: 1.7518\n",
            "Step 264/1000 | Loss: 1.7353\n",
            "Step 265/1000 | Loss: 1.7636\n",
            "Step 266/1000 | Loss: 1.8198\n",
            "Step 267/1000 | Loss: 1.5839\n",
            "Step 268/1000 | Loss: 1.6824\n",
            "Step 269/1000 | Loss: 1.5527\n",
            "Step 270/1000 | Loss: 1.6149\n",
            "Step 271/1000 | Loss: 1.7881\n",
            "Step 272/1000 | Loss: 1.8499\n",
            "Step 273/1000 | Loss: 1.7396\n",
            "Step 274/1000 | Loss: 1.5280\n",
            "Step 275/1000 | Loss: 1.5863\n",
            "Step 276/1000 | Loss: 1.4172\n",
            "Step 277/1000 | Loss: 1.7831\n",
            "Step 278/1000 | Loss: 1.4443\n",
            "Step 279/1000 | Loss: 1.6476\n",
            "Step 280/1000 | Loss: 1.6260\n",
            "Step 281/1000 | Loss: 1.4257\n",
            "Step 282/1000 | Loss: 1.4949\n",
            "Step 283/1000 | Loss: 1.6135\n",
            "Step 284/1000 | Loss: 1.3731\n",
            "Step 285/1000 | Loss: 1.2411\n",
            "Step 286/1000 | Loss: 1.2888\n",
            "Step 287/1000 | Loss: 1.3011\n",
            "Step 288/1000 | Loss: 1.7696\n",
            "Step 289/1000 | Loss: 1.6135\n",
            "Step 290/1000 | Loss: 1.3835\n",
            "Step 291/1000 | Loss: 1.8493\n",
            "Step 292/1000 | Loss: 1.4984\n",
            "Step 293/1000 | Loss: 1.7061\n",
            "Step 294/1000 | Loss: 1.6725\n",
            "Step 295/1000 | Loss: 1.5666\n",
            "Step 296/1000 | Loss: 1.6903\n",
            "Step 297/1000 | Loss: 2.0710\n",
            "Step 298/1000 | Loss: 1.5564\n",
            "Step 299/1000 | Loss: 1.5278\n",
            "Step 300/1000 | Loss: 1.6706\n",
            "Step 301/1000 | Loss: 1.5004\n",
            "Step 302/1000 | Loss: 1.3199\n",
            "Step 303/1000 | Loss: 1.8519\n",
            "Step 304/1000 | Loss: 1.6214\n",
            "Step 305/1000 | Loss: 1.5338\n",
            "Step 306/1000 | Loss: 1.4459\n",
            "Step 307/1000 | Loss: 1.5815\n",
            "Step 308/1000 | Loss: 1.6304\n",
            "Step 309/1000 | Loss: 1.5237\n",
            "Step 310/1000 | Loss: 1.5152\n",
            "Step 311/1000 | Loss: 1.8621\n",
            "Step 312/1000 | Loss: 1.6052\n",
            "Step 313/1000 | Loss: 1.5067\n",
            "Step 314/1000 | Loss: 1.6629\n",
            "Step 315/1000 | Loss: 1.5910\n",
            "Step 316/1000 | Loss: 1.6173\n",
            "Step 317/1000 | Loss: 1.3716\n",
            "Step 318/1000 | Loss: 1.7059\n",
            "Step 319/1000 | Loss: 1.4322\n",
            "Step 320/1000 | Loss: 1.7494\n",
            "Step 321/1000 | Loss: 1.5645\n",
            "Step 322/1000 | Loss: 1.6209\n",
            "Step 323/1000 | Loss: 1.4032\n",
            "Step 324/1000 | Loss: 1.6810\n",
            "Step 325/1000 | Loss: 1.8027\n",
            "Step 326/1000 | Loss: 1.8059\n",
            "Step 327/1000 | Loss: 1.7118\n",
            "Step 328/1000 | Loss: 1.6013\n",
            "Step 329/1000 | Loss: 1.7920\n",
            "Step 330/1000 | Loss: 1.6418\n",
            "Step 331/1000 | Loss: 1.6538\n",
            "Step 332/1000 | Loss: 1.4978\n",
            "Step 333/1000 | Loss: 1.4551\n",
            "Step 334/1000 | Loss: 1.5243\n",
            "Step 335/1000 | Loss: 1.5896\n",
            "Step 336/1000 | Loss: 1.8481\n",
            "Step 337/1000 | Loss: 1.7397\n",
            "Step 338/1000 | Loss: 1.7291\n",
            "Step 339/1000 | Loss: 1.7145\n",
            "Step 340/1000 | Loss: 1.7271\n",
            "Step 341/1000 | Loss: 1.6406\n",
            "Step 342/1000 | Loss: 1.5088\n",
            "Step 343/1000 | Loss: 1.6877\n",
            "Step 344/1000 | Loss: 1.8403\n",
            "Step 345/1000 | Loss: 1.7089\n",
            "Step 346/1000 | Loss: 1.7954\n",
            "Step 347/1000 | Loss: 1.7480\n",
            "Step 348/1000 | Loss: 1.4356\n",
            "Step 349/1000 | Loss: 1.1773\n",
            "Step 350/1000 | Loss: 1.7092\n",
            "Step 351/1000 | Loss: 1.8218\n",
            "Step 352/1000 | Loss: 1.8065\n",
            "Step 353/1000 | Loss: 1.7542\n",
            "Step 354/1000 | Loss: 1.4766\n",
            "Step 355/1000 | Loss: 1.5646\n",
            "Step 356/1000 | Loss: 1.7209\n",
            "Step 357/1000 | Loss: 1.3905\n",
            "Step 358/1000 | Loss: 1.5931\n",
            "Step 359/1000 | Loss: 1.5954\n",
            "Step 360/1000 | Loss: 1.5371\n",
            "Step 361/1000 | Loss: 1.6835\n",
            "Step 362/1000 | Loss: 1.4807\n",
            "Step 363/1000 | Loss: 1.5897\n",
            "Step 364/1000 | Loss: 1.7305\n",
            "Step 365/1000 | Loss: 1.4450\n",
            "Step 366/1000 | Loss: 1.2439\n",
            "Step 367/1000 | Loss: 1.5335\n",
            "Step 368/1000 | Loss: 1.6985\n",
            "Step 369/1000 | Loss: 1.7791\n",
            "Step 370/1000 | Loss: 1.6607\n",
            "Step 371/1000 | Loss: 1.8953\n",
            "Step 372/1000 | Loss: 1.7662\n",
            "Step 373/1000 | Loss: 1.8965\n",
            "Step 374/1000 | Loss: 1.6431\n",
            "Step 375/1000 | Loss: 1.6331\n",
            "Step 376/1000 | Loss: 1.7265\n",
            "Step 377/1000 | Loss: 1.7671\n",
            "Step 378/1000 | Loss: 1.8496\n",
            "Step 379/1000 | Loss: 1.6186\n",
            "Step 380/1000 | Loss: 1.6636\n",
            "Step 381/1000 | Loss: 1.5644\n",
            "Step 382/1000 | Loss: 2.0155\n",
            "Step 383/1000 | Loss: 1.7020\n",
            "Step 384/1000 | Loss: 1.9022\n",
            "Step 385/1000 | Loss: 1.8155\n",
            "Step 386/1000 | Loss: 1.9725\n",
            "Step 387/1000 | Loss: 1.9679\n",
            "Step 388/1000 | Loss: 1.9726\n",
            "Step 389/1000 | Loss: 1.8574\n",
            "Step 390/1000 | Loss: 1.8421\n",
            "Step 391/1000 | Loss: 1.9281\n",
            "Step 392/1000 | Loss: 2.1678\n",
            "Step 393/1000 | Loss: 1.5367\n",
            "Step 394/1000 | Loss: 1.7461\n",
            "Step 395/1000 | Loss: 1.7350\n",
            "Step 396/1000 | Loss: 1.7761\n",
            "Step 397/1000 | Loss: 2.0124\n",
            "Step 398/1000 | Loss: 1.7598\n",
            "Step 399/1000 | Loss: 1.7749\n",
            "Step 400/1000 | Loss: 2.0169\n",
            "Step 401/1000 | Loss: 1.8838\n",
            "Step 402/1000 | Loss: 1.8525\n",
            "Step 403/1000 | Loss: 1.9169\n",
            "Step 404/1000 | Loss: 2.2761\n",
            "Step 405/1000 | Loss: 1.9917\n",
            "Step 406/1000 | Loss: 1.6779\n",
            "Step 407/1000 | Loss: 1.7243\n",
            "Step 408/1000 | Loss: 1.9381\n",
            "Step 409/1000 | Loss: 1.8039\n",
            "Step 410/1000 | Loss: 1.7422\n",
            "Step 411/1000 | Loss: 2.0425\n",
            "Step 412/1000 | Loss: 1.9947\n",
            "Step 413/1000 | Loss: 1.8885\n",
            "Step 414/1000 | Loss: 1.7634\n",
            "Step 415/1000 | Loss: 1.5780\n",
            "Step 416/1000 | Loss: 1.7025\n",
            "Step 417/1000 | Loss: 1.6478\n",
            "Step 418/1000 | Loss: 1.5663\n",
            "Step 419/1000 | Loss: 1.6602\n",
            "Step 420/1000 | Loss: 1.6654\n",
            "Step 421/1000 | Loss: 1.7040\n",
            "Step 422/1000 | Loss: 1.7442\n",
            "Step 423/1000 | Loss: 1.7373\n",
            "Step 424/1000 | Loss: 1.8057\n",
            "Step 425/1000 | Loss: 1.9051\n",
            "Step 426/1000 | Loss: 1.9698\n",
            "Step 427/1000 | Loss: 1.7198\n",
            "Step 428/1000 | Loss: 1.7080\n",
            "Step 429/1000 | Loss: 1.7756\n",
            "Step 430/1000 | Loss: 1.9601\n",
            "Step 431/1000 | Loss: 1.7966\n",
            "Step 432/1000 | Loss: 1.5557\n",
            "Step 433/1000 | Loss: 1.7747\n",
            "Step 434/1000 | Loss: 1.7691\n",
            "Step 435/1000 | Loss: 1.6008\n",
            "Step 436/1000 | Loss: 1.6852\n",
            "Step 437/1000 | Loss: 1.7169\n",
            "Step 438/1000 | Loss: 1.5585\n",
            "Step 439/1000 | Loss: 1.6343\n",
            "Step 440/1000 | Loss: 1.6720\n",
            "Step 441/1000 | Loss: 1.5174\n",
            "Step 442/1000 | Loss: 1.7647\n",
            "Step 443/1000 | Loss: 1.4348\n",
            "Step 444/1000 | Loss: 1.8607\n",
            "Step 445/1000 | Loss: 1.7500\n",
            "Step 446/1000 | Loss: 1.9285\n",
            "Step 447/1000 | Loss: 1.6470\n",
            "Step 448/1000 | Loss: 2.1280\n",
            "Step 449/1000 | Loss: 1.8271\n",
            "Step 450/1000 | Loss: 1.7357\n",
            "Step 451/1000 | Loss: 1.7954\n",
            "Step 452/1000 | Loss: 1.8765\n",
            "Step 453/1000 | Loss: 1.7229\n",
            "Step 454/1000 | Loss: 1.4269\n",
            "Step 455/1000 | Loss: 1.6294\n",
            "Step 456/1000 | Loss: 1.6580\n",
            "Step 457/1000 | Loss: 1.6625\n",
            "Step 458/1000 | Loss: 1.2196\n",
            "Step 459/1000 | Loss: 1.5982\n",
            "Step 460/1000 | Loss: 1.7700\n",
            "Step 461/1000 | Loss: 1.9223\n",
            "Step 462/1000 | Loss: 1.5758\n",
            "Step 463/1000 | Loss: 1.7674\n",
            "Step 464/1000 | Loss: 1.5690\n",
            "Step 465/1000 | Loss: 1.5660\n",
            "Step 466/1000 | Loss: 1.7564\n",
            "Step 467/1000 | Loss: 1.6947\n",
            "Step 468/1000 | Loss: 1.6214\n",
            "Step 469/1000 | Loss: 1.3746\n",
            "Step 470/1000 | Loss: 1.5203\n",
            "Step 471/1000 | Loss: 1.7467\n",
            "Step 472/1000 | Loss: 1.4713\n",
            "Step 473/1000 | Loss: 1.5040\n",
            "Step 474/1000 | Loss: 1.3924\n",
            "Step 475/1000 | Loss: 1.4949\n",
            "Step 476/1000 | Loss: 1.4432\n",
            "Step 477/1000 | Loss: 1.3758\n",
            "Step 478/1000 | Loss: 1.2872\n",
            "Step 479/1000 | Loss: 1.5142\n",
            "Step 480/1000 | Loss: 1.6857\n",
            "Step 481/1000 | Loss: 1.6582\n",
            "Step 482/1000 | Loss: 1.7765\n",
            "Step 483/1000 | Loss: 1.8650\n",
            "Step 484/1000 | Loss: 1.5658\n",
            "Step 485/1000 | Loss: 1.8286\n",
            "Step 486/1000 | Loss: 1.7900\n",
            "Step 487/1000 | Loss: 1.4988\n",
            "Step 488/1000 | Loss: 1.5187\n",
            "Step 489/1000 | Loss: 1.5423\n",
            "Step 490/1000 | Loss: 1.5224\n",
            "Step 491/1000 | Loss: 1.9185\n",
            "Step 492/1000 | Loss: 1.8055\n",
            "Step 493/1000 | Loss: 1.9342\n",
            "Step 494/1000 | Loss: 1.6627\n",
            "Step 495/1000 | Loss: 1.7365\n",
            "Step 496/1000 | Loss: 1.5188\n",
            "Step 497/1000 | Loss: 1.8417\n",
            "Step 498/1000 | Loss: 1.7044\n",
            "Step 499/1000 | Loss: 1.7878\n",
            "Step 500/1000 | Loss: 1.7137\n",
            "Step 501/1000 | Loss: 1.7543\n",
            "Step 502/1000 | Loss: 2.0859\n",
            "Step 503/1000 | Loss: 1.6001\n",
            "Step 504/1000 | Loss: 1.8398\n",
            "Step 505/1000 | Loss: 1.8754\n",
            "Step 506/1000 | Loss: 1.7880\n",
            "Step 507/1000 | Loss: 1.7354\n",
            "Step 508/1000 | Loss: 1.6992\n",
            "Step 509/1000 | Loss: 1.6612\n",
            "Step 510/1000 | Loss: 1.7970\n",
            "Step 511/1000 | Loss: 1.4327\n",
            "Step 512/1000 | Loss: 1.9075\n",
            "Step 513/1000 | Loss: 1.5139\n",
            "Step 514/1000 | Loss: 1.4622\n",
            "Step 515/1000 | Loss: 1.6949\n",
            "Step 516/1000 | Loss: 1.5514\n",
            "Step 517/1000 | Loss: 1.6027\n",
            "Step 518/1000 | Loss: 1.6054\n",
            "Step 519/1000 | Loss: 1.6456\n",
            "Step 520/1000 | Loss: 1.5583\n",
            "Step 521/1000 | Loss: 1.7109\n",
            "Step 522/1000 | Loss: 1.4952\n",
            "Step 523/1000 | Loss: 1.5575\n",
            "Step 524/1000 | Loss: 1.4845\n",
            "Step 525/1000 | Loss: 1.5618\n",
            "Step 526/1000 | Loss: 1.5357\n",
            "Step 527/1000 | Loss: 1.5403\n",
            "Step 528/1000 | Loss: 1.5527\n",
            "Step 529/1000 | Loss: 1.3592\n",
            "Step 530/1000 | Loss: 1.7143\n",
            "Step 531/1000 | Loss: 1.6795\n",
            "Step 532/1000 | Loss: 1.5790\n",
            "Step 533/1000 | Loss: 1.5150\n",
            "Step 534/1000 | Loss: 1.7418\n",
            "Step 535/1000 | Loss: 1.6693\n",
            "Step 536/1000 | Loss: 1.6971\n",
            "Step 537/1000 | Loss: 1.7073\n",
            "Step 538/1000 | Loss: 1.4400\n",
            "Step 539/1000 | Loss: 1.6992\n",
            "Step 540/1000 | Loss: 1.1508\n",
            "Step 541/1000 | Loss: 1.0802\n",
            "Step 542/1000 | Loss: 1.3719\n",
            "Step 543/1000 | Loss: 1.6426\n",
            "Step 544/1000 | Loss: 1.4031\n",
            "Step 545/1000 | Loss: 1.4715\n",
            "Step 546/1000 | Loss: 1.4759\n",
            "Step 547/1000 | Loss: 1.4691\n",
            "Step 548/1000 | Loss: 1.6448\n",
            "Step 549/1000 | Loss: 1.4690\n",
            "Step 550/1000 | Loss: 1.3975\n",
            "Step 551/1000 | Loss: 1.4848\n",
            "Step 552/1000 | Loss: 1.4643\n",
            "Step 553/1000 | Loss: 1.5537\n",
            "Step 554/1000 | Loss: 1.5853\n",
            "Step 555/1000 | Loss: 1.5011\n",
            "Step 556/1000 | Loss: 1.3857\n",
            "Step 557/1000 | Loss: 1.4515\n",
            "Step 558/1000 | Loss: 1.2046\n",
            "Step 559/1000 | Loss: 1.3579\n",
            "Step 560/1000 | Loss: 1.4235\n",
            "Step 561/1000 | Loss: 1.3772\n",
            "Step 562/1000 | Loss: 1.3741\n",
            "Step 563/1000 | Loss: 1.5670\n",
            "Step 564/1000 | Loss: 1.7767\n",
            "Step 565/1000 | Loss: 1.5248\n",
            "Step 566/1000 | Loss: 1.7668\n",
            "Step 567/1000 | Loss: 1.5450\n",
            "Step 568/1000 | Loss: 1.5638\n",
            "Step 569/1000 | Loss: 1.4403\n",
            "Step 570/1000 | Loss: 1.3633\n",
            "Step 571/1000 | Loss: 1.6975\n",
            "Step 572/1000 | Loss: 1.6095\n",
            "Step 573/1000 | Loss: 1.3190\n",
            "Step 574/1000 | Loss: 1.4772\n",
            "Step 575/1000 | Loss: 1.2142\n",
            "Step 576/1000 | Loss: 1.4522\n",
            "Step 577/1000 | Loss: 1.4550\n",
            "Step 578/1000 | Loss: 1.5258\n",
            "Step 579/1000 | Loss: 1.8789\n",
            "Step 580/1000 | Loss: 1.9024\n",
            "Step 581/1000 | Loss: 1.6538\n",
            "Step 582/1000 | Loss: 1.5802\n",
            "Step 583/1000 | Loss: 1.6114\n",
            "Step 584/1000 | Loss: 1.7221\n",
            "Step 585/1000 | Loss: 1.7910\n",
            "Step 586/1000 | Loss: 1.7849\n",
            "Step 587/1000 | Loss: 1.7328\n",
            "Step 588/1000 | Loss: 1.6388\n",
            "Step 589/1000 | Loss: 1.7660\n",
            "Step 590/1000 | Loss: 1.6927\n",
            "Step 591/1000 | Loss: 1.5481\n",
            "Step 592/1000 | Loss: 1.8399\n",
            "Step 593/1000 | Loss: 1.7966\n",
            "Step 594/1000 | Loss: 1.6378\n",
            "Step 595/1000 | Loss: 1.6801\n",
            "Step 596/1000 | Loss: 1.8260\n",
            "Step 597/1000 | Loss: 1.5910\n",
            "Step 598/1000 | Loss: 1.6037\n",
            "Step 599/1000 | Loss: 1.4653\n",
            "Step 600/1000 | Loss: 1.6009\n",
            "Step 601/1000 | Loss: 1.6717\n",
            "Step 602/1000 | Loss: 1.8411\n",
            "Step 603/1000 | Loss: 1.7524\n",
            "Step 604/1000 | Loss: 1.5332\n",
            "Step 605/1000 | Loss: 1.6715\n",
            "Step 606/1000 | Loss: 1.5057\n",
            "Step 607/1000 | Loss: 1.8065\n",
            "Step 608/1000 | Loss: 1.3850\n",
            "Step 609/1000 | Loss: 1.5753\n",
            "Step 610/1000 | Loss: 1.6007\n",
            "Step 611/1000 | Loss: 1.3978\n",
            "Step 612/1000 | Loss: 1.4839\n",
            "Step 613/1000 | Loss: 1.5678\n",
            "Step 614/1000 | Loss: 1.3094\n",
            "Step 615/1000 | Loss: 1.1531\n",
            "Step 616/1000 | Loss: 1.2804\n",
            "Step 617/1000 | Loss: 1.2835\n",
            "Step 618/1000 | Loss: 1.7538\n",
            "Step 619/1000 | Loss: 1.6977\n",
            "Step 620/1000 | Loss: 1.3682\n",
            "Step 621/1000 | Loss: 1.7340\n",
            "Step 622/1000 | Loss: 1.4312\n",
            "Step 623/1000 | Loss: 1.5924\n",
            "Step 624/1000 | Loss: 1.6281\n",
            "Step 625/1000 | Loss: 1.5746\n",
            "Step 626/1000 | Loss: 1.6281\n",
            "Step 627/1000 | Loss: 2.0885\n",
            "Step 628/1000 | Loss: 1.5668\n",
            "Step 629/1000 | Loss: 1.4951\n",
            "Step 630/1000 | Loss: 1.7246\n",
            "Step 631/1000 | Loss: 1.5677\n",
            "Step 632/1000 | Loss: 1.3317\n",
            "Step 633/1000 | Loss: 1.9155\n",
            "Step 634/1000 | Loss: 1.6367\n",
            "Step 635/1000 | Loss: 1.5294\n",
            "Step 636/1000 | Loss: 1.3004\n",
            "Step 637/1000 | Loss: 1.4735\n",
            "Step 638/1000 | Loss: 1.5544\n",
            "Step 639/1000 | Loss: 1.5188\n",
            "Step 640/1000 | Loss: 1.5554\n",
            "Step 641/1000 | Loss: 1.8771\n",
            "Step 642/1000 | Loss: 1.5072\n",
            "Step 643/1000 | Loss: 1.5268\n",
            "Step 644/1000 | Loss: 1.5454\n",
            "Step 645/1000 | Loss: 1.5018\n",
            "Step 646/1000 | Loss: 1.5601\n",
            "Step 647/1000 | Loss: 1.3537\n",
            "Step 648/1000 | Loss: 1.7034\n",
            "Step 649/1000 | Loss: 1.4481\n",
            "Step 650/1000 | Loss: 1.7576\n",
            "Step 651/1000 | Loss: 1.5298\n",
            "Step 652/1000 | Loss: 1.5991\n",
            "Step 653/1000 | Loss: 1.4185\n",
            "Step 654/1000 | Loss: 1.8839\n",
            "Step 655/1000 | Loss: 1.8624\n",
            "Step 656/1000 | Loss: 1.7801\n",
            "Step 657/1000 | Loss: 1.7554\n",
            "Step 658/1000 | Loss: 1.5264\n",
            "Step 659/1000 | Loss: 1.6050\n",
            "Step 660/1000 | Loss: 1.5155\n",
            "Step 661/1000 | Loss: 1.5566\n",
            "Step 662/1000 | Loss: 1.4963\n",
            "Step 663/1000 | Loss: 1.5209\n",
            "Step 664/1000 | Loss: 1.6384\n",
            "Step 665/1000 | Loss: 1.7021\n",
            "Step 666/1000 | Loss: 1.7720\n",
            "Step 667/1000 | Loss: 1.7709\n",
            "Step 668/1000 | Loss: 1.6132\n",
            "Step 669/1000 | Loss: 1.6981\n",
            "Step 670/1000 | Loss: 1.7205\n",
            "Step 671/1000 | Loss: 1.6381\n",
            "Step 672/1000 | Loss: 1.5113\n",
            "Step 673/1000 | Loss: 1.6683\n",
            "Step 674/1000 | Loss: 1.7912\n",
            "Step 675/1000 | Loss: 1.6548\n",
            "Step 676/1000 | Loss: 1.7653\n",
            "Step 677/1000 | Loss: 1.7485\n",
            "Step 678/1000 | Loss: 1.3234\n",
            "Step 679/1000 | Loss: 1.2428\n",
            "Step 680/1000 | Loss: 1.5905\n",
            "Step 681/1000 | Loss: 1.7702\n",
            "Step 682/1000 | Loss: 1.8317\n",
            "Step 683/1000 | Loss: 1.8204\n",
            "Step 684/1000 | Loss: 1.4909\n",
            "Step 685/1000 | Loss: 1.5610\n",
            "Step 686/1000 | Loss: 1.7652\n",
            "Step 687/1000 | Loss: 1.3955\n",
            "Step 688/1000 | Loss: 1.6225\n",
            "Step 689/1000 | Loss: 1.6232\n",
            "Step 690/1000 | Loss: 1.4714\n",
            "Step 691/1000 | Loss: 1.5951\n",
            "Step 692/1000 | Loss: 1.3906\n",
            "Step 693/1000 | Loss: 1.5323\n",
            "Step 694/1000 | Loss: 1.7027\n",
            "Step 695/1000 | Loss: 1.4687\n",
            "Step 696/1000 | Loss: 1.2694\n",
            "Step 697/1000 | Loss: 1.5783\n",
            "Step 698/1000 | Loss: 1.6444\n",
            "Step 699/1000 | Loss: 1.7610\n",
            "Step 700/1000 | Loss: 1.6320\n",
            "Step 701/1000 | Loss: 1.8530\n",
            "Step 702/1000 | Loss: 1.7041\n",
            "Step 703/1000 | Loss: 1.7946\n",
            "Step 704/1000 | Loss: 1.5746\n",
            "Step 705/1000 | Loss: 1.4805\n",
            "Step 706/1000 | Loss: 1.5954\n",
            "Step 707/1000 | Loss: 1.7055\n",
            "Step 708/1000 | Loss: 1.7966\n",
            "Step 709/1000 | Loss: 1.5631\n",
            "Step 710/1000 | Loss: 1.6653\n",
            "Step 711/1000 | Loss: 1.5956\n",
            "Step 712/1000 | Loss: 1.9317\n",
            "Step 713/1000 | Loss: 1.7285\n",
            "Step 714/1000 | Loss: 1.8825\n",
            "Step 715/1000 | Loss: 1.7335\n",
            "Step 716/1000 | Loss: 1.9688\n",
            "Step 717/1000 | Loss: 1.9904\n",
            "Step 718/1000 | Loss: 1.9869\n",
            "Step 719/1000 | Loss: 1.8476\n",
            "Step 720/1000 | Loss: 1.9516\n",
            "Step 721/1000 | Loss: 1.7848\n",
            "Step 722/1000 | Loss: 2.0370\n",
            "Step 723/1000 | Loss: 1.4868\n",
            "Step 724/1000 | Loss: 1.6628\n",
            "Step 725/1000 | Loss: 1.7259\n",
            "Step 726/1000 | Loss: 1.7414\n",
            "Step 727/1000 | Loss: 1.9178\n",
            "Step 728/1000 | Loss: 1.7574\n",
            "Step 729/1000 | Loss: 1.7166\n",
            "Step 730/1000 | Loss: 2.0693\n",
            "Step 731/1000 | Loss: 1.7799\n",
            "Step 732/1000 | Loss: 1.8109\n",
            "Step 733/1000 | Loss: 1.7701\n",
            "Step 734/1000 | Loss: 2.1623\n",
            "Step 735/1000 | Loss: 1.9003\n",
            "Step 736/1000 | Loss: 1.6154\n",
            "Step 737/1000 | Loss: 1.6039\n",
            "Step 738/1000 | Loss: 1.8651\n",
            "Step 739/1000 | Loss: 1.7644\n",
            "Step 740/1000 | Loss: 1.6835\n",
            "Step 741/1000 | Loss: 1.8510\n",
            "Step 742/1000 | Loss: 1.9060\n",
            "Step 743/1000 | Loss: 1.7469\n",
            "Step 744/1000 | Loss: 1.6535\n",
            "Step 745/1000 | Loss: 1.5589\n",
            "Step 746/1000 | Loss: 1.6293\n",
            "Step 747/1000 | Loss: 1.5686\n",
            "Step 748/1000 | Loss: 1.6109\n",
            "Step 749/1000 | Loss: 1.6712\n",
            "Step 750/1000 | Loss: 1.6395\n",
            "Step 751/1000 | Loss: 1.5563\n",
            "Step 752/1000 | Loss: 1.7178\n",
            "Step 753/1000 | Loss: 1.7271\n",
            "Step 754/1000 | Loss: 1.7011\n",
            "Step 755/1000 | Loss: 1.9308\n",
            "Step 756/1000 | Loss: 2.0025\n",
            "Step 757/1000 | Loss: 1.8301\n",
            "Step 758/1000 | Loss: 1.7102\n",
            "Step 759/1000 | Loss: 1.8174\n",
            "Step 760/1000 | Loss: 1.9902\n",
            "Step 761/1000 | Loss: 1.8642\n",
            "Step 762/1000 | Loss: 1.4357\n",
            "Step 763/1000 | Loss: 1.7237\n",
            "Step 764/1000 | Loss: 1.7707\n",
            "Step 765/1000 | Loss: 1.5722\n",
            "Step 766/1000 | Loss: 1.6074\n",
            "Step 767/1000 | Loss: 1.6317\n",
            "Step 768/1000 | Loss: 1.5392\n",
            "Step 769/1000 | Loss: 1.5717\n",
            "Step 770/1000 | Loss: 1.6168\n",
            "Step 771/1000 | Loss: 1.4825\n",
            "Step 772/1000 | Loss: 1.7125\n",
            "Step 773/1000 | Loss: 1.3331\n",
            "Step 774/1000 | Loss: 1.8284\n",
            "Step 775/1000 | Loss: 1.6628\n",
            "Step 776/1000 | Loss: 1.8728\n",
            "Step 777/1000 | Loss: 1.5223\n",
            "Step 778/1000 | Loss: 2.0661\n",
            "Step 779/1000 | Loss: 1.7459\n",
            "Step 780/1000 | Loss: 1.6144\n",
            "Step 781/1000 | Loss: 1.7866\n",
            "Step 782/1000 | Loss: 1.8295\n",
            "Step 783/1000 | Loss: 1.7453\n",
            "Step 784/1000 | Loss: 1.4417\n",
            "Step 785/1000 | Loss: 1.5760\n",
            "Step 786/1000 | Loss: 1.6469\n",
            "Step 787/1000 | Loss: 1.6289\n",
            "Step 788/1000 | Loss: 1.1485\n",
            "Step 789/1000 | Loss: 1.5178\n",
            "Step 790/1000 | Loss: 1.7857\n",
            "Step 791/1000 | Loss: 1.8551\n",
            "Step 792/1000 | Loss: 1.5647\n",
            "Step 793/1000 | Loss: 1.6385\n",
            "Step 794/1000 | Loss: 1.6041\n",
            "Step 795/1000 | Loss: 1.5565\n",
            "Step 796/1000 | Loss: 1.7949\n",
            "Step 797/1000 | Loss: 1.6791\n",
            "Step 798/1000 | Loss: 1.5631\n",
            "Step 799/1000 | Loss: 1.3053\n",
            "Step 800/1000 | Loss: 1.3972\n",
            "Step 801/1000 | Loss: 1.5277\n",
            "Step 802/1000 | Loss: 1.4500\n",
            "Step 803/1000 | Loss: 1.4712\n",
            "Step 804/1000 | Loss: 1.4188\n",
            "Step 805/1000 | Loss: 1.4702\n",
            "Step 806/1000 | Loss: 1.3982\n",
            "Step 807/1000 | Loss: 1.3490\n",
            "Step 808/1000 | Loss: 1.2365\n",
            "Step 809/1000 | Loss: 1.4502\n",
            "Step 810/1000 | Loss: 1.6747\n",
            "Step 811/1000 | Loss: 1.7086\n",
            "Step 812/1000 | Loss: 1.7442\n",
            "Step 813/1000 | Loss: 1.8999\n",
            "Step 814/1000 | Loss: 1.5805\n",
            "Step 815/1000 | Loss: 1.7032\n",
            "Step 816/1000 | Loss: 1.7373\n",
            "Step 817/1000 | Loss: 1.4366\n",
            "Step 818/1000 | Loss: 1.4511\n",
            "Step 819/1000 | Loss: 1.3995\n",
            "Step 820/1000 | Loss: 1.5288\n",
            "Step 821/1000 | Loss: 1.8794\n",
            "Step 822/1000 | Loss: 1.8580\n",
            "Step 823/1000 | Loss: 1.9701\n",
            "Step 824/1000 | Loss: 1.7351\n",
            "Step 825/1000 | Loss: 1.8141\n",
            "Step 826/1000 | Loss: 1.5005\n",
            "Step 827/1000 | Loss: 1.8200\n",
            "Step 828/1000 | Loss: 1.7078\n",
            "Step 829/1000 | Loss: 1.7628\n",
            "Step 830/1000 | Loss: 1.6994\n",
            "Step 831/1000 | Loss: 1.8277\n",
            "Step 832/1000 | Loss: 2.0567\n",
            "Step 833/1000 | Loss: 1.5892\n",
            "Step 834/1000 | Loss: 1.8237\n",
            "Step 835/1000 | Loss: 1.7939\n",
            "Step 836/1000 | Loss: 1.7206\n",
            "Step 837/1000 | Loss: 1.7256\n",
            "Step 838/1000 | Loss: 1.7077\n",
            "Step 839/1000 | Loss: 1.5874\n",
            "Step 840/1000 | Loss: 1.8584\n",
            "Step 841/1000 | Loss: 1.4932\n",
            "Step 842/1000 | Loss: 1.9500\n",
            "Step 843/1000 | Loss: 1.5655\n",
            "Step 844/1000 | Loss: 1.4175\n",
            "Step 845/1000 | Loss: 1.5817\n",
            "Step 846/1000 | Loss: 1.5920\n",
            "Step 847/1000 | Loss: 1.6403\n",
            "Step 848/1000 | Loss: 1.5802\n",
            "Step 849/1000 | Loss: 1.5495\n",
            "Step 850/1000 | Loss: 1.4392\n",
            "Step 851/1000 | Loss: 1.6738\n",
            "Step 852/1000 | Loss: 1.5420\n",
            "Step 853/1000 | Loss: 1.5318\n",
            "Step 854/1000 | Loss: 1.4547\n",
            "Step 855/1000 | Loss: 1.6021\n",
            "Step 856/1000 | Loss: 1.4954\n",
            "Step 857/1000 | Loss: 1.5055\n",
            "Step 858/1000 | Loss: 1.5476\n",
            "Step 859/1000 | Loss: 1.2959\n",
            "Step 860/1000 | Loss: 1.6510\n",
            "Step 861/1000 | Loss: 1.6024\n",
            "Step 862/1000 | Loss: 1.4977\n",
            "Step 863/1000 | Loss: 1.5627\n",
            "Step 864/1000 | Loss: 1.7387\n",
            "Step 865/1000 | Loss: 1.6245\n",
            "Step 866/1000 | Loss: 1.5907\n",
            "Step 867/1000 | Loss: 1.7247\n",
            "Step 868/1000 | Loss: 1.3997\n",
            "Step 869/1000 | Loss: 1.6158\n",
            "Step 870/1000 | Loss: 1.1225\n",
            "Step 871/1000 | Loss: 1.1172\n",
            "Step 872/1000 | Loss: 1.4048\n",
            "Step 873/1000 | Loss: 1.6024\n",
            "Step 874/1000 | Loss: 1.3364\n",
            "Step 875/1000 | Loss: 1.3773\n",
            "Step 876/1000 | Loss: 1.3439\n",
            "Step 877/1000 | Loss: 1.3540\n",
            "Step 878/1000 | Loss: 1.5449\n",
            "Step 879/1000 | Loss: 1.3878\n",
            "Step 880/1000 | Loss: 1.4428\n",
            "Step 881/1000 | Loss: 1.4117\n",
            "Step 882/1000 | Loss: 1.4333\n",
            "Step 883/1000 | Loss: 1.5043\n",
            "Step 884/1000 | Loss: 1.5097\n",
            "Step 885/1000 | Loss: 1.4346\n",
            "Step 886/1000 | Loss: 1.3773\n",
            "Step 887/1000 | Loss: 1.4233\n",
            "Step 888/1000 | Loss: 1.0828\n",
            "Step 889/1000 | Loss: 1.3036\n",
            "Step 890/1000 | Loss: 1.3512\n",
            "Step 891/1000 | Loss: 1.2745\n",
            "Step 892/1000 | Loss: 1.3221\n",
            "Step 893/1000 | Loss: 1.5801\n",
            "Step 894/1000 | Loss: 1.7245\n",
            "Step 895/1000 | Loss: 1.4318\n",
            "Step 896/1000 | Loss: 1.6849\n",
            "Step 897/1000 | Loss: 1.5395\n",
            "Step 898/1000 | Loss: 1.4779\n",
            "Step 899/1000 | Loss: 1.3851\n",
            "Step 900/1000 | Loss: 1.2899\n",
            "Step 901/1000 | Loss: 1.6167\n",
            "Step 902/1000 | Loss: 1.5476\n",
            "Step 903/1000 | Loss: 1.2946\n",
            "Step 904/1000 | Loss: 1.4199\n",
            "Step 905/1000 | Loss: 1.1803\n",
            "Step 906/1000 | Loss: 1.3634\n",
            "Step 907/1000 | Loss: 1.3833\n",
            "Step 908/1000 | Loss: 1.3672\n",
            "Step 909/1000 | Loss: 1.8059\n",
            "Step 910/1000 | Loss: 1.8133\n",
            "Step 911/1000 | Loss: 1.6668\n",
            "Step 912/1000 | Loss: 1.5038\n",
            "Step 913/1000 | Loss: 1.5576\n",
            "Step 914/1000 | Loss: 1.7367\n",
            "Step 915/1000 | Loss: 1.7788\n",
            "Step 916/1000 | Loss: 1.6701\n",
            "Step 917/1000 | Loss: 1.6367\n",
            "Step 918/1000 | Loss: 1.6229\n",
            "Step 919/1000 | Loss: 1.7333\n",
            "Step 920/1000 | Loss: 1.6334\n",
            "Step 921/1000 | Loss: 1.5614\n",
            "Step 922/1000 | Loss: 1.8268\n",
            "Step 923/1000 | Loss: 1.7889\n",
            "Step 924/1000 | Loss: 1.6618\n",
            "Step 925/1000 | Loss: 1.7015\n",
            "Step 926/1000 | Loss: 1.8528\n",
            "Step 927/1000 | Loss: 1.5568\n",
            "Step 928/1000 | Loss: 1.6518\n",
            "Step 929/1000 | Loss: 1.4628\n",
            "Step 930/1000 | Loss: 1.5467\n",
            "Step 931/1000 | Loss: 1.7688\n",
            "Step 932/1000 | Loss: 1.7917\n",
            "Step 933/1000 | Loss: 1.7077\n",
            "Step 934/1000 | Loss: 1.4677\n",
            "Step 935/1000 | Loss: 1.6257\n",
            "Step 936/1000 | Loss: 1.4924\n",
            "Step 937/1000 | Loss: 1.6390\n",
            "Step 938/1000 | Loss: 1.4584\n",
            "Step 939/1000 | Loss: 1.6517\n",
            "Step 940/1000 | Loss: 1.6282\n",
            "Step 941/1000 | Loss: 1.4584\n",
            "Step 942/1000 | Loss: 1.5191\n",
            "Step 943/1000 | Loss: 1.5348\n",
            "Step 944/1000 | Loss: 1.2902\n",
            "Step 945/1000 | Loss: 1.2182\n",
            "Step 946/1000 | Loss: 1.2038\n",
            "Step 947/1000 | Loss: 1.2432\n",
            "Step 948/1000 | Loss: 1.7870\n",
            "Step 949/1000 | Loss: 1.6202\n",
            "Step 950/1000 | Loss: 1.3654\n",
            "Step 951/1000 | Loss: 1.7500\n",
            "Step 952/1000 | Loss: 1.3746\n",
            "Step 953/1000 | Loss: 1.6440\n",
            "Step 954/1000 | Loss: 1.6096\n",
            "Step 955/1000 | Loss: 1.5129\n",
            "Step 956/1000 | Loss: 1.5413\n",
            "Step 957/1000 | Loss: 1.9840\n",
            "Step 958/1000 | Loss: 1.4884\n",
            "Step 959/1000 | Loss: 1.5177\n",
            "Step 960/1000 | Loss: 1.6788\n",
            "Step 961/1000 | Loss: 1.4828\n",
            "Step 962/1000 | Loss: 1.2985\n",
            "Step 963/1000 | Loss: 1.8405\n",
            "Step 964/1000 | Loss: 1.6984\n",
            "Step 965/1000 | Loss: 1.5279\n",
            "Step 966/1000 | Loss: 1.3539\n",
            "Step 967/1000 | Loss: 1.4806\n",
            "Step 968/1000 | Loss: 1.4911\n",
            "Step 969/1000 | Loss: 1.4752\n",
            "Step 970/1000 | Loss: 1.4412\n",
            "Step 971/1000 | Loss: 1.8336\n",
            "Step 972/1000 | Loss: 1.4669\n",
            "Step 973/1000 | Loss: 1.5068\n",
            "Step 974/1000 | Loss: 1.4038\n",
            "Step 975/1000 | Loss: 1.4191\n",
            "Step 976/1000 | Loss: 1.4936\n",
            "Step 977/1000 | Loss: 1.2286\n",
            "Step 978/1000 | Loss: 1.6179\n",
            "Step 979/1000 | Loss: 1.3862\n",
            "Step 980/1000 | Loss: 1.7375\n",
            "Step 981/1000 | Loss: 1.4824\n",
            "Step 982/1000 | Loss: 1.6315\n",
            "Step 983/1000 | Loss: 1.3989\n",
            "Step 984/1000 | Loss: 1.7584\n",
            "Step 985/1000 | Loss: 1.7648\n",
            "Step 986/1000 | Loss: 1.6646\n",
            "Step 987/1000 | Loss: 1.7446\n",
            "Step 988/1000 | Loss: 1.4967\n",
            "Step 989/1000 | Loss: 1.5854\n",
            "Step 990/1000 | Loss: 1.5222\n",
            "Step 991/1000 | Loss: 1.5313\n",
            "Step 992/1000 | Loss: 1.4388\n",
            "Step 993/1000 | Loss: 1.4937\n",
            "Step 994/1000 | Loss: 1.5238\n",
            "Step 995/1000 | Loss: 1.6197\n",
            "Step 996/1000 | Loss: 1.7920\n",
            "Step 997/1000 | Loss: 1.7795\n",
            "Step 998/1000 | Loss: 1.5965\n",
            "Step 999/1000 | Loss: 1.6163\n",
            "Step 1000/1000 | Loss: 1.6170\n",
            "Epoch 19/20\n",
            "Step 1/1000 | Loss: 1.6171\n",
            "Step 2/1000 | Loss: 1.3886\n",
            "Step 3/1000 | Loss: 1.5698\n",
            "Step 4/1000 | Loss: 1.7848\n",
            "Step 5/1000 | Loss: 1.6649\n",
            "Step 6/1000 | Loss: 1.6808\n",
            "Step 7/1000 | Loss: 1.6393\n",
            "Step 8/1000 | Loss: 1.2904\n",
            "Step 9/1000 | Loss: 1.1043\n",
            "Step 10/1000 | Loss: 1.5343\n",
            "Step 11/1000 | Loss: 1.7808\n",
            "Step 12/1000 | Loss: 1.7223\n",
            "Step 13/1000 | Loss: 1.6367\n",
            "Step 14/1000 | Loss: 1.4535\n",
            "Step 15/1000 | Loss: 1.5455\n",
            "Step 16/1000 | Loss: 1.7414\n",
            "Step 17/1000 | Loss: 1.4075\n",
            "Step 18/1000 | Loss: 1.5520\n",
            "Step 19/1000 | Loss: 1.6539\n",
            "Step 20/1000 | Loss: 1.4374\n",
            "Step 21/1000 | Loss: 1.5972\n",
            "Step 22/1000 | Loss: 1.3338\n",
            "Step 23/1000 | Loss: 1.5526\n",
            "Step 24/1000 | Loss: 1.6799\n",
            "Step 25/1000 | Loss: 1.4020\n",
            "Step 26/1000 | Loss: 1.2249\n",
            "Step 27/1000 | Loss: 1.4363\n",
            "Step 28/1000 | Loss: 1.5892\n",
            "Step 29/1000 | Loss: 1.7378\n",
            "Step 30/1000 | Loss: 1.5969\n",
            "Step 31/1000 | Loss: 1.7898\n",
            "Step 32/1000 | Loss: 1.8162\n",
            "Step 33/1000 | Loss: 1.7178\n",
            "Step 34/1000 | Loss: 1.5108\n",
            "Step 35/1000 | Loss: 1.4559\n",
            "Step 36/1000 | Loss: 1.5276\n",
            "Step 37/1000 | Loss: 1.6285\n",
            "Step 38/1000 | Loss: 1.7109\n",
            "Step 39/1000 | Loss: 1.5088\n",
            "Step 40/1000 | Loss: 1.5779\n",
            "Step 41/1000 | Loss: 1.5141\n",
            "Step 42/1000 | Loss: 1.8980\n",
            "Step 43/1000 | Loss: 1.6602\n",
            "Step 44/1000 | Loss: 1.7863\n",
            "Step 45/1000 | Loss: 1.7031\n",
            "Step 46/1000 | Loss: 1.8299\n",
            "Step 47/1000 | Loss: 2.0592\n",
            "Step 48/1000 | Loss: 1.9782\n",
            "Step 49/1000 | Loss: 1.7183\n",
            "Step 50/1000 | Loss: 1.8161\n",
            "Step 51/1000 | Loss: 1.8106\n",
            "Step 52/1000 | Loss: 1.9811\n",
            "Step 53/1000 | Loss: 1.4496\n",
            "Step 54/1000 | Loss: 1.5877\n",
            "Step 55/1000 | Loss: 1.6708\n",
            "Step 56/1000 | Loss: 1.6526\n",
            "Step 57/1000 | Loss: 1.8279\n",
            "Step 58/1000 | Loss: 1.6338\n",
            "Step 59/1000 | Loss: 1.6619\n",
            "Step 60/1000 | Loss: 1.9384\n",
            "Step 61/1000 | Loss: 1.7110\n",
            "Step 62/1000 | Loss: 1.7244\n",
            "Step 63/1000 | Loss: 1.8149\n",
            "Step 64/1000 | Loss: 2.1708\n",
            "Step 65/1000 | Loss: 1.8495\n",
            "Step 66/1000 | Loss: 1.5667\n",
            "Step 67/1000 | Loss: 1.4968\n",
            "Step 68/1000 | Loss: 1.8329\n",
            "Step 69/1000 | Loss: 1.7104\n",
            "Step 70/1000 | Loss: 1.6514\n",
            "Step 71/1000 | Loss: 1.9083\n",
            "Step 72/1000 | Loss: 1.8752\n",
            "Step 73/1000 | Loss: 1.6484\n",
            "Step 74/1000 | Loss: 1.5748\n",
            "Step 75/1000 | Loss: 1.5419\n",
            "Step 76/1000 | Loss: 1.5511\n",
            "Step 77/1000 | Loss: 1.5166\n",
            "Step 78/1000 | Loss: 1.4702\n",
            "Step 79/1000 | Loss: 1.6042\n",
            "Step 80/1000 | Loss: 1.5389\n",
            "Step 81/1000 | Loss: 1.5006\n",
            "Step 82/1000 | Loss: 1.6320\n",
            "Step 83/1000 | Loss: 1.6155\n",
            "Step 84/1000 | Loss: 1.6431\n",
            "Step 85/1000 | Loss: 1.9425\n",
            "Step 86/1000 | Loss: 1.9994\n",
            "Step 87/1000 | Loss: 1.7846\n",
            "Step 88/1000 | Loss: 1.6577\n",
            "Step 89/1000 | Loss: 1.7088\n",
            "Step 90/1000 | Loss: 1.8417\n",
            "Step 91/1000 | Loss: 1.7302\n",
            "Step 92/1000 | Loss: 1.5053\n",
            "Step 93/1000 | Loss: 1.7270\n",
            "Step 94/1000 | Loss: 1.6816\n",
            "Step 95/1000 | Loss: 1.5405\n",
            "Step 96/1000 | Loss: 1.5208\n",
            "Step 97/1000 | Loss: 1.5517\n",
            "Step 98/1000 | Loss: 1.4006\n",
            "Step 99/1000 | Loss: 1.5047\n",
            "Step 100/1000 | Loss: 1.5009\n",
            "Step 101/1000 | Loss: 1.3623\n",
            "Step 102/1000 | Loss: 1.5826\n",
            "Step 103/1000 | Loss: 1.2856\n",
            "Step 104/1000 | Loss: 1.7239\n",
            "Step 105/1000 | Loss: 1.6120\n",
            "Step 106/1000 | Loss: 1.8737\n",
            "Step 107/1000 | Loss: 1.5391\n",
            "Step 108/1000 | Loss: 1.9646\n",
            "Step 109/1000 | Loss: 1.7211\n",
            "Step 110/1000 | Loss: 1.6043\n",
            "Step 111/1000 | Loss: 1.6666\n",
            "Step 112/1000 | Loss: 1.7994\n",
            "Step 113/1000 | Loss: 1.5757\n",
            "Step 114/1000 | Loss: 1.2273\n",
            "Step 115/1000 | Loss: 1.5178\n",
            "Step 116/1000 | Loss: 1.5246\n",
            "Step 117/1000 | Loss: 1.5742\n",
            "Step 118/1000 | Loss: 1.0832\n",
            "Step 119/1000 | Loss: 1.4138\n",
            "Step 120/1000 | Loss: 1.6620\n",
            "Step 121/1000 | Loss: 1.7759\n",
            "Step 122/1000 | Loss: 1.4637\n",
            "Step 123/1000 | Loss: 1.5835\n",
            "Step 124/1000 | Loss: 1.5079\n",
            "Step 125/1000 | Loss: 1.5130\n",
            "Step 126/1000 | Loss: 1.5659\n",
            "Step 127/1000 | Loss: 1.7545\n",
            "Step 128/1000 | Loss: 1.6488\n",
            "Step 129/1000 | Loss: 1.3392\n",
            "Step 130/1000 | Loss: 1.3798\n",
            "Step 131/1000 | Loss: 1.5262\n",
            "Step 132/1000 | Loss: 1.3564\n",
            "Step 133/1000 | Loss: 1.4460\n",
            "Step 134/1000 | Loss: 1.2812\n",
            "Step 135/1000 | Loss: 1.3851\n",
            "Step 136/1000 | Loss: 1.3179\n",
            "Step 137/1000 | Loss: 1.3198\n",
            "Step 138/1000 | Loss: 1.1892\n",
            "Step 139/1000 | Loss: 1.3345\n",
            "Step 140/1000 | Loss: 1.6147\n",
            "Step 141/1000 | Loss: 1.6011\n",
            "Step 142/1000 | Loss: 1.7455\n",
            "Step 143/1000 | Loss: 1.8217\n",
            "Step 144/1000 | Loss: 1.5667\n",
            "Step 145/1000 | Loss: 1.7682\n",
            "Step 146/1000 | Loss: 1.6455\n",
            "Step 147/1000 | Loss: 1.3982\n",
            "Step 148/1000 | Loss: 1.4461\n",
            "Step 149/1000 | Loss: 1.4009\n",
            "Step 150/1000 | Loss: 1.4891\n",
            "Step 151/1000 | Loss: 1.7639\n",
            "Step 152/1000 | Loss: 1.6893\n",
            "Step 153/1000 | Loss: 1.8052\n",
            "Step 154/1000 | Loss: 1.5942\n",
            "Step 155/1000 | Loss: 1.5776\n",
            "Step 156/1000 | Loss: 1.4684\n",
            "Step 157/1000 | Loss: 1.6999\n",
            "Step 158/1000 | Loss: 1.6051\n",
            "Step 159/1000 | Loss: 1.6423\n",
            "Step 160/1000 | Loss: 1.6230\n",
            "Step 161/1000 | Loss: 1.6484\n",
            "Step 162/1000 | Loss: 1.9563\n",
            "Step 163/1000 | Loss: 1.4476\n",
            "Step 164/1000 | Loss: 1.7129\n",
            "Step 165/1000 | Loss: 1.6881\n",
            "Step 166/1000 | Loss: 1.6149\n",
            "Step 167/1000 | Loss: 1.6188\n",
            "Step 168/1000 | Loss: 1.5557\n",
            "Step 169/1000 | Loss: 1.5191\n",
            "Step 170/1000 | Loss: 1.7122\n",
            "Step 171/1000 | Loss: 1.3503\n",
            "Step 172/1000 | Loss: 1.8217\n",
            "Step 173/1000 | Loss: 1.4667\n",
            "Step 174/1000 | Loss: 1.4311\n",
            "Step 175/1000 | Loss: 1.6250\n",
            "Step 176/1000 | Loss: 1.4794\n",
            "Step 177/1000 | Loss: 1.5198\n",
            "Step 178/1000 | Loss: 1.4932\n",
            "Step 179/1000 | Loss: 1.4490\n",
            "Step 180/1000 | Loss: 1.3725\n",
            "Step 181/1000 | Loss: 1.6571\n",
            "Step 182/1000 | Loss: 1.4635\n",
            "Step 183/1000 | Loss: 1.3722\n",
            "Step 184/1000 | Loss: 1.3380\n",
            "Step 185/1000 | Loss: 1.5172\n",
            "Step 186/1000 | Loss: 1.3690\n",
            "Step 187/1000 | Loss: 1.4695\n",
            "Step 188/1000 | Loss: 1.4403\n",
            "Step 189/1000 | Loss: 1.3168\n",
            "Step 190/1000 | Loss: 1.6536\n",
            "Step 191/1000 | Loss: 1.5732\n",
            "Step 192/1000 | Loss: 1.4208\n",
            "Step 193/1000 | Loss: 1.4903\n",
            "Step 194/1000 | Loss: 1.6106\n",
            "Step 195/1000 | Loss: 1.5094\n",
            "Step 196/1000 | Loss: 1.4103\n",
            "Step 197/1000 | Loss: 1.6365\n",
            "Step 198/1000 | Loss: 1.3301\n",
            "Step 199/1000 | Loss: 1.5055\n",
            "Step 200/1000 | Loss: 1.0488\n",
            "Step 201/1000 | Loss: 1.0613\n",
            "Step 202/1000 | Loss: 1.2812\n",
            "Step 203/1000 | Loss: 1.5871\n",
            "Step 204/1000 | Loss: 1.3471\n",
            "Step 205/1000 | Loss: 1.3599\n",
            "Step 206/1000 | Loss: 1.3017\n",
            "Step 207/1000 | Loss: 1.2942\n",
            "Step 208/1000 | Loss: 1.4300\n",
            "Step 209/1000 | Loss: 1.2966\n",
            "Step 210/1000 | Loss: 1.3267\n",
            "Step 211/1000 | Loss: 1.2871\n",
            "Step 212/1000 | Loss: 1.3311\n",
            "Step 213/1000 | Loss: 1.3960\n",
            "Step 214/1000 | Loss: 1.3893\n",
            "Step 215/1000 | Loss: 1.3877\n",
            "Step 216/1000 | Loss: 1.2439\n",
            "Step 217/1000 | Loss: 1.3884\n",
            "Step 218/1000 | Loss: 1.0791\n",
            "Step 219/1000 | Loss: 1.2378\n",
            "Step 220/1000 | Loss: 1.3443\n",
            "Step 221/1000 | Loss: 1.3119\n",
            "Step 222/1000 | Loss: 1.2706\n",
            "Step 223/1000 | Loss: 1.4970\n",
            "Step 224/1000 | Loss: 1.5091\n",
            "Step 225/1000 | Loss: 1.3964\n",
            "Step 226/1000 | Loss: 1.6207\n",
            "Step 227/1000 | Loss: 1.4743\n",
            "Step 228/1000 | Loss: 1.4268\n",
            "Step 229/1000 | Loss: 1.3210\n",
            "Step 230/1000 | Loss: 1.2454\n",
            "Step 231/1000 | Loss: 1.5614\n",
            "Step 232/1000 | Loss: 1.4643\n",
            "Step 233/1000 | Loss: 1.2690\n",
            "Step 234/1000 | Loss: 1.3164\n",
            "Step 235/1000 | Loss: 1.1343\n",
            "Step 236/1000 | Loss: 1.2993\n",
            "Step 237/1000 | Loss: 1.3162\n",
            "Step 238/1000 | Loss: 1.3126\n",
            "Step 239/1000 | Loss: 1.7486\n",
            "Step 240/1000 | Loss: 1.7364\n",
            "Step 241/1000 | Loss: 1.5445\n",
            "Step 242/1000 | Loss: 1.4787\n",
            "Step 243/1000 | Loss: 1.5112\n",
            "Step 244/1000 | Loss: 1.6492\n",
            "Step 245/1000 | Loss: 1.7343\n",
            "Step 246/1000 | Loss: 1.7064\n",
            "Step 247/1000 | Loss: 1.6137\n",
            "Step 248/1000 | Loss: 1.5821\n",
            "Step 249/1000 | Loss: 1.7004\n",
            "Step 250/1000 | Loss: 1.5341\n",
            "Step 251/1000 | Loss: 1.4185\n",
            "Step 252/1000 | Loss: 1.6230\n",
            "Step 253/1000 | Loss: 1.6031\n",
            "Step 254/1000 | Loss: 1.5395\n",
            "Step 255/1000 | Loss: 1.6326\n",
            "Step 256/1000 | Loss: 1.7199\n",
            "Step 257/1000 | Loss: 1.5315\n",
            "Step 258/1000 | Loss: 1.6261\n",
            "Step 259/1000 | Loss: 1.4536\n",
            "Step 260/1000 | Loss: 1.5500\n",
            "Step 261/1000 | Loss: 1.7124\n",
            "Step 262/1000 | Loss: 1.7576\n",
            "Step 263/1000 | Loss: 1.6775\n",
            "Step 264/1000 | Loss: 1.4925\n",
            "Step 265/1000 | Loss: 1.5583\n",
            "Step 266/1000 | Loss: 1.3556\n",
            "Step 267/1000 | Loss: 1.6895\n",
            "Step 268/1000 | Loss: 1.3119\n",
            "Step 269/1000 | Loss: 1.4751\n",
            "Step 270/1000 | Loss: 1.5174\n",
            "Step 271/1000 | Loss: 1.3693\n",
            "Step 272/1000 | Loss: 1.4564\n",
            "Step 273/1000 | Loss: 1.5109\n",
            "Step 274/1000 | Loss: 1.2709\n",
            "Step 275/1000 | Loss: 1.1994\n",
            "Step 276/1000 | Loss: 1.2115\n",
            "Step 277/1000 | Loss: 1.2068\n",
            "Step 278/1000 | Loss: 1.7414\n",
            "Step 279/1000 | Loss: 1.5415\n",
            "Step 280/1000 | Loss: 1.3821\n",
            "Step 281/1000 | Loss: 1.8285\n",
            "Step 282/1000 | Loss: 1.4961\n",
            "Step 283/1000 | Loss: 1.5642\n",
            "Step 284/1000 | Loss: 1.5770\n",
            "Step 285/1000 | Loss: 1.4720\n",
            "Step 286/1000 | Loss: 1.4638\n",
            "Step 287/1000 | Loss: 1.9639\n",
            "Step 288/1000 | Loss: 1.5077\n",
            "Step 289/1000 | Loss: 1.4533\n",
            "Step 290/1000 | Loss: 1.5934\n",
            "Step 291/1000 | Loss: 1.4582\n",
            "Step 292/1000 | Loss: 1.2938\n",
            "Step 293/1000 | Loss: 1.8102\n",
            "Step 294/1000 | Loss: 1.6476\n",
            "Step 295/1000 | Loss: 1.5063\n",
            "Step 296/1000 | Loss: 1.3880\n",
            "Step 297/1000 | Loss: 1.5405\n",
            "Step 298/1000 | Loss: 1.5840\n",
            "Step 299/1000 | Loss: 1.4812\n",
            "Step 300/1000 | Loss: 1.4540\n",
            "Step 301/1000 | Loss: 1.8214\n",
            "Step 302/1000 | Loss: 1.5106\n",
            "Step 303/1000 | Loss: 1.4734\n",
            "Step 304/1000 | Loss: 1.4575\n",
            "Step 305/1000 | Loss: 1.3832\n",
            "Step 306/1000 | Loss: 1.4443\n",
            "Step 307/1000 | Loss: 1.2126\n",
            "Step 308/1000 | Loss: 1.5064\n",
            "Step 309/1000 | Loss: 1.2640\n",
            "Step 310/1000 | Loss: 1.6088\n",
            "Step 311/1000 | Loss: 1.4171\n",
            "Step 312/1000 | Loss: 1.4735\n",
            "Step 313/1000 | Loss: 1.3807\n",
            "Step 314/1000 | Loss: 1.6959\n",
            "Step 315/1000 | Loss: 1.7262\n",
            "Step 316/1000 | Loss: 1.6306\n",
            "Step 317/1000 | Loss: 1.5970\n",
            "Step 318/1000 | Loss: 1.4296\n",
            "Step 319/1000 | Loss: 1.5711\n",
            "Step 320/1000 | Loss: 1.4468\n",
            "Step 321/1000 | Loss: 1.5284\n",
            "Step 322/1000 | Loss: 1.3717\n",
            "Step 323/1000 | Loss: 1.3800\n",
            "Step 324/1000 | Loss: 1.3630\n",
            "Step 325/1000 | Loss: 1.4713\n",
            "Step 326/1000 | Loss: 1.5774\n",
            "Step 327/1000 | Loss: 1.6788\n",
            "Step 328/1000 | Loss: 1.5080\n",
            "Step 329/1000 | Loss: 1.5886\n",
            "Step 330/1000 | Loss: 1.5860\n",
            "Step 331/1000 | Loss: 1.6051\n",
            "Step 332/1000 | Loss: 1.3454\n",
            "Step 333/1000 | Loss: 1.5231\n",
            "Step 334/1000 | Loss: 1.6382\n",
            "Step 335/1000 | Loss: 1.4688\n",
            "Step 336/1000 | Loss: 1.5851\n",
            "Step 337/1000 | Loss: 1.6556\n",
            "Step 338/1000 | Loss: 1.2606\n",
            "Step 339/1000 | Loss: 1.0926\n",
            "Step 340/1000 | Loss: 1.5166\n",
            "Step 341/1000 | Loss: 1.7033\n",
            "Step 342/1000 | Loss: 1.6482\n",
            "Step 343/1000 | Loss: 1.5694\n",
            "Step 344/1000 | Loss: 1.4131\n",
            "Step 345/1000 | Loss: 1.5038\n",
            "Step 346/1000 | Loss: 1.6278\n",
            "Step 347/1000 | Loss: 1.4453\n",
            "Step 348/1000 | Loss: 1.5008\n",
            "Step 349/1000 | Loss: 1.5421\n",
            "Step 350/1000 | Loss: 1.4513\n",
            "Step 351/1000 | Loss: 1.5371\n",
            "Step 352/1000 | Loss: 1.3393\n",
            "Step 353/1000 | Loss: 1.5275\n",
            "Step 354/1000 | Loss: 1.6044\n",
            "Step 355/1000 | Loss: 1.4035\n",
            "Step 356/1000 | Loss: 1.1872\n",
            "Step 357/1000 | Loss: 1.4859\n",
            "Step 358/1000 | Loss: 1.5711\n",
            "Step 359/1000 | Loss: 1.6910\n",
            "Step 360/1000 | Loss: 1.5433\n",
            "Step 361/1000 | Loss: 1.7129\n",
            "Step 362/1000 | Loss: 1.6728\n",
            "Step 363/1000 | Loss: 1.6910\n",
            "Step 364/1000 | Loss: 1.5940\n",
            "Step 365/1000 | Loss: 1.5643\n",
            "Step 366/1000 | Loss: 1.6084\n",
            "Step 367/1000 | Loss: 1.6209\n",
            "Step 368/1000 | Loss: 1.7142\n",
            "Step 369/1000 | Loss: 1.4406\n",
            "Step 370/1000 | Loss: 1.5064\n",
            "Step 371/1000 | Loss: 1.4809\n",
            "Step 372/1000 | Loss: 1.7610\n",
            "Step 373/1000 | Loss: 1.6387\n",
            "Step 374/1000 | Loss: 1.7671\n",
            "Step 375/1000 | Loss: 1.6396\n",
            "Step 376/1000 | Loss: 1.7968\n",
            "Step 377/1000 | Loss: 1.8717\n",
            "Step 378/1000 | Loss: 1.8635\n",
            "Step 379/1000 | Loss: 1.6442\n",
            "Step 380/1000 | Loss: 1.7413\n",
            "Step 381/1000 | Loss: 1.7076\n",
            "Step 382/1000 | Loss: 1.9353\n",
            "Step 383/1000 | Loss: 1.4422\n",
            "Step 384/1000 | Loss: 1.5633\n",
            "Step 385/1000 | Loss: 1.6033\n",
            "Step 386/1000 | Loss: 1.5749\n",
            "Step 387/1000 | Loss: 1.7911\n",
            "Step 388/1000 | Loss: 1.6051\n",
            "Step 389/1000 | Loss: 1.6495\n",
            "Step 390/1000 | Loss: 1.8202\n",
            "Step 391/1000 | Loss: 1.6475\n",
            "Step 392/1000 | Loss: 1.8289\n",
            "Step 393/1000 | Loss: 1.7389\n",
            "Step 394/1000 | Loss: 2.0704\n",
            "Step 395/1000 | Loss: 1.7598\n",
            "Step 396/1000 | Loss: 1.5160\n",
            "Step 397/1000 | Loss: 1.5288\n",
            "Step 398/1000 | Loss: 1.7986\n",
            "Step 399/1000 | Loss: 1.6016\n",
            "Step 400/1000 | Loss: 1.5365\n",
            "Step 401/1000 | Loss: 1.8148\n",
            "Step 402/1000 | Loss: 1.7502\n",
            "Step 403/1000 | Loss: 1.6400\n",
            "Step 404/1000 | Loss: 1.5872\n",
            "Step 405/1000 | Loss: 1.4128\n",
            "Step 406/1000 | Loss: 1.5560\n",
            "Step 407/1000 | Loss: 1.4523\n",
            "Step 408/1000 | Loss: 1.5032\n",
            "Step 409/1000 | Loss: 1.5056\n",
            "Step 410/1000 | Loss: 1.4507\n",
            "Step 411/1000 | Loss: 1.4498\n",
            "Step 412/1000 | Loss: 1.6081\n",
            "Step 413/1000 | Loss: 1.5916\n",
            "Step 414/1000 | Loss: 1.6037\n",
            "Step 415/1000 | Loss: 1.8397\n",
            "Step 416/1000 | Loss: 1.8441\n",
            "Step 417/1000 | Loss: 1.6598\n",
            "Step 418/1000 | Loss: 1.6290\n",
            "Step 419/1000 | Loss: 1.7042\n",
            "Step 420/1000 | Loss: 1.8679\n",
            "Step 421/1000 | Loss: 1.6596\n",
            "Step 422/1000 | Loss: 1.4024\n",
            "Step 423/1000 | Loss: 1.7473\n",
            "Step 424/1000 | Loss: 1.6491\n",
            "Step 425/1000 | Loss: 1.4779\n",
            "Step 426/1000 | Loss: 1.5074\n",
            "Step 427/1000 | Loss: 1.5298\n",
            "Step 428/1000 | Loss: 1.4079\n",
            "Step 429/1000 | Loss: 1.4520\n",
            "Step 430/1000 | Loss: 1.5112\n",
            "Step 431/1000 | Loss: 1.3446\n",
            "Step 432/1000 | Loss: 1.5663\n",
            "Step 433/1000 | Loss: 1.3746\n",
            "Step 434/1000 | Loss: 1.6796\n",
            "Step 435/1000 | Loss: 1.5374\n",
            "Step 436/1000 | Loss: 1.6669\n",
            "Step 437/1000 | Loss: 1.4780\n",
            "Step 438/1000 | Loss: 1.8721\n",
            "Step 439/1000 | Loss: 1.7389\n",
            "Step 440/1000 | Loss: 1.5901\n",
            "Step 441/1000 | Loss: 1.7318\n",
            "Step 442/1000 | Loss: 1.7059\n",
            "Step 443/1000 | Loss: 1.6899\n",
            "Step 444/1000 | Loss: 1.2959\n",
            "Step 445/1000 | Loss: 1.4797\n",
            "Step 446/1000 | Loss: 1.5208\n",
            "Step 447/1000 | Loss: 1.5103\n",
            "Step 448/1000 | Loss: 1.1066\n",
            "Step 449/1000 | Loss: 1.4459\n",
            "Step 450/1000 | Loss: 1.7165\n",
            "Step 451/1000 | Loss: 1.8236\n",
            "Step 452/1000 | Loss: 1.4886\n",
            "Step 453/1000 | Loss: 1.6249\n",
            "Step 454/1000 | Loss: 1.5132\n",
            "Step 455/1000 | Loss: 1.4740\n",
            "Step 456/1000 | Loss: 1.7025\n",
            "Step 457/1000 | Loss: 1.6556\n",
            "Step 458/1000 | Loss: 1.5689\n",
            "Step 459/1000 | Loss: 1.2526\n",
            "Step 460/1000 | Loss: 1.4120\n",
            "Step 461/1000 | Loss: 1.5080\n",
            "Step 462/1000 | Loss: 1.3777\n",
            "Step 463/1000 | Loss: 1.4159\n",
            "Step 464/1000 | Loss: 1.3326\n",
            "Step 465/1000 | Loss: 1.3275\n",
            "Step 466/1000 | Loss: 1.2666\n",
            "Step 467/1000 | Loss: 1.3584\n",
            "Step 468/1000 | Loss: 1.2661\n",
            "Step 469/1000 | Loss: 1.3739\n",
            "Step 470/1000 | Loss: 1.5999\n",
            "Step 471/1000 | Loss: 1.5561\n",
            "Step 472/1000 | Loss: 1.5964\n",
            "Step 473/1000 | Loss: 1.7586\n",
            "Step 474/1000 | Loss: 1.4919\n",
            "Step 475/1000 | Loss: 1.6401\n",
            "Step 476/1000 | Loss: 1.6650\n",
            "Step 477/1000 | Loss: 1.3698\n",
            "Step 478/1000 | Loss: 1.3448\n",
            "Step 479/1000 | Loss: 1.3822\n",
            "Step 480/1000 | Loss: 1.4190\n",
            "Step 481/1000 | Loss: 1.8085\n",
            "Step 482/1000 | Loss: 1.6007\n",
            "Step 483/1000 | Loss: 1.7642\n",
            "Step 484/1000 | Loss: 1.5547\n",
            "Step 485/1000 | Loss: 1.5193\n",
            "Step 486/1000 | Loss: 1.4240\n",
            "Step 487/1000 | Loss: 1.7042\n",
            "Step 488/1000 | Loss: 1.4985\n",
            "Step 489/1000 | Loss: 1.6154\n",
            "Step 490/1000 | Loss: 1.5686\n",
            "Step 491/1000 | Loss: 1.5601\n",
            "Step 492/1000 | Loss: 1.8683\n",
            "Step 493/1000 | Loss: 1.5158\n",
            "Step 494/1000 | Loss: 1.6807\n",
            "Step 495/1000 | Loss: 1.6339\n",
            "Step 496/1000 | Loss: 1.5876\n",
            "Step 497/1000 | Loss: 1.5791\n",
            "Step 498/1000 | Loss: 1.5729\n",
            "Step 499/1000 | Loss: 1.5142\n",
            "Step 500/1000 | Loss: 1.6014\n",
            "Step 501/1000 | Loss: 1.3060\n",
            "Step 502/1000 | Loss: 1.6374\n",
            "Step 503/1000 | Loss: 1.3339\n",
            "Step 504/1000 | Loss: 1.3127\n",
            "Step 505/1000 | Loss: 1.5598\n",
            "Step 506/1000 | Loss: 1.3813\n",
            "Step 507/1000 | Loss: 1.4722\n",
            "Step 508/1000 | Loss: 1.5182\n",
            "Step 509/1000 | Loss: 1.4006\n",
            "Step 510/1000 | Loss: 1.2666\n",
            "Step 511/1000 | Loss: 1.6039\n",
            "Step 512/1000 | Loss: 1.3748\n",
            "Step 513/1000 | Loss: 1.3874\n",
            "Step 514/1000 | Loss: 1.3409\n",
            "Step 515/1000 | Loss: 1.5352\n",
            "Step 516/1000 | Loss: 1.3277\n",
            "Step 517/1000 | Loss: 1.3676\n",
            "Step 518/1000 | Loss: 1.3691\n",
            "Step 519/1000 | Loss: 1.2682\n",
            "Step 520/1000 | Loss: 1.5622\n",
            "Step 521/1000 | Loss: 1.4851\n",
            "Step 522/1000 | Loss: 1.3875\n",
            "Step 523/1000 | Loss: 1.4107\n",
            "Step 524/1000 | Loss: 1.5944\n",
            "Step 525/1000 | Loss: 1.4775\n",
            "Step 526/1000 | Loss: 1.4673\n",
            "Step 527/1000 | Loss: 1.6083\n",
            "Step 528/1000 | Loss: 1.2819\n",
            "Step 529/1000 | Loss: 1.4835\n",
            "Step 530/1000 | Loss: 1.0409\n",
            "Step 531/1000 | Loss: 1.0028\n",
            "Step 532/1000 | Loss: 1.2463\n",
            "Step 533/1000 | Loss: 1.5344\n",
            "Step 534/1000 | Loss: 1.2214\n",
            "Step 535/1000 | Loss: 1.2773\n",
            "Step 536/1000 | Loss: 1.3435\n",
            "Step 537/1000 | Loss: 1.3631\n",
            "Step 538/1000 | Loss: 1.4753\n",
            "Step 539/1000 | Loss: 1.2797\n",
            "Step 540/1000 | Loss: 1.2842\n",
            "Step 541/1000 | Loss: 1.2840\n",
            "Step 542/1000 | Loss: 1.2877\n",
            "Step 543/1000 | Loss: 1.3795\n",
            "Step 544/1000 | Loss: 1.3651\n",
            "Step 545/1000 | Loss: 1.3538\n",
            "Step 546/1000 | Loss: 1.2359\n",
            "Step 547/1000 | Loss: 1.2543\n",
            "Step 548/1000 | Loss: 1.0283\n",
            "Step 549/1000 | Loss: 1.1552\n",
            "Step 550/1000 | Loss: 1.2314\n",
            "Step 551/1000 | Loss: 1.2351\n",
            "Step 552/1000 | Loss: 1.2928\n",
            "Step 553/1000 | Loss: 1.4092\n",
            "Step 554/1000 | Loss: 1.5747\n",
            "Step 555/1000 | Loss: 1.3618\n",
            "Step 556/1000 | Loss: 1.5668\n",
            "Step 557/1000 | Loss: 1.4529\n",
            "Step 558/1000 | Loss: 1.3786\n",
            "Step 559/1000 | Loss: 1.3493\n",
            "Step 560/1000 | Loss: 1.2674\n",
            "Step 561/1000 | Loss: 1.5579\n",
            "Step 562/1000 | Loss: 1.4013\n",
            "Step 563/1000 | Loss: 1.1707\n",
            "Step 564/1000 | Loss: 1.3187\n",
            "Step 565/1000 | Loss: 1.1015\n",
            "Step 566/1000 | Loss: 1.3676\n",
            "Step 567/1000 | Loss: 1.2631\n",
            "Step 568/1000 | Loss: 1.3445\n",
            "Step 569/1000 | Loss: 1.7089\n",
            "Step 570/1000 | Loss: 1.7317\n",
            "Step 571/1000 | Loss: 1.5010\n",
            "Step 572/1000 | Loss: 1.4701\n",
            "Step 573/1000 | Loss: 1.5052\n",
            "Step 574/1000 | Loss: 1.6286\n",
            "Step 575/1000 | Loss: 1.6939\n",
            "Step 576/1000 | Loss: 1.6928\n",
            "Step 577/1000 | Loss: 1.6053\n",
            "Step 578/1000 | Loss: 1.5799\n",
            "Step 579/1000 | Loss: 1.7870\n",
            "Step 580/1000 | Loss: 1.4959\n",
            "Step 581/1000 | Loss: 1.4239\n",
            "Step 582/1000 | Loss: 1.6051\n",
            "Step 583/1000 | Loss: 1.6082\n",
            "Step 584/1000 | Loss: 1.5098\n",
            "Step 585/1000 | Loss: 1.5266\n",
            "Step 586/1000 | Loss: 1.6744\n",
            "Step 587/1000 | Loss: 1.4356\n",
            "Step 588/1000 | Loss: 1.5355\n",
            "Step 589/1000 | Loss: 1.3834\n",
            "Step 590/1000 | Loss: 1.4375\n",
            "Step 591/1000 | Loss: 1.6440\n",
            "Step 592/1000 | Loss: 1.7223\n",
            "Step 593/1000 | Loss: 1.6956\n",
            "Step 594/1000 | Loss: 1.5119\n",
            "Step 595/1000 | Loss: 1.5700\n",
            "Step 596/1000 | Loss: 1.3683\n",
            "Step 597/1000 | Loss: 1.5221\n",
            "Step 598/1000 | Loss: 1.2958\n",
            "Step 599/1000 | Loss: 1.5414\n",
            "Step 600/1000 | Loss: 1.5250\n",
            "Step 601/1000 | Loss: 1.4123\n",
            "Step 602/1000 | Loss: 1.4376\n",
            "Step 603/1000 | Loss: 1.4148\n",
            "Step 604/1000 | Loss: 1.2337\n",
            "Step 605/1000 | Loss: 1.1699\n",
            "Step 606/1000 | Loss: 1.1630\n",
            "Step 607/1000 | Loss: 1.2113\n",
            "Step 608/1000 | Loss: 1.6996\n",
            "Step 609/1000 | Loss: 1.5863\n",
            "Step 610/1000 | Loss: 1.4576\n",
            "Step 611/1000 | Loss: 1.7077\n",
            "Step 612/1000 | Loss: 1.4800\n",
            "Step 613/1000 | Loss: 1.5526\n",
            "Step 614/1000 | Loss: 1.5667\n",
            "Step 615/1000 | Loss: 1.4522\n",
            "Step 616/1000 | Loss: 1.5332\n",
            "Step 617/1000 | Loss: 1.9808\n",
            "Step 618/1000 | Loss: 1.4616\n",
            "Step 619/1000 | Loss: 1.4660\n",
            "Step 620/1000 | Loss: 1.6232\n",
            "Step 621/1000 | Loss: 1.4770\n",
            "Step 622/1000 | Loss: 1.2824\n",
            "Step 623/1000 | Loss: 1.7728\n",
            "Step 624/1000 | Loss: 1.5159\n",
            "Step 625/1000 | Loss: 1.4509\n",
            "Step 626/1000 | Loss: 1.3480\n",
            "Step 627/1000 | Loss: 1.5396\n",
            "Step 628/1000 | Loss: 1.5240\n",
            "Step 629/1000 | Loss: 1.5464\n",
            "Step 630/1000 | Loss: 1.4206\n",
            "Step 631/1000 | Loss: 1.8268\n",
            "Step 632/1000 | Loss: 1.4218\n",
            "Step 633/1000 | Loss: 1.4768\n",
            "Step 634/1000 | Loss: 1.5143\n",
            "Step 635/1000 | Loss: 1.3914\n",
            "Step 636/1000 | Loss: 1.4475\n",
            "Step 637/1000 | Loss: 1.2757\n",
            "Step 638/1000 | Loss: 1.5502\n",
            "Step 639/1000 | Loss: 1.3797\n",
            "Step 640/1000 | Loss: 1.5408\n",
            "Step 641/1000 | Loss: 1.3373\n",
            "Step 642/1000 | Loss: 1.4660\n",
            "Step 643/1000 | Loss: 1.2935\n",
            "Step 644/1000 | Loss: 1.6474\n",
            "Step 645/1000 | Loss: 1.7826\n",
            "Step 646/1000 | Loss: 1.7172\n",
            "Step 647/1000 | Loss: 1.6171\n",
            "Step 648/1000 | Loss: 1.4596\n",
            "Step 649/1000 | Loss: 1.5842\n",
            "Step 650/1000 | Loss: 1.4987\n",
            "Step 651/1000 | Loss: 1.4859\n",
            "Step 652/1000 | Loss: 1.3927\n",
            "Step 653/1000 | Loss: 1.4814\n",
            "Step 654/1000 | Loss: 1.4128\n",
            "Step 655/1000 | Loss: 1.4259\n",
            "Step 656/1000 | Loss: 1.5927\n",
            "Step 657/1000 | Loss: 1.5476\n",
            "Step 658/1000 | Loss: 1.5120\n",
            "Step 659/1000 | Loss: 1.5755\n",
            "Step 660/1000 | Loss: 1.5308\n",
            "Step 661/1000 | Loss: 1.5038\n",
            "Step 662/1000 | Loss: 1.3830\n",
            "Step 663/1000 | Loss: 1.5825\n",
            "Step 664/1000 | Loss: 1.6549\n",
            "Step 665/1000 | Loss: 1.3982\n",
            "Step 666/1000 | Loss: 1.5132\n",
            "Step 667/1000 | Loss: 1.5638\n",
            "Step 668/1000 | Loss: 1.1800\n",
            "Step 669/1000 | Loss: 1.0185\n",
            "Step 670/1000 | Loss: 1.4680\n",
            "Step 671/1000 | Loss: 1.5752\n",
            "Step 672/1000 | Loss: 1.5912\n",
            "Step 673/1000 | Loss: 1.6000\n",
            "Step 674/1000 | Loss: 1.3608\n",
            "Step 675/1000 | Loss: 1.3494\n",
            "Step 676/1000 | Loss: 1.5104\n",
            "Step 677/1000 | Loss: 1.3362\n",
            "Step 678/1000 | Loss: 1.4989\n",
            "Step 679/1000 | Loss: 1.5256\n",
            "Step 680/1000 | Loss: 1.4660\n",
            "Step 681/1000 | Loss: 1.5596\n",
            "Step 682/1000 | Loss: 1.3195\n",
            "Step 683/1000 | Loss: 1.5223\n",
            "Step 684/1000 | Loss: 1.6399\n",
            "Step 685/1000 | Loss: 1.3869\n",
            "Step 686/1000 | Loss: 1.2044\n",
            "Step 687/1000 | Loss: 1.5535\n",
            "Step 688/1000 | Loss: 1.6323\n",
            "Step 689/1000 | Loss: 1.6771\n",
            "Step 690/1000 | Loss: 1.6281\n",
            "Step 691/1000 | Loss: 1.7386\n",
            "Step 692/1000 | Loss: 1.7149\n",
            "Step 693/1000 | Loss: 1.6998\n",
            "Step 694/1000 | Loss: 1.5387\n",
            "Step 695/1000 | Loss: 1.4835\n",
            "Step 696/1000 | Loss: 1.5461\n",
            "Step 697/1000 | Loss: 1.5684\n",
            "Step 698/1000 | Loss: 1.7320\n",
            "Step 699/1000 | Loss: 1.4830\n",
            "Step 700/1000 | Loss: 1.5357\n",
            "Step 701/1000 | Loss: 1.4907\n",
            "Step 702/1000 | Loss: 1.7409\n",
            "Step 703/1000 | Loss: 1.5458\n",
            "Step 704/1000 | Loss: 1.6916\n",
            "Step 705/1000 | Loss: 1.5613\n",
            "Step 706/1000 | Loss: 1.7943\n",
            "Step 707/1000 | Loss: 1.9027\n",
            "Step 708/1000 | Loss: 1.9481\n",
            "Step 709/1000 | Loss: 1.6805\n",
            "Step 710/1000 | Loss: 1.7466\n",
            "Step 711/1000 | Loss: 1.7836\n",
            "Step 712/1000 | Loss: 2.0129\n",
            "Step 713/1000 | Loss: 1.4695\n",
            "Step 714/1000 | Loss: 1.5787\n",
            "Step 715/1000 | Loss: 1.5547\n",
            "Step 716/1000 | Loss: 1.5710\n",
            "Step 717/1000 | Loss: 1.8045\n",
            "Step 718/1000 | Loss: 1.6151\n",
            "Step 719/1000 | Loss: 1.6703\n",
            "Step 720/1000 | Loss: 1.7856\n",
            "Step 721/1000 | Loss: 1.6505\n",
            "Step 722/1000 | Loss: 1.6843\n",
            "Step 723/1000 | Loss: 1.6971\n",
            "Step 724/1000 | Loss: 2.0468\n",
            "Step 725/1000 | Loss: 1.8455\n",
            "Step 726/1000 | Loss: 1.6558\n",
            "Step 727/1000 | Loss: 1.5962\n",
            "Step 728/1000 | Loss: 1.8272\n",
            "Step 729/1000 | Loss: 1.7692\n",
            "Step 730/1000 | Loss: 1.6729\n",
            "Step 731/1000 | Loss: 1.8295\n",
            "Step 732/1000 | Loss: 1.8184\n",
            "Step 733/1000 | Loss: 1.7100\n",
            "Step 734/1000 | Loss: 1.6240\n",
            "Step 735/1000 | Loss: 1.4679\n",
            "Step 736/1000 | Loss: 1.5784\n",
            "Step 737/1000 | Loss: 1.4254\n",
            "Step 738/1000 | Loss: 1.3255\n",
            "Step 739/1000 | Loss: 1.4904\n",
            "Step 740/1000 | Loss: 1.4393\n",
            "Step 741/1000 | Loss: 1.4627\n",
            "Step 742/1000 | Loss: 1.5942\n",
            "Step 743/1000 | Loss: 1.5855\n",
            "Step 744/1000 | Loss: 1.6378\n",
            "Step 745/1000 | Loss: 1.8069\n",
            "Step 746/1000 | Loss: 1.8542\n",
            "Step 747/1000 | Loss: 1.6532\n",
            "Step 748/1000 | Loss: 1.5823\n",
            "Step 749/1000 | Loss: 1.6314\n",
            "Step 750/1000 | Loss: 1.8310\n",
            "Step 751/1000 | Loss: 1.6605\n",
            "Step 752/1000 | Loss: 1.4263\n",
            "Step 753/1000 | Loss: 1.5389\n",
            "Step 754/1000 | Loss: 1.6538\n",
            "Step 755/1000 | Loss: 1.4513\n",
            "Step 756/1000 | Loss: 1.5512\n",
            "Step 757/1000 | Loss: 1.4605\n",
            "Step 758/1000 | Loss: 1.4103\n",
            "Step 759/1000 | Loss: 1.4671\n",
            "Step 760/1000 | Loss: 1.4525\n",
            "Step 761/1000 | Loss: 1.3384\n",
            "Step 762/1000 | Loss: 1.5204\n",
            "Step 763/1000 | Loss: 1.2814\n",
            "Step 764/1000 | Loss: 1.7293\n",
            "Step 765/1000 | Loss: 1.5880\n",
            "Step 766/1000 | Loss: 1.7841\n",
            "Step 767/1000 | Loss: 1.5259\n",
            "Step 768/1000 | Loss: 1.8844\n",
            "Step 769/1000 | Loss: 1.6390\n",
            "Step 770/1000 | Loss: 1.6319\n",
            "Step 771/1000 | Loss: 1.6429\n",
            "Step 772/1000 | Loss: 1.6470\n",
            "Step 773/1000 | Loss: 1.6153\n",
            "Step 774/1000 | Loss: 1.2813\n",
            "Step 775/1000 | Loss: 1.4291\n",
            "Step 776/1000 | Loss: 1.4729\n",
            "Step 777/1000 | Loss: 1.5322\n",
            "Step 778/1000 | Loss: 1.0739\n",
            "Step 779/1000 | Loss: 1.4434\n",
            "Step 780/1000 | Loss: 1.7174\n",
            "Step 781/1000 | Loss: 1.8184\n",
            "Step 782/1000 | Loss: 1.5047\n",
            "Step 783/1000 | Loss: 1.6337\n",
            "Step 784/1000 | Loss: 1.5508\n",
            "Step 785/1000 | Loss: 1.5643\n",
            "Step 786/1000 | Loss: 1.7531\n",
            "Step 787/1000 | Loss: 1.6883\n",
            "Step 788/1000 | Loss: 1.5477\n",
            "Step 789/1000 | Loss: 1.3046\n",
            "Step 790/1000 | Loss: 1.3123\n",
            "Step 791/1000 | Loss: 1.4929\n",
            "Step 792/1000 | Loss: 1.3952\n",
            "Step 793/1000 | Loss: 1.3630\n",
            "Step 794/1000 | Loss: 1.3916\n",
            "Step 795/1000 | Loss: 1.3748\n",
            "Step 796/1000 | Loss: 1.3589\n",
            "Step 797/1000 | Loss: 1.3671\n",
            "Step 798/1000 | Loss: 1.1878\n",
            "Step 799/1000 | Loss: 1.3317\n",
            "Step 800/1000 | Loss: 1.5682\n",
            "Step 801/1000 | Loss: 1.5303\n",
            "Step 802/1000 | Loss: 1.6488\n",
            "Step 803/1000 | Loss: 1.7422\n",
            "Step 804/1000 | Loss: 1.5562\n",
            "Step 805/1000 | Loss: 1.6291\n",
            "Step 806/1000 | Loss: 1.6116\n",
            "Step 807/1000 | Loss: 1.3224\n",
            "Step 808/1000 | Loss: 1.3678\n",
            "Step 809/1000 | Loss: 1.3721\n",
            "Step 810/1000 | Loss: 1.4362\n",
            "Step 811/1000 | Loss: 1.7492\n",
            "Step 812/1000 | Loss: 1.6163\n",
            "Step 813/1000 | Loss: 1.7676\n",
            "Step 814/1000 | Loss: 1.5619\n",
            "Step 815/1000 | Loss: 1.5597\n",
            "Step 816/1000 | Loss: 1.3971\n",
            "Step 817/1000 | Loss: 1.5908\n",
            "Step 818/1000 | Loss: 1.4977\n",
            "Step 819/1000 | Loss: 1.5915\n",
            "Step 820/1000 | Loss: 1.5295\n",
            "Step 821/1000 | Loss: 1.5173\n",
            "Step 822/1000 | Loss: 1.8440\n",
            "Step 823/1000 | Loss: 1.4783\n",
            "Step 824/1000 | Loss: 1.6123\n",
            "Step 825/1000 | Loss: 1.6808\n",
            "Step 826/1000 | Loss: 1.5558\n",
            "Step 827/1000 | Loss: 1.5926\n",
            "Step 828/1000 | Loss: 1.4944\n",
            "Step 829/1000 | Loss: 1.3792\n",
            "Step 830/1000 | Loss: 1.6369\n",
            "Step 831/1000 | Loss: 1.3115\n",
            "Step 832/1000 | Loss: 1.6618\n",
            "Step 833/1000 | Loss: 1.3761\n",
            "Step 834/1000 | Loss: 1.2887\n",
            "Step 835/1000 | Loss: 1.5651\n",
            "Step 836/1000 | Loss: 1.4869\n",
            "Step 837/1000 | Loss: 1.5034\n",
            "Step 838/1000 | Loss: 1.4483\n",
            "Step 839/1000 | Loss: 1.3544\n",
            "Step 840/1000 | Loss: 1.3748\n",
            "Step 841/1000 | Loss: 1.5097\n",
            "Step 842/1000 | Loss: 1.4132\n",
            "Step 843/1000 | Loss: 1.3889\n",
            "Step 844/1000 | Loss: 1.3512\n",
            "Step 845/1000 | Loss: 1.4976\n",
            "Step 846/1000 | Loss: 1.3790\n",
            "Step 847/1000 | Loss: 1.3529\n",
            "Step 848/1000 | Loss: 1.4239\n",
            "Step 849/1000 | Loss: 1.2718\n",
            "Step 850/1000 | Loss: 1.5007\n",
            "Step 851/1000 | Loss: 1.5264\n",
            "Step 852/1000 | Loss: 1.3729\n",
            "Step 853/1000 | Loss: 1.3638\n",
            "Step 854/1000 | Loss: 1.5244\n",
            "Step 855/1000 | Loss: 1.4028\n",
            "Step 856/1000 | Loss: 1.3959\n",
            "Step 857/1000 | Loss: 1.5554\n",
            "Step 858/1000 | Loss: 1.2545\n",
            "Step 859/1000 | Loss: 1.4847\n",
            "Step 860/1000 | Loss: 1.0396\n",
            "Step 861/1000 | Loss: 0.9588\n",
            "Step 862/1000 | Loss: 1.2055\n",
            "Step 863/1000 | Loss: 1.4647\n",
            "Step 864/1000 | Loss: 1.2041\n",
            "Step 865/1000 | Loss: 1.3302\n",
            "Step 866/1000 | Loss: 1.3380\n",
            "Step 867/1000 | Loss: 1.2474\n",
            "Step 868/1000 | Loss: 1.4221\n",
            "Step 869/1000 | Loss: 1.2726\n",
            "Step 870/1000 | Loss: 1.3116\n",
            "Step 871/1000 | Loss: 1.3321\n",
            "Step 872/1000 | Loss: 1.2850\n",
            "Step 873/1000 | Loss: 1.3022\n",
            "Step 874/1000 | Loss: 1.3957\n",
            "Step 875/1000 | Loss: 1.2735\n",
            "Step 876/1000 | Loss: 1.1897\n",
            "Step 877/1000 | Loss: 1.2728\n",
            "Step 878/1000 | Loss: 1.1106\n",
            "Step 879/1000 | Loss: 1.1356\n",
            "Step 880/1000 | Loss: 1.2221\n",
            "Step 881/1000 | Loss: 1.1784\n",
            "Step 882/1000 | Loss: 1.1467\n",
            "Step 883/1000 | Loss: 1.3684\n",
            "Step 884/1000 | Loss: 1.5175\n",
            "Step 885/1000 | Loss: 1.3323\n",
            "Step 886/1000 | Loss: 1.5104\n",
            "Step 887/1000 | Loss: 1.3970\n",
            "Step 888/1000 | Loss: 1.2702\n",
            "Step 889/1000 | Loss: 1.3244\n",
            "Step 890/1000 | Loss: 1.2911\n",
            "Step 891/1000 | Loss: 1.5824\n",
            "Step 892/1000 | Loss: 1.4122\n",
            "Step 893/1000 | Loss: 1.1540\n",
            "Step 894/1000 | Loss: 1.2346\n",
            "Step 895/1000 | Loss: 1.0744\n",
            "Step 896/1000 | Loss: 1.2468\n",
            "Step 897/1000 | Loss: 1.1983\n",
            "Step 898/1000 | Loss: 1.2582\n",
            "Step 899/1000 | Loss: 1.6625\n",
            "Step 900/1000 | Loss: 1.6968\n",
            "Step 901/1000 | Loss: 1.4294\n",
            "Step 902/1000 | Loss: 1.4190\n",
            "Step 903/1000 | Loss: 1.4262\n",
            "Step 904/1000 | Loss: 1.6579\n",
            "Step 905/1000 | Loss: 1.6612\n",
            "Step 906/1000 | Loss: 1.6351\n",
            "Step 907/1000 | Loss: 1.5531\n",
            "Step 908/1000 | Loss: 1.4813\n",
            "Step 909/1000 | Loss: 1.7345\n",
            "Step 910/1000 | Loss: 1.4718\n",
            "Step 911/1000 | Loss: 1.3596\n",
            "Step 912/1000 | Loss: 1.5687\n",
            "Step 913/1000 | Loss: 1.5563\n",
            "Step 914/1000 | Loss: 1.4362\n",
            "Step 915/1000 | Loss: 1.5058\n",
            "Step 916/1000 | Loss: 1.6515\n",
            "Step 917/1000 | Loss: 1.4662\n",
            "Step 918/1000 | Loss: 1.5080\n",
            "Step 919/1000 | Loss: 1.2981\n",
            "Step 920/1000 | Loss: 1.4983\n",
            "Step 921/1000 | Loss: 1.5392\n",
            "Step 922/1000 | Loss: 1.6823\n",
            "Step 923/1000 | Loss: 1.6521\n",
            "Step 924/1000 | Loss: 1.4149\n",
            "Step 925/1000 | Loss: 1.4718\n",
            "Step 926/1000 | Loss: 1.3034\n",
            "Step 927/1000 | Loss: 1.5501\n",
            "Step 928/1000 | Loss: 1.3326\n",
            "Step 929/1000 | Loss: 1.4089\n",
            "Step 930/1000 | Loss: 1.4155\n",
            "Step 931/1000 | Loss: 1.3647\n",
            "Step 932/1000 | Loss: 1.3489\n",
            "Step 933/1000 | Loss: 1.4680\n",
            "Step 934/1000 | Loss: 1.2729\n",
            "Step 935/1000 | Loss: 1.0867\n",
            "Step 936/1000 | Loss: 1.1703\n",
            "Step 937/1000 | Loss: 1.1188\n",
            "Step 938/1000 | Loss: 1.6586\n",
            "Step 939/1000 | Loss: 1.5368\n",
            "Step 940/1000 | Loss: 1.2711\n",
            "Step 941/1000 | Loss: 1.6744\n",
            "Step 942/1000 | Loss: 1.3970\n",
            "Step 943/1000 | Loss: 1.5616\n",
            "Step 944/1000 | Loss: 1.5684\n",
            "Step 945/1000 | Loss: 1.3227\n",
            "Step 946/1000 | Loss: 1.4495\n",
            "Step 947/1000 | Loss: 1.9012\n",
            "Step 948/1000 | Loss: 1.4436\n",
            "Step 949/1000 | Loss: 1.3131\n",
            "Step 950/1000 | Loss: 1.6073\n",
            "Step 951/1000 | Loss: 1.4595\n",
            "Step 952/1000 | Loss: 1.2968\n",
            "Step 953/1000 | Loss: 1.8033\n",
            "Step 954/1000 | Loss: 1.5329\n",
            "Step 955/1000 | Loss: 1.4229\n",
            "Step 956/1000 | Loss: 1.2760\n",
            "Step 957/1000 | Loss: 1.5279\n",
            "Step 958/1000 | Loss: 1.4113\n",
            "Step 959/1000 | Loss: 1.4856\n",
            "Step 960/1000 | Loss: 1.4852\n",
            "Step 961/1000 | Loss: 1.8128\n",
            "Step 962/1000 | Loss: 1.4737\n",
            "Step 963/1000 | Loss: 1.4820\n",
            "Step 964/1000 | Loss: 1.5482\n",
            "Step 965/1000 | Loss: 1.4034\n",
            "Step 966/1000 | Loss: 1.5004\n",
            "Step 967/1000 | Loss: 1.2390\n",
            "Step 968/1000 | Loss: 1.5385\n",
            "Step 969/1000 | Loss: 1.3496\n",
            "Step 970/1000 | Loss: 1.6954\n",
            "Step 971/1000 | Loss: 1.3291\n",
            "Step 972/1000 | Loss: 1.5758\n",
            "Step 973/1000 | Loss: 1.2463\n",
            "Step 974/1000 | Loss: 1.6028\n",
            "Step 975/1000 | Loss: 1.7454\n",
            "Step 976/1000 | Loss: 1.6747\n",
            "Step 977/1000 | Loss: 1.6402\n",
            "Step 978/1000 | Loss: 1.4236\n",
            "Step 979/1000 | Loss: 1.5475\n",
            "Step 980/1000 | Loss: 1.4357\n",
            "Step 981/1000 | Loss: 1.4561\n",
            "Step 982/1000 | Loss: 1.3946\n",
            "Step 983/1000 | Loss: 1.3120\n",
            "Step 984/1000 | Loss: 1.4346\n",
            "Step 985/1000 | Loss: 1.3771\n",
            "Step 986/1000 | Loss: 1.6127\n",
            "Step 987/1000 | Loss: 1.5247\n",
            "Step 988/1000 | Loss: 1.5010\n",
            "Step 989/1000 | Loss: 1.5298\n",
            "Step 990/1000 | Loss: 1.5675\n",
            "Step 991/1000 | Loss: 1.4363\n",
            "Step 992/1000 | Loss: 1.2881\n",
            "Step 993/1000 | Loss: 1.5176\n",
            "Step 994/1000 | Loss: 1.5854\n",
            "Step 995/1000 | Loss: 1.4281\n",
            "Step 996/1000 | Loss: 1.6645\n",
            "Step 997/1000 | Loss: 1.5111\n",
            "Step 998/1000 | Loss: 1.1977\n",
            "Step 999/1000 | Loss: 0.9897\n",
            "Step 1000/1000 | Loss: 1.3990\n",
            "Epoch 20/20\n",
            "Step 1/1000 | Loss: 1.5725\n",
            "Step 2/1000 | Loss: 1.6208\n",
            "Step 3/1000 | Loss: 1.4657\n",
            "Step 4/1000 | Loss: 1.2914\n",
            "Step 5/1000 | Loss: 1.3307\n",
            "Step 6/1000 | Loss: 1.4520\n",
            "Step 7/1000 | Loss: 1.3298\n",
            "Step 8/1000 | Loss: 1.4256\n",
            "Step 9/1000 | Loss: 1.4146\n",
            "Step 10/1000 | Loss: 1.3992\n",
            "Step 11/1000 | Loss: 1.4488\n",
            "Step 12/1000 | Loss: 1.2746\n",
            "Step 13/1000 | Loss: 1.4632\n",
            "Step 14/1000 | Loss: 1.5547\n",
            "Step 15/1000 | Loss: 1.3461\n",
            "Step 16/1000 | Loss: 1.1729\n",
            "Step 17/1000 | Loss: 1.4216\n",
            "Step 18/1000 | Loss: 1.5451\n",
            "Step 19/1000 | Loss: 1.6931\n",
            "Step 20/1000 | Loss: 1.5539\n",
            "Step 21/1000 | Loss: 1.7597\n",
            "Step 22/1000 | Loss: 1.6217\n",
            "Step 23/1000 | Loss: 1.5865\n",
            "Step 24/1000 | Loss: 1.4422\n",
            "Step 25/1000 | Loss: 1.4499\n",
            "Step 26/1000 | Loss: 1.5341\n",
            "Step 27/1000 | Loss: 1.6385\n",
            "Step 28/1000 | Loss: 1.6385\n",
            "Step 29/1000 | Loss: 1.4254\n",
            "Step 30/1000 | Loss: 1.4537\n",
            "Step 31/1000 | Loss: 1.4233\n",
            "Step 32/1000 | Loss: 1.7421\n",
            "Step 33/1000 | Loss: 1.5610\n",
            "Step 34/1000 | Loss: 1.6485\n",
            "Step 35/1000 | Loss: 1.5892\n",
            "Step 36/1000 | Loss: 1.7935\n",
            "Step 37/1000 | Loss: 1.8158\n",
            "Step 38/1000 | Loss: 1.8975\n",
            "Step 39/1000 | Loss: 1.6576\n",
            "Step 40/1000 | Loss: 1.7845\n",
            "Step 41/1000 | Loss: 1.7322\n",
            "Step 42/1000 | Loss: 1.8853\n",
            "Step 43/1000 | Loss: 1.3619\n",
            "Step 44/1000 | Loss: 1.5364\n",
            "Step 45/1000 | Loss: 1.5801\n",
            "Step 46/1000 | Loss: 1.5345\n",
            "Step 47/1000 | Loss: 1.7762\n",
            "Step 48/1000 | Loss: 1.5808\n",
            "Step 49/1000 | Loss: 1.6251\n",
            "Step 50/1000 | Loss: 1.7655\n",
            "Step 51/1000 | Loss: 1.6186\n",
            "Step 52/1000 | Loss: 1.6609\n",
            "Step 53/1000 | Loss: 1.6406\n",
            "Step 54/1000 | Loss: 1.9528\n",
            "Step 55/1000 | Loss: 1.7461\n",
            "Step 56/1000 | Loss: 1.4634\n",
            "Step 57/1000 | Loss: 1.5650\n",
            "Step 58/1000 | Loss: 1.7394\n",
            "Step 59/1000 | Loss: 1.7112\n",
            "Step 60/1000 | Loss: 1.6160\n",
            "Step 61/1000 | Loss: 1.7746\n",
            "Step 62/1000 | Loss: 1.7480\n",
            "Step 63/1000 | Loss: 1.5943\n",
            "Step 64/1000 | Loss: 1.5405\n",
            "Step 65/1000 | Loss: 1.4220\n",
            "Step 66/1000 | Loss: 1.5208\n",
            "Step 67/1000 | Loss: 1.4293\n",
            "Step 68/1000 | Loss: 1.5766\n",
            "Step 69/1000 | Loss: 1.5313\n",
            "Step 70/1000 | Loss: 1.4138\n",
            "Step 71/1000 | Loss: 1.4649\n",
            "Step 72/1000 | Loss: 1.4779\n",
            "Step 73/1000 | Loss: 1.6085\n",
            "Step 74/1000 | Loss: 1.5440\n",
            "Step 75/1000 | Loss: 1.6519\n",
            "Step 76/1000 | Loss: 1.8972\n",
            "Step 77/1000 | Loss: 1.6280\n",
            "Step 78/1000 | Loss: 1.4301\n",
            "Step 79/1000 | Loss: 1.6466\n",
            "Step 80/1000 | Loss: 1.8747\n",
            "Step 81/1000 | Loss: 1.6593\n",
            "Step 82/1000 | Loss: 1.3733\n",
            "Step 83/1000 | Loss: 1.6133\n",
            "Step 84/1000 | Loss: 1.6526\n",
            "Step 85/1000 | Loss: 1.4854\n",
            "Step 86/1000 | Loss: 1.5204\n",
            "Step 87/1000 | Loss: 1.4471\n",
            "Step 88/1000 | Loss: 1.3249\n",
            "Step 89/1000 | Loss: 1.3760\n",
            "Step 90/1000 | Loss: 1.4149\n",
            "Step 91/1000 | Loss: 1.2518\n",
            "Step 92/1000 | Loss: 1.5171\n",
            "Step 93/1000 | Loss: 1.2816\n",
            "Step 94/1000 | Loss: 1.6372\n",
            "Step 95/1000 | Loss: 1.6073\n",
            "Step 96/1000 | Loss: 1.7856\n",
            "Step 97/1000 | Loss: 1.4361\n",
            "Step 98/1000 | Loss: 1.9369\n",
            "Step 99/1000 | Loss: 1.6632\n",
            "Step 100/1000 | Loss: 1.6448\n",
            "Step 101/1000 | Loss: 1.6730\n",
            "Step 102/1000 | Loss: 1.6127\n",
            "Step 103/1000 | Loss: 1.5882\n",
            "Step 104/1000 | Loss: 1.2948\n",
            "Step 105/1000 | Loss: 1.4072\n",
            "Step 106/1000 | Loss: 1.5073\n",
            "Step 107/1000 | Loss: 1.5374\n",
            "Step 108/1000 | Loss: 1.0325\n",
            "Step 109/1000 | Loss: 1.3490\n",
            "Step 110/1000 | Loss: 1.5839\n",
            "Step 111/1000 | Loss: 1.7271\n",
            "Step 112/1000 | Loss: 1.4580\n",
            "Step 113/1000 | Loss: 1.5391\n",
            "Step 114/1000 | Loss: 1.5083\n",
            "Step 115/1000 | Loss: 1.4193\n",
            "Step 116/1000 | Loss: 1.6154\n",
            "Step 117/1000 | Loss: 1.6376\n",
            "Step 118/1000 | Loss: 1.4773\n",
            "Step 119/1000 | Loss: 1.2919\n",
            "Step 120/1000 | Loss: 1.3686\n",
            "Step 121/1000 | Loss: 1.4866\n",
            "Step 122/1000 | Loss: 1.3313\n",
            "Step 123/1000 | Loss: 1.3221\n",
            "Step 124/1000 | Loss: 1.2457\n",
            "Step 125/1000 | Loss: 1.2978\n",
            "Step 126/1000 | Loss: 1.3419\n",
            "Step 127/1000 | Loss: 1.2492\n",
            "Step 128/1000 | Loss: 1.1747\n",
            "Step 129/1000 | Loss: 1.3122\n",
            "Step 130/1000 | Loss: 1.5965\n",
            "Step 131/1000 | Loss: 1.4633\n",
            "Step 132/1000 | Loss: 1.5716\n",
            "Step 133/1000 | Loss: 1.7213\n",
            "Step 134/1000 | Loss: 1.4354\n",
            "Step 135/1000 | Loss: 1.5932\n",
            "Step 136/1000 | Loss: 1.5361\n",
            "Step 137/1000 | Loss: 1.3041\n",
            "Step 138/1000 | Loss: 1.3087\n",
            "Step 139/1000 | Loss: 1.2844\n",
            "Step 140/1000 | Loss: 1.3572\n",
            "Step 141/1000 | Loss: 1.7064\n",
            "Step 142/1000 | Loss: 1.4823\n",
            "Step 143/1000 | Loss: 1.7497\n",
            "Step 144/1000 | Loss: 1.4678\n",
            "Step 145/1000 | Loss: 1.5202\n",
            "Step 146/1000 | Loss: 1.3249\n",
            "Step 147/1000 | Loss: 1.5563\n",
            "Step 148/1000 | Loss: 1.5008\n",
            "Step 149/1000 | Loss: 1.5724\n",
            "Step 150/1000 | Loss: 1.4763\n",
            "Step 151/1000 | Loss: 1.4790\n",
            "Step 152/1000 | Loss: 1.8185\n",
            "Step 153/1000 | Loss: 1.3593\n",
            "Step 154/1000 | Loss: 1.5680\n",
            "Step 155/1000 | Loss: 1.5919\n",
            "Step 156/1000 | Loss: 1.5022\n",
            "Step 157/1000 | Loss: 1.5182\n",
            "Step 158/1000 | Loss: 1.3950\n",
            "Step 159/1000 | Loss: 1.3638\n",
            "Step 160/1000 | Loss: 1.5793\n",
            "Step 161/1000 | Loss: 1.2328\n",
            "Step 162/1000 | Loss: 1.5868\n",
            "Step 163/1000 | Loss: 1.3498\n",
            "Step 164/1000 | Loss: 1.1811\n",
            "Step 165/1000 | Loss: 1.4764\n",
            "Step 166/1000 | Loss: 1.4283\n",
            "Step 167/1000 | Loss: 1.4142\n",
            "Step 168/1000 | Loss: 1.4179\n",
            "Step 169/1000 | Loss: 1.4615\n",
            "Step 170/1000 | Loss: 1.2402\n",
            "Step 171/1000 | Loss: 1.5343\n",
            "Step 172/1000 | Loss: 1.3894\n",
            "Step 173/1000 | Loss: 1.3529\n",
            "Step 174/1000 | Loss: 1.2649\n",
            "Step 175/1000 | Loss: 1.4516\n",
            "Step 176/1000 | Loss: 1.2876\n",
            "Step 177/1000 | Loss: 1.3350\n",
            "Step 178/1000 | Loss: 1.3167\n",
            "Step 179/1000 | Loss: 1.1698\n",
            "Step 180/1000 | Loss: 1.5228\n",
            "Step 181/1000 | Loss: 1.4123\n",
            "Step 182/1000 | Loss: 1.4061\n",
            "Step 183/1000 | Loss: 1.4080\n",
            "Step 184/1000 | Loss: 1.4984\n",
            "Step 185/1000 | Loss: 1.4493\n",
            "Step 186/1000 | Loss: 1.3807\n",
            "Step 187/1000 | Loss: 1.5870\n",
            "Step 188/1000 | Loss: 1.2390\n",
            "Step 189/1000 | Loss: 1.3800\n",
            "Step 190/1000 | Loss: 0.9508\n",
            "Step 191/1000 | Loss: 0.9011\n",
            "Step 192/1000 | Loss: 1.2128\n",
            "Step 193/1000 | Loss: 1.4568\n",
            "Step 194/1000 | Loss: 1.1384\n",
            "Step 195/1000 | Loss: 1.1918\n",
            "Step 196/1000 | Loss: 1.2506\n",
            "Step 197/1000 | Loss: 1.2339\n",
            "Step 198/1000 | Loss: 1.3580\n",
            "Step 199/1000 | Loss: 1.2121\n",
            "Step 200/1000 | Loss: 1.2147\n",
            "Step 201/1000 | Loss: 1.1865\n",
            "Step 202/1000 | Loss: 1.2344\n",
            "Step 203/1000 | Loss: 1.3198\n",
            "Step 204/1000 | Loss: 1.3011\n",
            "Step 205/1000 | Loss: 1.2786\n",
            "Step 206/1000 | Loss: 1.1400\n",
            "Step 207/1000 | Loss: 1.1902\n",
            "Step 208/1000 | Loss: 0.9866\n",
            "Step 209/1000 | Loss: 1.1842\n",
            "Step 210/1000 | Loss: 1.2190\n",
            "Step 211/1000 | Loss: 1.2469\n",
            "Step 212/1000 | Loss: 1.2653\n",
            "Step 213/1000 | Loss: 1.3829\n",
            "Step 214/1000 | Loss: 1.5511\n",
            "Step 215/1000 | Loss: 1.2963\n",
            "Step 216/1000 | Loss: 1.5522\n",
            "Step 217/1000 | Loss: 1.3926\n",
            "Step 218/1000 | Loss: 1.2994\n",
            "Step 219/1000 | Loss: 1.2313\n",
            "Step 220/1000 | Loss: 1.1977\n",
            "Step 221/1000 | Loss: 1.4754\n",
            "Step 222/1000 | Loss: 1.3862\n",
            "Step 223/1000 | Loss: 1.1775\n",
            "Step 224/1000 | Loss: 1.2815\n",
            "Step 225/1000 | Loss: 1.0366\n",
            "Step 226/1000 | Loss: 1.1781\n",
            "Step 227/1000 | Loss: 1.1999\n",
            "Step 228/1000 | Loss: 1.2936\n",
            "Step 229/1000 | Loss: 1.6920\n",
            "Step 230/1000 | Loss: 1.5846\n",
            "Step 231/1000 | Loss: 1.3761\n",
            "Step 232/1000 | Loss: 1.3943\n",
            "Step 233/1000 | Loss: 1.3476\n",
            "Step 234/1000 | Loss: 1.5553\n",
            "Step 235/1000 | Loss: 1.5912\n",
            "Step 236/1000 | Loss: 1.5242\n",
            "Step 237/1000 | Loss: 1.5322\n",
            "Step 238/1000 | Loss: 1.4571\n",
            "Step 239/1000 | Loss: 1.6717\n",
            "Step 240/1000 | Loss: 1.4926\n",
            "Step 241/1000 | Loss: 1.4328\n",
            "Step 242/1000 | Loss: 1.6267\n",
            "Step 243/1000 | Loss: 1.5144\n",
            "Step 244/1000 | Loss: 1.4779\n",
            "Step 245/1000 | Loss: 1.5310\n",
            "Step 246/1000 | Loss: 1.5660\n",
            "Step 247/1000 | Loss: 1.4305\n",
            "Step 248/1000 | Loss: 1.4194\n",
            "Step 249/1000 | Loss: 1.2741\n",
            "Step 250/1000 | Loss: 1.4184\n",
            "Step 251/1000 | Loss: 1.5068\n",
            "Step 252/1000 | Loss: 1.5499\n",
            "Step 253/1000 | Loss: 1.5899\n",
            "Step 254/1000 | Loss: 1.3427\n",
            "Step 255/1000 | Loss: 1.4582\n",
            "Step 256/1000 | Loss: 1.3642\n",
            "Step 257/1000 | Loss: 1.5044\n",
            "Step 258/1000 | Loss: 1.2040\n",
            "Step 259/1000 | Loss: 1.4101\n",
            "Step 260/1000 | Loss: 1.4286\n",
            "Step 261/1000 | Loss: 1.3657\n",
            "Step 262/1000 | Loss: 1.2208\n",
            "Step 263/1000 | Loss: 1.4234\n",
            "Step 264/1000 | Loss: 1.0860\n",
            "Step 265/1000 | Loss: 1.0254\n",
            "Step 266/1000 | Loss: 1.0927\n",
            "Step 267/1000 | Loss: 1.0678\n",
            "Step 268/1000 | Loss: 1.5375\n",
            "Step 269/1000 | Loss: 1.3661\n",
            "Step 270/1000 | Loss: 1.3132\n",
            "Step 271/1000 | Loss: 1.6048\n",
            "Step 272/1000 | Loss: 1.3464\n",
            "Step 273/1000 | Loss: 1.4810\n",
            "Step 274/1000 | Loss: 1.4752\n",
            "Step 275/1000 | Loss: 1.4189\n",
            "Step 276/1000 | Loss: 1.4816\n",
            "Step 277/1000 | Loss: 1.8786\n",
            "Step 278/1000 | Loss: 1.3998\n",
            "Step 279/1000 | Loss: 1.3184\n",
            "Step 280/1000 | Loss: 1.4964\n",
            "Step 281/1000 | Loss: 1.3534\n",
            "Step 282/1000 | Loss: 1.1245\n",
            "Step 283/1000 | Loss: 1.5935\n",
            "Step 284/1000 | Loss: 1.4628\n",
            "Step 285/1000 | Loss: 1.2886\n",
            "Step 286/1000 | Loss: 1.2788\n",
            "Step 287/1000 | Loss: 1.4734\n",
            "Step 288/1000 | Loss: 1.4514\n",
            "Step 289/1000 | Loss: 1.3333\n",
            "Step 290/1000 | Loss: 1.3070\n",
            "Step 291/1000 | Loss: 1.6925\n",
            "Step 292/1000 | Loss: 1.3427\n",
            "Step 293/1000 | Loss: 1.3934\n",
            "Step 294/1000 | Loss: 1.4171\n",
            "Step 295/1000 | Loss: 1.3868\n",
            "Step 296/1000 | Loss: 1.3916\n",
            "Step 297/1000 | Loss: 1.1596\n",
            "Step 298/1000 | Loss: 1.4687\n",
            "Step 299/1000 | Loss: 1.1662\n",
            "Step 300/1000 | Loss: 1.4707\n",
            "Step 301/1000 | Loss: 1.2868\n",
            "Step 302/1000 | Loss: 1.4269\n",
            "Step 303/1000 | Loss: 1.1824\n",
            "Step 304/1000 | Loss: 1.5340\n",
            "Step 305/1000 | Loss: 1.6412\n",
            "Step 306/1000 | Loss: 1.5707\n",
            "Step 307/1000 | Loss: 1.5518\n",
            "Step 308/1000 | Loss: 1.3752\n",
            "Step 309/1000 | Loss: 1.4981\n",
            "Step 310/1000 | Loss: 1.4242\n",
            "Step 311/1000 | Loss: 1.4167\n",
            "Step 312/1000 | Loss: 1.4355\n",
            "Step 313/1000 | Loss: 1.3245\n",
            "Step 314/1000 | Loss: 1.4220\n",
            "Step 315/1000 | Loss: 1.4712\n",
            "Step 316/1000 | Loss: 1.6997\n",
            "Step 317/1000 | Loss: 1.5682\n",
            "Step 318/1000 | Loss: 1.4808\n",
            "Step 319/1000 | Loss: 1.4877\n",
            "Step 320/1000 | Loss: 1.4639\n",
            "Step 321/1000 | Loss: 1.4115\n",
            "Step 322/1000 | Loss: 1.2796\n",
            "Step 323/1000 | Loss: 1.4334\n",
            "Step 324/1000 | Loss: 1.5492\n",
            "Step 325/1000 | Loss: 1.4487\n",
            "Step 326/1000 | Loss: 1.5408\n",
            "Step 327/1000 | Loss: 1.4850\n",
            "Step 328/1000 | Loss: 1.1556\n",
            "Step 329/1000 | Loss: 1.0821\n",
            "Step 330/1000 | Loss: 1.3049\n",
            "Step 331/1000 | Loss: 1.4457\n",
            "Step 332/1000 | Loss: 1.5999\n",
            "Step 333/1000 | Loss: 1.4972\n",
            "Step 334/1000 | Loss: 1.2840\n",
            "Step 335/1000 | Loss: 1.2840\n",
            "Step 336/1000 | Loss: 1.4782\n",
            "Step 337/1000 | Loss: 1.2355\n",
            "Step 338/1000 | Loss: 1.4072\n",
            "Step 339/1000 | Loss: 1.3818\n",
            "Step 340/1000 | Loss: 1.3528\n",
            "Step 341/1000 | Loss: 1.4299\n",
            "Step 342/1000 | Loss: 1.1831\n",
            "Step 343/1000 | Loss: 1.3934\n",
            "Step 344/1000 | Loss: 1.5745\n",
            "Step 345/1000 | Loss: 1.2439\n",
            "Step 346/1000 | Loss: 1.0660\n",
            "Step 347/1000 | Loss: 1.3556\n",
            "Step 348/1000 | Loss: 1.4956\n",
            "Step 349/1000 | Loss: 1.5382\n",
            "Step 350/1000 | Loss: 1.5024\n",
            "Step 351/1000 | Loss: 1.5833\n",
            "Step 352/1000 | Loss: 1.6348\n",
            "Step 353/1000 | Loss: 1.6080\n",
            "Step 354/1000 | Loss: 1.3995\n",
            "Step 355/1000 | Loss: 1.4021\n",
            "Step 356/1000 | Loss: 1.4535\n",
            "Step 357/1000 | Loss: 1.5820\n",
            "Step 358/1000 | Loss: 1.5754\n",
            "Step 359/1000 | Loss: 1.3811\n",
            "Step 360/1000 | Loss: 1.4676\n",
            "Step 361/1000 | Loss: 1.5082\n",
            "Step 362/1000 | Loss: 1.7686\n",
            "Step 363/1000 | Loss: 1.5373\n",
            "Step 364/1000 | Loss: 1.6378\n",
            "Step 365/1000 | Loss: 1.6065\n",
            "Step 366/1000 | Loss: 1.6970\n",
            "Step 367/1000 | Loss: 1.7258\n",
            "Step 368/1000 | Loss: 1.8150\n",
            "Step 369/1000 | Loss: 1.6297\n",
            "Step 370/1000 | Loss: 1.7074\n",
            "Step 371/1000 | Loss: 1.6879\n",
            "Step 372/1000 | Loss: 1.9226\n",
            "Step 373/1000 | Loss: 1.3704\n",
            "Step 374/1000 | Loss: 1.5774\n",
            "Step 375/1000 | Loss: 1.4927\n",
            "Step 376/1000 | Loss: 1.5214\n",
            "Step 377/1000 | Loss: 1.6991\n",
            "Step 378/1000 | Loss: 1.5792\n",
            "Step 379/1000 | Loss: 1.5745\n",
            "Step 380/1000 | Loss: 1.6867\n",
            "Step 381/1000 | Loss: 1.6457\n",
            "Step 382/1000 | Loss: 1.6077\n",
            "Step 383/1000 | Loss: 1.6440\n",
            "Step 384/1000 | Loss: 1.8530\n",
            "Step 385/1000 | Loss: 1.6545\n",
            "Step 386/1000 | Loss: 1.4736\n",
            "Step 387/1000 | Loss: 1.4525\n",
            "Step 388/1000 | Loss: 1.6808\n",
            "Step 389/1000 | Loss: 1.5884\n",
            "Step 390/1000 | Loss: 1.5922\n",
            "Step 391/1000 | Loss: 1.7800\n",
            "Step 392/1000 | Loss: 1.7626\n",
            "Step 393/1000 | Loss: 1.5538\n",
            "Step 394/1000 | Loss: 1.5274\n",
            "Step 395/1000 | Loss: 1.3921\n",
            "Step 396/1000 | Loss: 1.5397\n",
            "Step 397/1000 | Loss: 1.3780\n",
            "Step 398/1000 | Loss: 1.3979\n",
            "Step 399/1000 | Loss: 1.4622\n",
            "Step 400/1000 | Loss: 1.4282\n",
            "Step 401/1000 | Loss: 1.3118\n",
            "Step 402/1000 | Loss: 1.5112\n",
            "Step 403/1000 | Loss: 1.5595\n",
            "Step 404/1000 | Loss: 1.5394\n",
            "Step 405/1000 | Loss: 1.5985\n",
            "Step 406/1000 | Loss: 1.7756\n",
            "Step 407/1000 | Loss: 1.5497\n",
            "Step 408/1000 | Loss: 1.5152\n",
            "Step 409/1000 | Loss: 1.5956\n",
            "Step 410/1000 | Loss: 1.7302\n",
            "Step 411/1000 | Loss: 1.5852\n",
            "Step 412/1000 | Loss: 1.3679\n",
            "Step 413/1000 | Loss: 1.5353\n",
            "Step 414/1000 | Loss: 1.5799\n",
            "Step 415/1000 | Loss: 1.4478\n",
            "Step 416/1000 | Loss: 1.4579\n",
            "Step 417/1000 | Loss: 1.4591\n",
            "Step 418/1000 | Loss: 1.4067\n",
            "Step 419/1000 | Loss: 1.3735\n",
            "Step 420/1000 | Loss: 1.3636\n",
            "Step 421/1000 | Loss: 1.2823\n",
            "Step 422/1000 | Loss: 1.4958\n",
            "Step 423/1000 | Loss: 1.2605\n",
            "Step 424/1000 | Loss: 1.6100\n",
            "Step 425/1000 | Loss: 1.4399\n",
            "Step 426/1000 | Loss: 1.7233\n",
            "Step 427/1000 | Loss: 1.5080\n",
            "Step 428/1000 | Loss: 1.8572\n",
            "Step 429/1000 | Loss: 1.5506\n",
            "Step 430/1000 | Loss: 1.5821\n",
            "Step 431/1000 | Loss: 1.5526\n",
            "Step 432/1000 | Loss: 1.5143\n",
            "Step 433/1000 | Loss: 1.5477\n",
            "Step 434/1000 | Loss: 1.2330\n",
            "Step 435/1000 | Loss: 1.3657\n",
            "Step 436/1000 | Loss: 1.4974\n",
            "Step 437/1000 | Loss: 1.4441\n",
            "Step 438/1000 | Loss: 1.0798\n",
            "Step 439/1000 | Loss: 1.3870\n",
            "Step 440/1000 | Loss: 1.5712\n",
            "Step 441/1000 | Loss: 1.7254\n",
            "Step 442/1000 | Loss: 1.3837\n",
            "Step 443/1000 | Loss: 1.4709\n",
            "Step 444/1000 | Loss: 1.5583\n",
            "Step 445/1000 | Loss: 1.3429\n",
            "Step 446/1000 | Loss: 1.5500\n",
            "Step 447/1000 | Loss: 1.5637\n",
            "Step 448/1000 | Loss: 1.4233\n",
            "Step 449/1000 | Loss: 1.2875\n",
            "Step 450/1000 | Loss: 1.3012\n",
            "Step 451/1000 | Loss: 1.4482\n",
            "Step 452/1000 | Loss: 1.2723\n",
            "Step 453/1000 | Loss: 1.3241\n",
            "Step 454/1000 | Loss: 1.2289\n",
            "Step 455/1000 | Loss: 1.2467\n",
            "Step 456/1000 | Loss: 1.2999\n",
            "Step 457/1000 | Loss: 1.1976\n",
            "Step 458/1000 | Loss: 1.1836\n",
            "Step 459/1000 | Loss: 1.2895\n",
            "Step 460/1000 | Loss: 1.5044\n",
            "Step 461/1000 | Loss: 1.4882\n",
            "Step 462/1000 | Loss: 1.4997\n",
            "Step 463/1000 | Loss: 1.6214\n",
            "Step 464/1000 | Loss: 1.4276\n",
            "Step 465/1000 | Loss: 1.5533\n",
            "Step 466/1000 | Loss: 1.4647\n",
            "Step 467/1000 | Loss: 1.2703\n",
            "Step 468/1000 | Loss: 1.3604\n",
            "Step 469/1000 | Loss: 1.3277\n",
            "Step 470/1000 | Loss: 1.3467\n",
            "Step 471/1000 | Loss: 1.6627\n",
            "Step 472/1000 | Loss: 1.5044\n",
            "Step 473/1000 | Loss: 1.7456\n",
            "Step 474/1000 | Loss: 1.3957\n",
            "Step 475/1000 | Loss: 1.5258\n",
            "Step 476/1000 | Loss: 1.3415\n",
            "Step 477/1000 | Loss: 1.6147\n",
            "Step 478/1000 | Loss: 1.3876\n",
            "Step 479/1000 | Loss: 1.4852\n",
            "Step 480/1000 | Loss: 1.4171\n",
            "Step 481/1000 | Loss: 1.5050\n",
            "Step 482/1000 | Loss: 1.8142\n",
            "Step 483/1000 | Loss: 1.4496\n",
            "Step 484/1000 | Loss: 1.5692\n",
            "Step 485/1000 | Loss: 1.5667\n",
            "Step 486/1000 | Loss: 1.4809\n",
            "Step 487/1000 | Loss: 1.4811\n",
            "Step 488/1000 | Loss: 1.3520\n",
            "Step 489/1000 | Loss: 1.3910\n",
            "Step 490/1000 | Loss: 1.4658\n",
            "Step 491/1000 | Loss: 1.1998\n",
            "Step 492/1000 | Loss: 1.5709\n",
            "Step 493/1000 | Loss: 1.2655\n",
            "Step 494/1000 | Loss: 1.1449\n",
            "Step 495/1000 | Loss: 1.3891\n",
            "Step 496/1000 | Loss: 1.3091\n",
            "Step 497/1000 | Loss: 1.3470\n",
            "Step 498/1000 | Loss: 1.4365\n",
            "Step 499/1000 | Loss: 1.3280\n",
            "Step 500/1000 | Loss: 1.2422\n",
            "Step 501/1000 | Loss: 1.4094\n",
            "Step 502/1000 | Loss: 1.2811\n",
            "Step 503/1000 | Loss: 1.3300\n",
            "Step 504/1000 | Loss: 1.2612\n",
            "Step 505/1000 | Loss: 1.4099\n",
            "Step 506/1000 | Loss: 1.3807\n",
            "Step 507/1000 | Loss: 1.3528\n",
            "Step 508/1000 | Loss: 1.3864\n",
            "Step 509/1000 | Loss: 1.1635\n",
            "Step 510/1000 | Loss: 1.4688\n",
            "Step 511/1000 | Loss: 1.4425\n",
            "Step 512/1000 | Loss: 1.3515\n",
            "Step 513/1000 | Loss: 1.3390\n",
            "Step 514/1000 | Loss: 1.5518\n",
            "Step 515/1000 | Loss: 1.3995\n",
            "Step 516/1000 | Loss: 1.3505\n",
            "Step 517/1000 | Loss: 1.6104\n",
            "Step 518/1000 | Loss: 1.1765\n",
            "Step 519/1000 | Loss: 1.4495\n",
            "Step 520/1000 | Loss: 0.9536\n",
            "Step 521/1000 | Loss: 0.9455\n",
            "Step 522/1000 | Loss: 1.1670\n",
            "Step 523/1000 | Loss: 1.4118\n",
            "Step 524/1000 | Loss: 1.1399\n",
            "Step 525/1000 | Loss: 1.2424\n",
            "Step 526/1000 | Loss: 1.2088\n",
            "Step 527/1000 | Loss: 1.1523\n",
            "Step 528/1000 | Loss: 1.3510\n",
            "Step 529/1000 | Loss: 1.2145\n",
            "Step 530/1000 | Loss: 1.1760\n",
            "Step 531/1000 | Loss: 1.1283\n",
            "Step 532/1000 | Loss: 1.2002\n",
            "Step 533/1000 | Loss: 1.3135\n",
            "Step 534/1000 | Loss: 1.3430\n",
            "Step 535/1000 | Loss: 1.1599\n",
            "Step 536/1000 | Loss: 1.1439\n",
            "Step 537/1000 | Loss: 1.2489\n",
            "Step 538/1000 | Loss: 0.9907\n",
            "Step 539/1000 | Loss: 1.1692\n",
            "Step 540/1000 | Loss: 1.2730\n",
            "Step 541/1000 | Loss: 1.2405\n",
            "Step 542/1000 | Loss: 1.2471\n",
            "Step 543/1000 | Loss: 1.3790\n",
            "Step 544/1000 | Loss: 1.5004\n",
            "Step 545/1000 | Loss: 1.3807\n",
            "Step 546/1000 | Loss: 1.4763\n",
            "Step 547/1000 | Loss: 1.3074\n",
            "Step 548/1000 | Loss: 1.3167\n",
            "Step 549/1000 | Loss: 1.2781\n",
            "Step 550/1000 | Loss: 1.2113\n",
            "Step 551/1000 | Loss: 1.5236\n",
            "Step 552/1000 | Loss: 1.3299\n",
            "Step 553/1000 | Loss: 1.1288\n",
            "Step 554/1000 | Loss: 1.2073\n",
            "Step 555/1000 | Loss: 1.0711\n",
            "Step 556/1000 | Loss: 1.1759\n",
            "Step 557/1000 | Loss: 1.1963\n",
            "Step 558/1000 | Loss: 1.2456\n",
            "Step 559/1000 | Loss: 1.6025\n",
            "Step 560/1000 | Loss: 1.5644\n",
            "Step 561/1000 | Loss: 1.4063\n",
            "Step 562/1000 | Loss: 1.3666\n",
            "Step 563/1000 | Loss: 1.3587\n",
            "Step 564/1000 | Loss: 1.6201\n",
            "Step 565/1000 | Loss: 1.6246\n",
            "Step 566/1000 | Loss: 1.4955\n",
            "Step 567/1000 | Loss: 1.5203\n",
            "Step 568/1000 | Loss: 1.4287\n",
            "Step 569/1000 | Loss: 1.5795\n",
            "Step 570/1000 | Loss: 1.4453\n",
            "Step 571/1000 | Loss: 1.3094\n",
            "Step 572/1000 | Loss: 1.5559\n",
            "Step 573/1000 | Loss: 1.4942\n",
            "Step 574/1000 | Loss: 1.4399\n",
            "Step 575/1000 | Loss: 1.4920\n",
            "Step 576/1000 | Loss: 1.5962\n",
            "Step 577/1000 | Loss: 1.4287\n",
            "Step 578/1000 | Loss: 1.4185\n",
            "Step 579/1000 | Loss: 1.2587\n",
            "Step 580/1000 | Loss: 1.4400\n",
            "Step 581/1000 | Loss: 1.5522\n",
            "Step 582/1000 | Loss: 1.5664\n",
            "Step 583/1000 | Loss: 1.5713\n",
            "Step 584/1000 | Loss: 1.3955\n",
            "Step 585/1000 | Loss: 1.4051\n",
            "Step 586/1000 | Loss: 1.2707\n",
            "Step 587/1000 | Loss: 1.4893\n",
            "Step 588/1000 | Loss: 1.2748\n",
            "Step 589/1000 | Loss: 1.4298\n",
            "Step 590/1000 | Loss: 1.4066\n",
            "Step 591/1000 | Loss: 1.3427\n",
            "Step 592/1000 | Loss: 1.2091\n",
            "Step 593/1000 | Loss: 1.3275\n",
            "Step 594/1000 | Loss: 1.1842\n",
            "Step 595/1000 | Loss: 1.0929\n",
            "Step 596/1000 | Loss: 1.0136\n",
            "Step 597/1000 | Loss: 1.0579\n",
            "Step 598/1000 | Loss: 1.5329\n",
            "Step 599/1000 | Loss: 1.4493\n",
            "Step 600/1000 | Loss: 1.2565\n",
            "Step 601/1000 | Loss: 1.5564\n",
            "Step 602/1000 | Loss: 1.2838\n",
            "Step 603/1000 | Loss: 1.4050\n",
            "Step 604/1000 | Loss: 1.4604\n",
            "Step 605/1000 | Loss: 1.3268\n",
            "Step 606/1000 | Loss: 1.4133\n",
            "Step 607/1000 | Loss: 1.8611\n",
            "Step 608/1000 | Loss: 1.3283\n",
            "Step 609/1000 | Loss: 1.3110\n",
            "Step 610/1000 | Loss: 1.4974\n",
            "Step 611/1000 | Loss: 1.3554\n",
            "Step 612/1000 | Loss: 1.1746\n",
            "Step 613/1000 | Loss: 1.6213\n",
            "Step 614/1000 | Loss: 1.3700\n",
            "Step 615/1000 | Loss: 1.2615\n",
            "Step 616/1000 | Loss: 1.2166\n",
            "Step 617/1000 | Loss: 1.3086\n",
            "Step 618/1000 | Loss: 1.3749\n",
            "Step 619/1000 | Loss: 1.4224\n",
            "Step 620/1000 | Loss: 1.3473\n",
            "Step 621/1000 | Loss: 1.6305\n",
            "Step 622/1000 | Loss: 1.3638\n",
            "Step 623/1000 | Loss: 1.3930\n",
            "Step 624/1000 | Loss: 1.4118\n",
            "Step 625/1000 | Loss: 1.2735\n",
            "Step 626/1000 | Loss: 1.4611\n",
            "Step 627/1000 | Loss: 1.1988\n",
            "Step 628/1000 | Loss: 1.4478\n",
            "Step 629/1000 | Loss: 1.2118\n",
            "Step 630/1000 | Loss: 1.4729\n",
            "Step 631/1000 | Loss: 1.2969\n",
            "Step 632/1000 | Loss: 1.4360\n",
            "Step 633/1000 | Loss: 1.1628\n",
            "Step 634/1000 | Loss: 1.5269\n",
            "Step 635/1000 | Loss: 1.7041\n",
            "Step 636/1000 | Loss: 1.6303\n",
            "Step 637/1000 | Loss: 1.5463\n",
            "Step 638/1000 | Loss: 1.3308\n",
            "Step 639/1000 | Loss: 1.5010\n",
            "Step 640/1000 | Loss: 1.3533\n",
            "Step 641/1000 | Loss: 1.4197\n",
            "Step 642/1000 | Loss: 1.3355\n",
            "Step 643/1000 | Loss: 1.3450\n",
            "Step 644/1000 | Loss: 1.3615\n",
            "Step 645/1000 | Loss: 1.3941\n",
            "Step 646/1000 | Loss: 1.6458\n",
            "Step 647/1000 | Loss: 1.5338\n",
            "Step 648/1000 | Loss: 1.5150\n",
            "Step 649/1000 | Loss: 1.4326\n",
            "Step 650/1000 | Loss: 1.4690\n",
            "Step 651/1000 | Loss: 1.4312\n",
            "Step 652/1000 | Loss: 1.3075\n",
            "Step 653/1000 | Loss: 1.3793\n",
            "Step 654/1000 | Loss: 1.4759\n",
            "Step 655/1000 | Loss: 1.3474\n",
            "Step 656/1000 | Loss: 1.4892\n",
            "Step 657/1000 | Loss: 1.4460\n",
            "Step 658/1000 | Loss: 1.1349\n",
            "Step 659/1000 | Loss: 1.0467\n",
            "Step 660/1000 | Loss: 1.3751\n",
            "Step 661/1000 | Loss: 1.5163\n",
            "Step 662/1000 | Loss: 1.5059\n",
            "Step 663/1000 | Loss: 1.3706\n",
            "Step 664/1000 | Loss: 1.2797\n",
            "Step 665/1000 | Loss: 1.3163\n",
            "Step 666/1000 | Loss: 1.4401\n",
            "Step 667/1000 | Loss: 1.2576\n",
            "Step 668/1000 | Loss: 1.4221\n",
            "Step 669/1000 | Loss: 1.3158\n",
            "Step 670/1000 | Loss: 1.3002\n",
            "Step 671/1000 | Loss: 1.4760\n",
            "Step 672/1000 | Loss: 1.1979\n",
            "Step 673/1000 | Loss: 1.4410\n",
            "Step 674/1000 | Loss: 1.4398\n",
            "Step 675/1000 | Loss: 1.2101\n",
            "Step 676/1000 | Loss: 1.0404\n",
            "Step 677/1000 | Loss: 1.4679\n",
            "Step 678/1000 | Loss: 1.5212\n",
            "Step 679/1000 | Loss: 1.6675\n",
            "Step 680/1000 | Loss: 1.5587\n",
            "Step 681/1000 | Loss: 1.6229\n",
            "Step 682/1000 | Loss: 1.5748\n",
            "Step 683/1000 | Loss: 1.5652\n",
            "Step 684/1000 | Loss: 1.4291\n",
            "Step 685/1000 | Loss: 1.4620\n",
            "Step 686/1000 | Loss: 1.5017\n",
            "Step 687/1000 | Loss: 1.6341\n",
            "Step 688/1000 | Loss: 1.7216\n",
            "Step 689/1000 | Loss: 1.4784\n",
            "Step 690/1000 | Loss: 1.4994\n",
            "Step 691/1000 | Loss: 1.4916\n",
            "Step 692/1000 | Loss: 1.7596\n",
            "Step 693/1000 | Loss: 1.4562\n",
            "Step 694/1000 | Loss: 1.6679\n",
            "Step 695/1000 | Loss: 1.7398\n",
            "Step 696/1000 | Loss: 1.8551\n",
            "Step 697/1000 | Loss: 1.8207\n",
            "Step 698/1000 | Loss: 1.9428\n",
            "Step 699/1000 | Loss: 1.6724\n",
            "Step 700/1000 | Loss: 1.7798\n",
            "Step 701/1000 | Loss: 1.6939\n",
            "Step 702/1000 | Loss: 1.9181\n",
            "Step 703/1000 | Loss: 1.3847\n",
            "Step 704/1000 | Loss: 1.5433\n",
            "Step 705/1000 | Loss: 1.5799\n",
            "Step 706/1000 | Loss: 1.5815\n",
            "Step 707/1000 | Loss: 1.6627\n",
            "Step 708/1000 | Loss: 1.5288\n",
            "Step 709/1000 | Loss: 1.6113\n",
            "Step 710/1000 | Loss: 1.7509\n",
            "Step 711/1000 | Loss: 1.6246\n",
            "Step 712/1000 | Loss: 1.7365\n",
            "Step 713/1000 | Loss: 1.5922\n",
            "Step 714/1000 | Loss: 1.9293\n",
            "Step 715/1000 | Loss: 1.6926\n",
            "Step 716/1000 | Loss: 1.4739\n",
            "Step 717/1000 | Loss: 1.4834\n",
            "Step 718/1000 | Loss: 1.6437\n",
            "Step 719/1000 | Loss: 1.5369\n",
            "Step 720/1000 | Loss: 1.5239\n",
            "Step 721/1000 | Loss: 1.6939\n",
            "Step 722/1000 | Loss: 1.7524\n",
            "Step 723/1000 | Loss: 1.5650\n",
            "Step 724/1000 | Loss: 1.5278\n",
            "Step 725/1000 | Loss: 1.3493\n",
            "Step 726/1000 | Loss: 1.4689\n",
            "Step 727/1000 | Loss: 1.3942\n",
            "Step 728/1000 | Loss: 1.3554\n",
            "Step 729/1000 | Loss: 1.5115\n",
            "Step 730/1000 | Loss: 1.4080\n",
            "Step 731/1000 | Loss: 1.4209\n",
            "Step 732/1000 | Loss: 1.5121\n",
            "Step 733/1000 | Loss: 1.5334\n",
            "Step 734/1000 | Loss: 1.4467\n",
            "Step 735/1000 | Loss: 1.6193\n",
            "Step 736/1000 | Loss: 1.6775\n",
            "Step 737/1000 | Loss: 1.5663\n",
            "Step 738/1000 | Loss: 1.4044\n",
            "Step 739/1000 | Loss: 1.5431\n",
            "Step 740/1000 | Loss: 1.7745\n",
            "Step 741/1000 | Loss: 1.5347\n",
            "Step 742/1000 | Loss: 1.2773\n",
            "Step 743/1000 | Loss: 1.5535\n",
            "Step 744/1000 | Loss: 1.6055\n",
            "Step 745/1000 | Loss: 1.3992\n",
            "Step 746/1000 | Loss: 1.4279\n",
            "Step 747/1000 | Loss: 1.4288\n",
            "Step 748/1000 | Loss: 1.3924\n",
            "Step 749/1000 | Loss: 1.4323\n",
            "Step 750/1000 | Loss: 1.4274\n",
            "Step 751/1000 | Loss: 1.2823\n",
            "Step 752/1000 | Loss: 1.5101\n",
            "Step 753/1000 | Loss: 1.1682\n",
            "Step 754/1000 | Loss: 1.6002\n",
            "Step 755/1000 | Loss: 1.4789\n",
            "Step 756/1000 | Loss: 1.6272\n",
            "Step 757/1000 | Loss: 1.3654\n",
            "Step 758/1000 | Loss: 1.7604\n",
            "Step 759/1000 | Loss: 1.5809\n",
            "Step 760/1000 | Loss: 1.5112\n",
            "Step 761/1000 | Loss: 1.5073\n",
            "Step 762/1000 | Loss: 1.6192\n",
            "Step 763/1000 | Loss: 1.5763\n",
            "Step 764/1000 | Loss: 1.2491\n",
            "Step 765/1000 | Loss: 1.3354\n",
            "Step 766/1000 | Loss: 1.4445\n",
            "Step 767/1000 | Loss: 1.4536\n",
            "Step 768/1000 | Loss: 1.0541\n",
            "Step 769/1000 | Loss: 1.4017\n",
            "Step 770/1000 | Loss: 1.6450\n",
            "Step 771/1000 | Loss: 1.7531\n",
            "Step 772/1000 | Loss: 1.4345\n",
            "Step 773/1000 | Loss: 1.4939\n",
            "Step 774/1000 | Loss: 1.4201\n",
            "Step 775/1000 | Loss: 1.3652\n",
            "Step 776/1000 | Loss: 1.4825\n",
            "Step 777/1000 | Loss: 1.5577\n",
            "Step 778/1000 | Loss: 1.4496\n",
            "Step 779/1000 | Loss: 1.2512\n",
            "Step 780/1000 | Loss: 1.3140\n",
            "Step 781/1000 | Loss: 1.4118\n",
            "Step 782/1000 | Loss: 1.2179\n",
            "Step 783/1000 | Loss: 1.2568\n",
            "Step 784/1000 | Loss: 1.2093\n",
            "Step 785/1000 | Loss: 1.1966\n",
            "Step 786/1000 | Loss: 1.2617\n",
            "Step 787/1000 | Loss: 1.2495\n",
            "Step 788/1000 | Loss: 1.1159\n",
            "Step 789/1000 | Loss: 1.2632\n",
            "Step 790/1000 | Loss: 1.4775\n",
            "Step 791/1000 | Loss: 1.5064\n",
            "Step 792/1000 | Loss: 1.4833\n",
            "Step 793/1000 | Loss: 1.6877\n",
            "Step 794/1000 | Loss: 1.4017\n",
            "Step 795/1000 | Loss: 1.5261\n",
            "Step 796/1000 | Loss: 1.5939\n",
            "Step 797/1000 | Loss: 1.2185\n",
            "Step 798/1000 | Loss: 1.3071\n",
            "Step 799/1000 | Loss: 1.2862\n",
            "Step 800/1000 | Loss: 1.3621\n",
            "Step 801/1000 | Loss: 1.6316\n",
            "Step 802/1000 | Loss: 1.5323\n",
            "Step 803/1000 | Loss: 1.7222\n",
            "Step 804/1000 | Loss: 1.5153\n",
            "Step 805/1000 | Loss: 1.4875\n",
            "Step 806/1000 | Loss: 1.3833\n",
            "Step 807/1000 | Loss: 1.5479\n",
            "Step 808/1000 | Loss: 1.4229\n",
            "Step 809/1000 | Loss: 1.4990\n",
            "Step 810/1000 | Loss: 1.4619\n",
            "Step 811/1000 | Loss: 1.4634\n",
            "Step 812/1000 | Loss: 1.7671\n",
            "Step 813/1000 | Loss: 1.3505\n",
            "Step 814/1000 | Loss: 1.5831\n",
            "Step 815/1000 | Loss: 1.5036\n",
            "Step 816/1000 | Loss: 1.5337\n",
            "Step 817/1000 | Loss: 1.4870\n",
            "Step 818/1000 | Loss: 1.4403\n",
            "Step 819/1000 | Loss: 1.3330\n",
            "Step 820/1000 | Loss: 1.4335\n",
            "Step 821/1000 | Loss: 1.1397\n",
            "Step 822/1000 | Loss: 1.4883\n",
            "Step 823/1000 | Loss: 1.2672\n",
            "Step 824/1000 | Loss: 1.1174\n",
            "Step 825/1000 | Loss: 1.3926\n",
            "Step 826/1000 | Loss: 1.2933\n",
            "Step 827/1000 | Loss: 1.3097\n",
            "Step 828/1000 | Loss: 1.2706\n",
            "Step 829/1000 | Loss: 1.3119\n",
            "Step 830/1000 | Loss: 1.1717\n",
            "Step 831/1000 | Loss: 1.4048\n",
            "Step 832/1000 | Loss: 1.4026\n",
            "Step 833/1000 | Loss: 1.3092\n",
            "Step 834/1000 | Loss: 1.2916\n",
            "Step 835/1000 | Loss: 1.4869\n",
            "Step 836/1000 | Loss: 1.2978\n",
            "Step 837/1000 | Loss: 1.2799\n",
            "Step 838/1000 | Loss: 1.2831\n",
            "Step 839/1000 | Loss: 1.0809\n",
            "Step 840/1000 | Loss: 1.4070\n",
            "Step 841/1000 | Loss: 1.3949\n",
            "Step 842/1000 | Loss: 1.3505\n",
            "Step 843/1000 | Loss: 1.2809\n",
            "Step 844/1000 | Loss: 1.4971\n",
            "Step 845/1000 | Loss: 1.3612\n",
            "Step 846/1000 | Loss: 1.3571\n",
            "Step 847/1000 | Loss: 1.5339\n",
            "Step 848/1000 | Loss: 1.2406\n",
            "Step 849/1000 | Loss: 1.4168\n",
            "Step 850/1000 | Loss: 1.0479\n",
            "Step 851/1000 | Loss: 0.9703\n",
            "Step 852/1000 | Loss: 1.1949\n",
            "Step 853/1000 | Loss: 1.4695\n",
            "Step 854/1000 | Loss: 1.1897\n",
            "Step 855/1000 | Loss: 1.2315\n",
            "Step 856/1000 | Loss: 1.2136\n",
            "Step 857/1000 | Loss: 1.2039\n",
            "Step 858/1000 | Loss: 1.3733\n",
            "Step 859/1000 | Loss: 1.2213\n",
            "Step 860/1000 | Loss: 1.2409\n",
            "Step 861/1000 | Loss: 1.2189\n",
            "Step 862/1000 | Loss: 1.2801\n",
            "Step 863/1000 | Loss: 1.3432\n",
            "Step 864/1000 | Loss: 1.3033\n",
            "Step 865/1000 | Loss: 1.3016\n",
            "Step 866/1000 | Loss: 1.1797\n",
            "Step 867/1000 | Loss: 1.2034\n",
            "Step 868/1000 | Loss: 1.0406\n",
            "Step 869/1000 | Loss: 1.1469\n",
            "Step 870/1000 | Loss: 1.2272\n",
            "Step 871/1000 | Loss: 1.1432\n",
            "Step 872/1000 | Loss: 1.1938\n",
            "Step 873/1000 | Loss: 1.3696\n",
            "Step 874/1000 | Loss: 1.5851\n",
            "Step 875/1000 | Loss: 1.3663\n",
            "Step 876/1000 | Loss: 1.5659\n",
            "Step 877/1000 | Loss: 1.4200\n",
            "Step 878/1000 | Loss: 1.3562\n",
            "Step 879/1000 | Loss: 1.2842\n",
            "Step 880/1000 | Loss: 1.1934\n",
            "Step 881/1000 | Loss: 1.4424\n",
            "Step 882/1000 | Loss: 1.3818\n",
            "Step 883/1000 | Loss: 1.0657\n",
            "Step 884/1000 | Loss: 1.2141\n",
            "Step 885/1000 | Loss: 1.0050\n",
            "Step 886/1000 | Loss: 1.1582\n",
            "Step 887/1000 | Loss: 1.3231\n",
            "Step 888/1000 | Loss: 1.2545\n",
            "Step 889/1000 | Loss: 1.6396\n",
            "Step 890/1000 | Loss: 1.6065\n",
            "Step 891/1000 | Loss: 1.4173\n",
            "Step 892/1000 | Loss: 1.3440\n",
            "Step 893/1000 | Loss: 1.3213\n",
            "Step 894/1000 | Loss: 1.5287\n",
            "Step 895/1000 | Loss: 1.5800\n",
            "Step 896/1000 | Loss: 1.5845\n",
            "Step 897/1000 | Loss: 1.4484\n",
            "Step 898/1000 | Loss: 1.4265\n",
            "Step 899/1000 | Loss: 1.5967\n",
            "Step 900/1000 | Loss: 1.4052\n",
            "Step 901/1000 | Loss: 1.3581\n",
            "Step 902/1000 | Loss: 1.4948\n",
            "Step 903/1000 | Loss: 1.4457\n",
            "Step 904/1000 | Loss: 1.3663\n",
            "Step 905/1000 | Loss: 1.4055\n",
            "Step 906/1000 | Loss: 1.5874\n",
            "Step 907/1000 | Loss: 1.4263\n",
            "Step 908/1000 | Loss: 1.4105\n",
            "Step 909/1000 | Loss: 1.1746\n",
            "Step 910/1000 | Loss: 1.3896\n",
            "Step 911/1000 | Loss: 1.4728\n",
            "Step 912/1000 | Loss: 1.5106\n",
            "Step 913/1000 | Loss: 1.4687\n",
            "Step 914/1000 | Loss: 1.3162\n",
            "Step 915/1000 | Loss: 1.4350\n",
            "Step 916/1000 | Loss: 1.3367\n",
            "Step 917/1000 | Loss: 1.4308\n",
            "Step 918/1000 | Loss: 1.2403\n",
            "Step 919/1000 | Loss: 1.3930\n",
            "Step 920/1000 | Loss: 1.3937\n",
            "Step 921/1000 | Loss: 1.2618\n",
            "Step 922/1000 | Loss: 1.2905\n",
            "Step 923/1000 | Loss: 1.3373\n",
            "Step 924/1000 | Loss: 1.1319\n",
            "Step 925/1000 | Loss: 1.0731\n",
            "Step 926/1000 | Loss: 1.1305\n",
            "Step 927/1000 | Loss: 1.0869\n",
            "Step 928/1000 | Loss: 1.4905\n",
            "Step 929/1000 | Loss: 1.4542\n",
            "Step 930/1000 | Loss: 1.3112\n",
            "Step 931/1000 | Loss: 1.5308\n",
            "Step 932/1000 | Loss: 1.3188\n",
            "Step 933/1000 | Loss: 1.4123\n",
            "Step 934/1000 | Loss: 1.4370\n",
            "Step 935/1000 | Loss: 1.3082\n",
            "Step 936/1000 | Loss: 1.3970\n",
            "Step 937/1000 | Loss: 1.7834\n",
            "Step 938/1000 | Loss: 1.3075\n",
            "Step 939/1000 | Loss: 1.2756\n",
            "Step 940/1000 | Loss: 1.4596\n",
            "Step 941/1000 | Loss: 1.2847\n",
            "Step 942/1000 | Loss: 1.1136\n",
            "Step 943/1000 | Loss: 1.5775\n",
            "Step 944/1000 | Loss: 1.3901\n",
            "Step 945/1000 | Loss: 1.2715\n",
            "Step 946/1000 | Loss: 1.2563\n",
            "Step 947/1000 | Loss: 1.2806\n",
            "Step 948/1000 | Loss: 1.2634\n",
            "Step 949/1000 | Loss: 1.2726\n",
            "Step 950/1000 | Loss: 1.2539\n",
            "Step 951/1000 | Loss: 1.6079\n",
            "Step 952/1000 | Loss: 1.3243\n",
            "Step 953/1000 | Loss: 1.3662\n",
            "Step 954/1000 | Loss: 1.4920\n",
            "Step 955/1000 | Loss: 1.3065\n",
            "Step 956/1000 | Loss: 1.4034\n",
            "Step 957/1000 | Loss: 1.1489\n",
            "Step 958/1000 | Loss: 1.3611\n",
            "Step 959/1000 | Loss: 1.2165\n",
            "Step 960/1000 | Loss: 1.4275\n",
            "Step 961/1000 | Loss: 1.3265\n",
            "Step 962/1000 | Loss: 1.3734\n",
            "Step 963/1000 | Loss: 1.2613\n",
            "Step 964/1000 | Loss: 1.4795\n",
            "Step 965/1000 | Loss: 1.5994\n",
            "Step 966/1000 | Loss: 1.4811\n",
            "Step 967/1000 | Loss: 1.5724\n",
            "Step 968/1000 | Loss: 1.3392\n",
            "Step 969/1000 | Loss: 1.5062\n",
            "Step 970/1000 | Loss: 1.3688\n",
            "Step 971/1000 | Loss: 1.3311\n",
            "Step 972/1000 | Loss: 1.3133\n",
            "Step 973/1000 | Loss: 1.3402\n",
            "Step 974/1000 | Loss: 1.3955\n",
            "Step 975/1000 | Loss: 1.4192\n",
            "Step 976/1000 | Loss: 1.6479\n",
            "Step 977/1000 | Loss: 1.4840\n",
            "Step 978/1000 | Loss: 1.4916\n",
            "Step 979/1000 | Loss: 1.4855\n",
            "Step 980/1000 | Loss: 1.5265\n",
            "Step 981/1000 | Loss: 1.4926\n",
            "Step 982/1000 | Loss: 1.2368\n",
            "Step 983/1000 | Loss: 1.3931\n",
            "Step 984/1000 | Loss: 1.4834\n",
            "Step 985/1000 | Loss: 1.3708\n",
            "Step 986/1000 | Loss: 1.4410\n",
            "Step 987/1000 | Loss: 1.3848\n",
            "Step 988/1000 | Loss: 1.1242\n",
            "Step 989/1000 | Loss: 0.9280\n",
            "Step 990/1000 | Loss: 1.2333\n",
            "Step 991/1000 | Loss: 1.4564\n",
            "Step 992/1000 | Loss: 1.4952\n",
            "Step 993/1000 | Loss: 1.3785\n",
            "Step 994/1000 | Loss: 1.2973\n",
            "Step 995/1000 | Loss: 1.2897\n",
            "Step 996/1000 | Loss: 1.4333\n",
            "Step 997/1000 | Loss: 1.1967\n",
            "Step 998/1000 | Loss: 1.3812\n",
            "Step 999/1000 | Loss: 1.3967\n",
            "Step 1000/1000 | Loss: 1.3810\n",
            "Training completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import time\n",
        "import tiktoken  # Assuming this is a utility for tokenization\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Set float32 matmul precision\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024  # reduced max sequence length to fit into 4GB GPU\n",
        "    vocab_size: int = 50304  # number of tokens\n",
        "    n_layer: int = 6  # increased number of layers for better learning\n",
        "    n_head: int = 8  # increased number of heads for better learning\n",
        "    n_embd: int = 256  # increased embedding dimension for better learning\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "# Initialize model and prepare for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(42)\n",
        "model = GPT(GPTConfig()).to(device)\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "train_loader = DataLoaderLite(B=4, T=256)  # Adjust batch size and sequence length as needed\n",
        "\n",
        "# Cosine learning rate decay schedule\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 1000\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for step in range(max_steps):\n",
        "        t0 = time.time()\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Automatic mixed precision\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Adjust learning rate\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        optimizer.step()\n",
        "        torch.cuda.synchronize() if device == 'cuda' else None\n",
        "        t1 = time.time()\n",
        "        dt = (t1 - t0) * 1000\n",
        "        tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "\n",
        "        print(f'Step {step + 1}/{max_steps} | Loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}')\n",
        "\n",
        "print(\"Training completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIE3grrzzIYG",
        "outputId": "1169441e-02bf-4542-e165-145060ab8bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens\n",
            "1 epoch = 330 batches\n",
            "Epoch 1/20\n",
            "Step 1/1000 | Loss: 10.8735 | dt: 113.56ms | tok/sec: 9017.13\n",
            "Step 2/1000 | Loss: 10.5486 | dt: 87.81ms | tok/sec: 11661.85\n",
            "Step 3/1000 | Loss: 10.3371 | dt: 62.83ms | tok/sec: 16298.14\n",
            "Step 4/1000 | Loss: 10.0952 | dt: 62.22ms | tok/sec: 16457.52\n",
            "Step 5/1000 | Loss: 10.1463 | dt: 61.43ms | tok/sec: 16668.87\n",
            "Step 6/1000 | Loss: 9.9253 | dt: 67.80ms | tok/sec: 15104.03\n",
            "Step 7/1000 | Loss: 9.8955 | dt: 70.71ms | tok/sec: 14482.48\n",
            "Step 8/1000 | Loss: 9.7390 | dt: 61.32ms | tok/sec: 16698.16\n",
            "Step 9/1000 | Loss: 9.6423 | dt: 58.76ms | tok/sec: 17427.13\n",
            "Step 10/1000 | Loss: 9.4429 | dt: 64.62ms | tok/sec: 15846.54\n",
            "Step 11/1000 | Loss: 9.1802 | dt: 57.99ms | tok/sec: 17657.69\n",
            "Step 12/1000 | Loss: 9.2253 | dt: 59.65ms | tok/sec: 17166.62\n",
            "Step 13/1000 | Loss: 8.8134 | dt: 63.11ms | tok/sec: 16224.87\n",
            "Step 14/1000 | Loss: 8.7492 | dt: 59.37ms | tok/sec: 17249.21\n",
            "Step 15/1000 | Loss: 8.5228 | dt: 61.83ms | tok/sec: 16562.61\n",
            "Step 16/1000 | Loss: 8.3697 | dt: 59.41ms | tok/sec: 17236.68\n",
            "Step 17/1000 | Loss: 8.1023 | dt: 58.09ms | tok/sec: 17626.67\n",
            "Step 18/1000 | Loss: 7.9038 | dt: 63.25ms | tok/sec: 16189.16\n",
            "Step 19/1000 | Loss: 7.6835 | dt: 62.85ms | tok/sec: 16291.90\n",
            "Step 20/1000 | Loss: 7.5807 | dt: 70.35ms | tok/sec: 14554.97\n",
            "Step 21/1000 | Loss: 7.1474 | dt: 64.72ms | tok/sec: 15823.01\n",
            "Step 22/1000 | Loss: 7.4208 | dt: 71.26ms | tok/sec: 14370.06\n",
            "Step 23/1000 | Loss: 6.8920 | dt: 57.73ms | tok/sec: 17737.46\n",
            "Step 24/1000 | Loss: 6.6354 | dt: 58.82ms | tok/sec: 17409.68\n",
            "Step 25/1000 | Loss: 6.7939 | dt: 61.38ms | tok/sec: 16683.31\n",
            "Step 26/1000 | Loss: 6.6443 | dt: 57.37ms | tok/sec: 17847.66\n",
            "Step 27/1000 | Loss: 6.5865 | dt: 58.48ms | tok/sec: 17510.61\n",
            "Step 28/1000 | Loss: 6.3481 | dt: 60.35ms | tok/sec: 16968.98\n",
            "Step 29/1000 | Loss: 6.4128 | dt: 65.29ms | tok/sec: 15683.36\n",
            "Step 30/1000 | Loss: 6.3157 | dt: 70.51ms | tok/sec: 14523.32\n",
            "Step 31/1000 | Loss: 6.2527 | dt: 92.82ms | tok/sec: 11032.60\n",
            "Step 32/1000 | Loss: 6.1843 | dt: 90.65ms | tok/sec: 11295.83\n",
            "Step 33/1000 | Loss: 6.4653 | dt: 58.91ms | tok/sec: 17381.14\n",
            "Step 34/1000 | Loss: 6.4223 | dt: 61.53ms | tok/sec: 16641.81\n",
            "Step 35/1000 | Loss: 6.2521 | dt: 57.64ms | tok/sec: 17764.68\n",
            "Step 36/1000 | Loss: 5.8343 | dt: 58.28ms | tok/sec: 17569.20\n",
            "Step 37/1000 | Loss: 5.8187 | dt: 57.73ms | tok/sec: 17737.17\n",
            "Step 38/1000 | Loss: 6.2091 | dt: 55.47ms | tok/sec: 18459.16\n",
            "Step 39/1000 | Loss: 5.6629 | dt: 56.05ms | tok/sec: 18268.29\n",
            "Step 40/1000 | Loss: 6.2727 | dt: 57.07ms | tok/sec: 17944.22\n",
            "Step 41/1000 | Loss: 6.2408 | dt: 57.86ms | tok/sec: 17699.16\n",
            "Step 42/1000 | Loss: 6.3188 | dt: 58.02ms | tok/sec: 17649.20\n",
            "Step 43/1000 | Loss: 6.1163 | dt: 58.05ms | tok/sec: 17640.86\n",
            "Step 44/1000 | Loss: 6.1144 | dt: 57.76ms | tok/sec: 17729.70\n",
            "Step 45/1000 | Loss: 6.1893 | dt: 56.34ms | tok/sec: 18173.76\n",
            "Step 46/1000 | Loss: 6.0560 | dt: 55.33ms | tok/sec: 18506.33\n",
            "Step 47/1000 | Loss: 6.9749 | dt: 55.45ms | tok/sec: 18468.61\n",
            "Step 48/1000 | Loss: 6.7312 | dt: 57.64ms | tok/sec: 17766.15\n",
            "Step 49/1000 | Loss: 6.9493 | dt: 59.14ms | tok/sec: 17313.53\n",
            "Step 50/1000 | Loss: 6.7549 | dt: 60.75ms | tok/sec: 16856.82\n",
            "Step 51/1000 | Loss: 6.5942 | dt: 57.73ms | tok/sec: 17736.80\n",
            "Step 52/1000 | Loss: 6.6264 | dt: 59.59ms | tok/sec: 17184.13\n",
            "Step 53/1000 | Loss: 6.7929 | dt: 59.58ms | tok/sec: 17186.88\n",
            "Step 54/1000 | Loss: 6.5778 | dt: 61.51ms | tok/sec: 16647.48\n",
            "Step 55/1000 | Loss: 6.7055 | dt: 58.61ms | tok/sec: 17471.57\n",
            "Step 56/1000 | Loss: 6.5912 | dt: 59.37ms | tok/sec: 17247.06\n",
            "Step 57/1000 | Loss: 6.3368 | dt: 59.68ms | tok/sec: 17159.48\n",
            "Step 58/1000 | Loss: 6.5735 | dt: 59.62ms | tok/sec: 17176.71\n",
            "Step 59/1000 | Loss: 6.1388 | dt: 59.21ms | tok/sec: 17292.97\n",
            "Step 60/1000 | Loss: 6.1297 | dt: 59.42ms | tok/sec: 17234.54\n",
            "Step 61/1000 | Loss: 6.1843 | dt: 59.31ms | tok/sec: 17264.74\n",
            "Step 62/1000 | Loss: 6.3376 | dt: 58.52ms | tok/sec: 17498.62\n",
            "Step 63/1000 | Loss: 6.2202 | dt: 57.70ms | tok/sec: 17745.89\n",
            "Step 64/1000 | Loss: 6.3889 | dt: 55.99ms | tok/sec: 18290.31\n",
            "Step 65/1000 | Loss: 6.0119 | dt: 55.92ms | tok/sec: 18312.77\n",
            "Step 66/1000 | Loss: 6.4199 | dt: 58.18ms | tok/sec: 17600.16\n",
            "Step 67/1000 | Loss: 6.2566 | dt: 58.66ms | tok/sec: 17455.24\n",
            "Step 68/1000 | Loss: 5.8649 | dt: 57.46ms | tok/sec: 17822.55\n",
            "Step 69/1000 | Loss: 5.9707 | dt: 57.32ms | tok/sec: 17863.54\n",
            "Step 70/1000 | Loss: 5.9778 | dt: 55.89ms | tok/sec: 18320.82\n",
            "Step 71/1000 | Loss: 5.8361 | dt: 57.04ms | tok/sec: 17951.50\n",
            "Step 72/1000 | Loss: 5.8885 | dt: 58.07ms | tok/sec: 17633.18\n",
            "Step 73/1000 | Loss: 6.0379 | dt: 58.68ms | tok/sec: 17449.50\n",
            "Step 74/1000 | Loss: 6.2275 | dt: 58.48ms | tok/sec: 17510.97\n",
            "Step 75/1000 | Loss: 6.0107 | dt: 58.04ms | tok/sec: 17642.82\n",
            "Step 76/1000 | Loss: 6.0789 | dt: 54.42ms | tok/sec: 18817.44\n",
            "Step 77/1000 | Loss: 5.9196 | dt: 57.79ms | tok/sec: 17717.93\n",
            "Step 78/1000 | Loss: 6.0107 | dt: 58.13ms | tok/sec: 17616.62\n",
            "Step 79/1000 | Loss: 6.2487 | dt: 58.54ms | tok/sec: 17491.28\n",
            "Step 80/1000 | Loss: 6.1642 | dt: 58.46ms | tok/sec: 17515.18\n",
            "Step 81/1000 | Loss: 6.6049 | dt: 58.47ms | tok/sec: 17513.54\n",
            "Step 82/1000 | Loss: 6.2913 | dt: 61.46ms | tok/sec: 16661.56\n",
            "Step 83/1000 | Loss: 6.2629 | dt: 57.94ms | tok/sec: 17674.04\n",
            "Step 84/1000 | Loss: 6.2886 | dt: 57.10ms | tok/sec: 17932.23\n",
            "Step 85/1000 | Loss: 6.0734 | dt: 56.49ms | tok/sec: 18127.27\n",
            "Step 86/1000 | Loss: 6.2133 | dt: 57.13ms | tok/sec: 17924.60\n",
            "Step 87/1000 | Loss: 6.0981 | dt: 58.17ms | tok/sec: 17604.85\n",
            "Step 88/1000 | Loss: 5.9731 | dt: 58.84ms | tok/sec: 17404.46\n",
            "Step 89/1000 | Loss: 6.2515 | dt: 57.90ms | tok/sec: 17684.44\n",
            "Step 90/1000 | Loss: 6.2613 | dt: 58.88ms | tok/sec: 17391.49\n",
            "Step 91/1000 | Loss: 6.1118 | dt: 57.77ms | tok/sec: 17724.65\n",
            "Step 92/1000 | Loss: 6.1167 | dt: 58.45ms | tok/sec: 17519.97\n",
            "Step 93/1000 | Loss: 6.1115 | dt: 56.37ms | tok/sec: 18164.76\n",
            "Step 94/1000 | Loss: 6.2906 | dt: 57.06ms | tok/sec: 17945.12\n",
            "Step 95/1000 | Loss: 6.3743 | dt: 57.38ms | tok/sec: 17845.58\n",
            "Step 96/1000 | Loss: 6.3947 | dt: 57.40ms | tok/sec: 17838.47\n",
            "Step 97/1000 | Loss: 6.3161 | dt: 57.20ms | tok/sec: 17903.01\n",
            "Step 98/1000 | Loss: 6.2461 | dt: 56.33ms | tok/sec: 18179.45\n",
            "Step 99/1000 | Loss: 6.4374 | dt: 57.44ms | tok/sec: 17826.91\n",
            "Step 100/1000 | Loss: 6.2307 | dt: 59.09ms | tok/sec: 17330.30\n",
            "Step 101/1000 | Loss: 6.2457 | dt: 57.94ms | tok/sec: 17672.07\n",
            "Step 102/1000 | Loss: 6.2953 | dt: 57.60ms | tok/sec: 17777.99\n",
            "Step 103/1000 | Loss: 6.3383 | dt: 57.13ms | tok/sec: 17925.05\n",
            "Step 104/1000 | Loss: 6.0789 | dt: 56.81ms | tok/sec: 18025.03\n",
            "Step 105/1000 | Loss: 6.1341 | dt: 57.14ms | tok/sec: 17921.46\n",
            "Step 106/1000 | Loss: 6.3338 | dt: 57.04ms | tok/sec: 17953.82\n",
            "Step 107/1000 | Loss: 5.8697 | dt: 57.91ms | tok/sec: 17682.69\n",
            "Step 108/1000 | Loss: 6.0180 | dt: 58.03ms | tok/sec: 17644.99\n",
            "Step 109/1000 | Loss: 5.9267 | dt: 59.23ms | tok/sec: 17288.86\n",
            "Step 110/1000 | Loss: 6.0047 | dt: 58.82ms | tok/sec: 17410.17\n",
            "Step 111/1000 | Loss: 6.0954 | dt: 57.96ms | tok/sec: 17668.07\n",
            "Step 112/1000 | Loss: 6.2695 | dt: 56.07ms | tok/sec: 18261.30\n",
            "Step 113/1000 | Loss: 6.2532 | dt: 57.15ms | tok/sec: 17918.77\n",
            "Step 114/1000 | Loss: 5.9523 | dt: 58.26ms | tok/sec: 17575.89\n",
            "Step 115/1000 | Loss: 6.0560 | dt: 58.81ms | tok/sec: 17412.15\n",
            "Step 116/1000 | Loss: 5.7881 | dt: 58.79ms | tok/sec: 17419.14\n",
            "Step 117/1000 | Loss: 6.0740 | dt: 59.43ms | tok/sec: 17229.49\n",
            "Step 118/1000 | Loss: 5.7458 | dt: 58.51ms | tok/sec: 17500.12\n",
            "Step 119/1000 | Loss: 5.9732 | dt: 56.98ms | tok/sec: 17972.53\n",
            "Step 120/1000 | Loss: 5.7483 | dt: 55.52ms | tok/sec: 18443.55\n",
            "Step 121/1000 | Loss: 5.8490 | dt: 57.33ms | tok/sec: 17860.65\n",
            "Step 122/1000 | Loss: 5.6045 | dt: 58.63ms | tok/sec: 17465.82\n",
            "Step 123/1000 | Loss: 5.7854 | dt: 58.74ms | tok/sec: 17431.93\n",
            "Step 124/1000 | Loss: 5.5558 | dt: 58.83ms | tok/sec: 17405.23\n",
            "Step 125/1000 | Loss: 5.2133 | dt: 58.65ms | tok/sec: 17458.22\n",
            "Step 126/1000 | Loss: 5.4871 | dt: 58.96ms | tok/sec: 17367.65\n",
            "Step 127/1000 | Loss: 5.5079 | dt: 57.76ms | tok/sec: 17728.97\n",
            "Step 128/1000 | Loss: 6.0412 | dt: 55.46ms | tok/sec: 18463.13\n",
            "Step 129/1000 | Loss: 5.7777 | dt: 56.72ms | tok/sec: 18052.30\n",
            "Step 130/1000 | Loss: 6.0101 | dt: 58.17ms | tok/sec: 17604.71\n",
            "Step 131/1000 | Loss: 6.2199 | dt: 58.55ms | tok/sec: 17489.93\n",
            "Step 132/1000 | Loss: 5.8933 | dt: 59.24ms | tok/sec: 17284.75\n",
            "Step 133/1000 | Loss: 6.2552 | dt: 59.06ms | tok/sec: 17338.76\n",
            "Step 134/1000 | Loss: 6.1635 | dt: 58.62ms | tok/sec: 17468.31\n",
            "Step 135/1000 | Loss: 6.0695 | dt: 59.29ms | tok/sec: 17271.48\n",
            "Step 136/1000 | Loss: 6.0724 | dt: 56.00ms | tok/sec: 18285.09\n",
            "Step 137/1000 | Loss: 6.5963 | dt: 58.37ms | tok/sec: 17544.44\n",
            "Step 138/1000 | Loss: 5.9096 | dt: 57.32ms | tok/sec: 17863.99\n",
            "Step 139/1000 | Loss: 5.7586 | dt: 58.67ms | tok/sec: 17454.11\n",
            "Step 140/1000 | Loss: 6.0937 | dt: 58.80ms | tok/sec: 17416.10\n",
            "Step 141/1000 | Loss: 5.8449 | dt: 57.56ms | tok/sec: 17790.22\n",
            "Step 142/1000 | Loss: 5.6878 | dt: 58.77ms | tok/sec: 17423.10\n",
            "Step 143/1000 | Loss: 6.3562 | dt: 57.05ms | tok/sec: 17950.60\n",
            "Step 144/1000 | Loss: 6.3107 | dt: 56.14ms | tok/sec: 18239.90\n",
            "Step 145/1000 | Loss: 5.8287 | dt: 57.74ms | tok/sec: 17733.95\n",
            "Step 146/1000 | Loss: 5.6968 | dt: 57.35ms | tok/sec: 17855.97\n",
            "Step 147/1000 | Loss: 5.7252 | dt: 59.06ms | tok/sec: 17337.78\n",
            "Step 148/1000 | Loss: 5.9070 | dt: 59.55ms | tok/sec: 17195.90\n",
            "Step 149/1000 | Loss: 5.9724 | dt: 59.15ms | tok/sec: 17313.18\n",
            "Step 150/1000 | Loss: 5.8109 | dt: 58.62ms | tok/sec: 17467.52\n",
            "Step 151/1000 | Loss: 6.1874 | dt: 56.75ms | tok/sec: 18043.50\n",
            "Step 152/1000 | Loss: 5.8744 | dt: 56.26ms | tok/sec: 18199.63\n",
            "Step 153/1000 | Loss: 5.9327 | dt: 56.90ms | tok/sec: 17997.76\n",
            "Step 154/1000 | Loss: 5.7660 | dt: 58.46ms | tok/sec: 17516.39\n",
            "Step 155/1000 | Loss: 5.6558 | dt: 58.72ms | tok/sec: 17437.31\n",
            "Step 156/1000 | Loss: 5.5906 | dt: 57.30ms | tok/sec: 17871.27\n",
            "Step 157/1000 | Loss: 5.4618 | dt: 59.10ms | tok/sec: 17327.22\n",
            "Step 158/1000 | Loss: 5.6143 | dt: 57.81ms | tok/sec: 17713.18\n",
            "Step 159/1000 | Loss: 5.3943 | dt: 54.74ms | tok/sec: 18705.16\n",
            "Step 160/1000 | Loss: 5.7401 | dt: 57.98ms | tok/sec: 17660.52\n",
            "Step 161/1000 | Loss: 5.6552 | dt: 58.78ms | tok/sec: 17420.76\n",
            "Step 162/1000 | Loss: 5.5780 | dt: 58.66ms | tok/sec: 17457.23\n",
            "Step 163/1000 | Loss: 5.4955 | dt: 59.21ms | tok/sec: 17294.50\n",
            "Step 164/1000 | Loss: 6.0096 | dt: 59.56ms | tok/sec: 17194.17\n",
            "Step 165/1000 | Loss: 6.0097 | dt: 59.13ms | tok/sec: 17316.46\n",
            "Step 166/1000 | Loss: 5.8695 | dt: 58.58ms | tok/sec: 17481.32\n",
            "Step 167/1000 | Loss: 5.7147 | dt: 57.94ms | tok/sec: 17673.31\n",
            "Step 168/1000 | Loss: 5.5273 | dt: 56.13ms | tok/sec: 18242.38\n",
            "Step 169/1000 | Loss: 5.7507 | dt: 57.71ms | tok/sec: 17745.31\n",
            "Step 170/1000 | Loss: 5.6551 | dt: 58.65ms | tok/sec: 17460.71\n",
            "Step 171/1000 | Loss: 5.7136 | dt: 58.65ms | tok/sec: 17460.42\n",
            "Step 172/1000 | Loss: 5.6069 | dt: 58.98ms | tok/sec: 17361.40\n",
            "Step 173/1000 | Loss: 5.6038 | dt: 59.71ms | tok/sec: 17149.96\n",
            "Step 174/1000 | Loss: 5.4085 | dt: 59.00ms | tok/sec: 17356.21\n",
            "Step 175/1000 | Loss: 5.4649 | dt: 58.05ms | tok/sec: 17640.79\n",
            "Step 176/1000 | Loss: 5.9700 | dt: 58.14ms | tok/sec: 17612.07\n",
            "Step 177/1000 | Loss: 5.8226 | dt: 54.79ms | tok/sec: 18689.70\n",
            "Step 178/1000 | Loss: 5.7762 | dt: 57.76ms | tok/sec: 17728.60\n",
            "Step 179/1000 | Loss: 5.9178 | dt: 58.75ms | tok/sec: 17430.02\n",
            "Step 180/1000 | Loss: 5.8207 | dt: 58.82ms | tok/sec: 17409.68\n",
            "Step 181/1000 | Loss: 5.6672 | dt: 59.04ms | tok/sec: 17343.03\n",
            "Step 182/1000 | Loss: 5.3302 | dt: 59.39ms | tok/sec: 17243.32\n",
            "Step 183/1000 | Loss: 5.7093 | dt: 58.61ms | tok/sec: 17470.44\n",
            "Step 184/1000 | Loss: 5.8511 | dt: 57.05ms | tok/sec: 17949.10\n",
            "Step 185/1000 | Loss: 5.5249 | dt: 56.50ms | tok/sec: 18122.30\n",
            "Step 186/1000 | Loss: 5.7396 | dt: 56.89ms | tok/sec: 17999.12\n",
            "Step 187/1000 | Loss: 5.6161 | dt: 58.38ms | tok/sec: 17539.86\n",
            "Step 188/1000 | Loss: 5.1298 | dt: 58.65ms | tok/sec: 17460.07\n",
            "Step 189/1000 | Loss: 4.9949 | dt: 59.12ms | tok/sec: 17319.95\n",
            "Step 190/1000 | Loss: 5.3806 | dt: 59.29ms | tok/sec: 17271.13\n",
            "Step 191/1000 | Loss: 5.8327 | dt: 59.47ms | tok/sec: 17219.82\n",
            "Step 192/1000 | Loss: 5.8933 | dt: 58.32ms | tok/sec: 17558.79\n",
            "Step 193/1000 | Loss: 5.6528 | dt: 58.14ms | tok/sec: 17613.95\n",
            "Step 194/1000 | Loss: 5.4083 | dt: 55.99ms | tok/sec: 18289.92\n",
            "Step 195/1000 | Loss: 5.4646 | dt: 56.73ms | tok/sec: 18049.27\n",
            "Step 196/1000 | Loss: 5.5612 | dt: 58.38ms | tok/sec: 17541.72\n",
            "Step 197/1000 | Loss: 5.3113 | dt: 58.29ms | tok/sec: 17567.05\n",
            "Step 198/1000 | Loss: 5.4510 | dt: 58.93ms | tok/sec: 17376.15\n",
            "Step 199/1000 | Loss: 5.4139 | dt: 59.56ms | tok/sec: 17191.97\n",
            "Step 200/1000 | Loss: 5.2687 | dt: 58.68ms | tok/sec: 17449.78\n",
            "Step 201/1000 | Loss: 5.6392 | dt: 58.43ms | tok/sec: 17526.33\n",
            "Step 202/1000 | Loss: 5.2451 | dt: 57.72ms | tok/sec: 17742.08\n",
            "Step 203/1000 | Loss: 5.4071 | dt: 56.56ms | tok/sec: 18104.50\n",
            "Step 204/1000 | Loss: 5.7106 | dt: 56.83ms | tok/sec: 18018.75\n",
            "Step 205/1000 | Loss: 5.3115 | dt: 58.62ms | tok/sec: 17468.02\n",
            "Step 206/1000 | Loss: 4.8439 | dt: 59.13ms | tok/sec: 17318.00\n",
            "Step 207/1000 | Loss: 5.4449 | dt: 58.99ms | tok/sec: 17357.96\n",
            "Step 208/1000 | Loss: 5.7807 | dt: 59.44ms | tok/sec: 17226.10\n",
            "Step 209/1000 | Loss: 6.2725 | dt: 59.06ms | tok/sec: 17337.22\n",
            "Step 210/1000 | Loss: 6.0456 | dt: 56.13ms | tok/sec: 18242.30\n",
            "Step 211/1000 | Loss: 6.0309 | dt: 56.26ms | tok/sec: 18202.48\n",
            "Step 212/1000 | Loss: 6.1933 | dt: 57.45ms | tok/sec: 17823.66\n",
            "Step 213/1000 | Loss: 6.2690 | dt: 58.72ms | tok/sec: 17437.24\n",
            "Step 214/1000 | Loss: 5.9290 | dt: 58.78ms | tok/sec: 17421.89\n",
            "Step 215/1000 | Loss: 5.7532 | dt: 59.38ms | tok/sec: 17245.75\n",
            "Step 216/1000 | Loss: 5.9386 | dt: 59.35ms | tok/sec: 17254.34\n",
            "Step 217/1000 | Loss: 5.7815 | dt: 59.18ms | tok/sec: 17303.07\n",
            "Step 218/1000 | Loss: 5.8615 | dt: 58.31ms | tok/sec: 17562.81\n",
            "Step 219/1000 | Loss: 5.6150 | dt: 57.82ms | tok/sec: 17710.04\n",
            "Step 220/1000 | Loss: 5.6892 | dt: 56.28ms | tok/sec: 18195.62\n",
            "Step 221/1000 | Loss: 5.6888 | dt: 57.71ms | tok/sec: 17743.91\n",
            "Step 222/1000 | Loss: 5.9132 | dt: 58.52ms | tok/sec: 17499.48\n",
            "Step 223/1000 | Loss: 5.6020 | dt: 58.82ms | tok/sec: 17409.39\n",
            "Step 224/1000 | Loss: 5.9904 | dt: 59.00ms | tok/sec: 17355.02\n",
            "Step 225/1000 | Loss: 5.9310 | dt: 59.33ms | tok/sec: 17258.36\n",
            "Step 226/1000 | Loss: 6.1996 | dt: 59.41ms | tok/sec: 17237.10\n",
            "Step 227/1000 | Loss: 6.2256 | dt: 58.92ms | tok/sec: 17380.51\n",
            "Step 228/1000 | Loss: 6.3081 | dt: 58.31ms | tok/sec: 17560.01\n",
            "Step 229/1000 | Loss: 5.9648 | dt: 57.34ms | tok/sec: 17857.30\n",
            "Step 230/1000 | Loss: 5.9865 | dt: 57.10ms | tok/sec: 17933.28\n",
            "Step 231/1000 | Loss: 5.9176 | dt: 57.90ms | tok/sec: 17686.84\n",
            "Step 232/1000 | Loss: 6.3134 | dt: 58.48ms | tok/sec: 17511.47\n",
            "Step 233/1000 | Loss: 5.8225 | dt: 59.11ms | tok/sec: 17323.17\n",
            "Step 234/1000 | Loss: 5.6494 | dt: 59.52ms | tok/sec: 17204.37\n",
            "Step 235/1000 | Loss: 5.6041 | dt: 59.30ms | tok/sec: 17268.01\n",
            "Step 236/1000 | Loss: 5.6553 | dt: 59.00ms | tok/sec: 17355.30\n",
            "Step 237/1000 | Loss: 5.9993 | dt: 60.96ms | tok/sec: 16796.77\n",
            "Step 238/1000 | Loss: 5.7109 | dt: 58.70ms | tok/sec: 17445.67\n",
            "Step 239/1000 | Loss: 5.5620 | dt: 60.24ms | tok/sec: 16997.99\n",
            "Step 240/1000 | Loss: 5.8350 | dt: 60.99ms | tok/sec: 16789.09\n",
            "Step 241/1000 | Loss: 5.4972 | dt: 59.41ms | tok/sec: 17234.95\n",
            "Step 242/1000 | Loss: 5.6022 | dt: 60.59ms | tok/sec: 16901.67\n",
            "Step 243/1000 | Loss: 5.5530 | dt: 59.48ms | tok/sec: 17215.47\n",
            "Step 244/1000 | Loss: 6.2615 | dt: 59.06ms | tok/sec: 17337.50\n",
            "Step 245/1000 | Loss: 5.8830 | dt: 59.07ms | tok/sec: 17334.77\n",
            "Step 246/1000 | Loss: 5.3794 | dt: 59.34ms | tok/sec: 17256.77\n",
            "Step 247/1000 | Loss: 5.4233 | dt: 58.17ms | tok/sec: 17603.34\n",
            "Step 248/1000 | Loss: 5.9039 | dt: 54.73ms | tok/sec: 18710.62\n",
            "Step 249/1000 | Loss: 5.6762 | dt: 56.77ms | tok/sec: 18037.59\n",
            "Step 250/1000 | Loss: 5.8992 | dt: 59.68ms | tok/sec: 17156.81\n",
            "Step 251/1000 | Loss: 6.0359 | dt: 58.62ms | tok/sec: 17468.95\n",
            "Step 252/1000 | Loss: 5.9017 | dt: 58.41ms | tok/sec: 17531.98\n",
            "Step 253/1000 | Loss: 5.8863 | dt: 60.78ms | tok/sec: 16848.03\n",
            "Step 254/1000 | Loss: 5.6226 | dt: 60.64ms | tok/sec: 16885.99\n",
            "Step 255/1000 | Loss: 5.4022 | dt: 60.67ms | tok/sec: 16877.62\n",
            "Step 256/1000 | Loss: 5.7544 | dt: 59.53ms | tok/sec: 17201.20\n",
            "Step 257/1000 | Loss: 5.3600 | dt: 59.26ms | tok/sec: 17280.37\n",
            "Step 258/1000 | Loss: 5.1904 | dt: 59.22ms | tok/sec: 17292.20\n",
            "Step 259/1000 | Loss: 5.7590 | dt: 60.26ms | tok/sec: 16992.21\n",
            "Step 260/1000 | Loss: 5.5887 | dt: 59.83ms | tok/sec: 17115.58\n",
            "Step 261/1000 | Loss: 5.3713 | dt: 60.09ms | tok/sec: 17040.28\n",
            "Step 262/1000 | Loss: 5.5175 | dt: 60.64ms | tok/sec: 16885.45\n",
            "Step 263/1000 | Loss: 5.7388 | dt: 60.80ms | tok/sec: 16842.68\n",
            "Step 264/1000 | Loss: 5.8003 | dt: 60.19ms | tok/sec: 17011.73\n",
            "Step 265/1000 | Loss: 5.9728 | dt: 59.88ms | tok/sec: 17101.48\n",
            "Step 266/1000 | Loss: 6.0802 | dt: 60.38ms | tok/sec: 16959.33\n",
            "Step 267/1000 | Loss: 5.6446 | dt: 60.01ms | tok/sec: 17063.36\n",
            "Step 268/1000 | Loss: 5.7163 | dt: 59.51ms | tok/sec: 17208.44\n",
            "Step 269/1000 | Loss: 5.8396 | dt: 60.58ms | tok/sec: 16903.06\n",
            "Step 270/1000 | Loss: 5.8898 | dt: 59.47ms | tok/sec: 17218.71\n",
            "Step 271/1000 | Loss: 5.5067 | dt: 59.30ms | tok/sec: 17269.19\n",
            "Step 272/1000 | Loss: 5.4853 | dt: 58.74ms | tok/sec: 17433.63\n",
            "Step 273/1000 | Loss: 5.6262 | dt: 58.27ms | tok/sec: 17574.24\n",
            "Step 274/1000 | Loss: 5.7783 | dt: 56.14ms | tok/sec: 18240.44\n",
            "Step 275/1000 | Loss: 5.3589 | dt: 57.36ms | tok/sec: 17853.15\n",
            "Step 276/1000 | Loss: 5.5631 | dt: 58.78ms | tok/sec: 17419.70\n",
            "Step 277/1000 | Loss: 5.3891 | dt: 59.23ms | tok/sec: 17289.97\n",
            "Step 278/1000 | Loss: 5.2480 | dt: 58.88ms | tok/sec: 17391.14\n",
            "Step 279/1000 | Loss: 5.2859 | dt: 59.72ms | tok/sec: 17146.81\n",
            "Step 280/1000 | Loss: 5.4024 | dt: 59.52ms | tok/sec: 17203.89\n",
            "Step 281/1000 | Loss: 4.9693 | dt: 59.93ms | tok/sec: 17086.17\n",
            "Step 282/1000 | Loss: 5.3978 | dt: 58.90ms | tok/sec: 17385.72\n",
            "Step 283/1000 | Loss: 4.9379 | dt: 57.96ms | tok/sec: 17668.51\n",
            "Step 284/1000 | Loss: 5.5393 | dt: 56.74ms | tok/sec: 18048.58\n",
            "Step 285/1000 | Loss: 5.6266 | dt: 57.36ms | tok/sec: 17853.37\n",
            "Step 286/1000 | Loss: 6.2490 | dt: 58.26ms | tok/sec: 17575.82\n",
            "Step 287/1000 | Loss: 5.3932 | dt: 59.12ms | tok/sec: 17320.16\n",
            "Step 288/1000 | Loss: 6.0360 | dt: 58.75ms | tok/sec: 17429.67\n",
            "Step 289/1000 | Loss: 5.9669 | dt: 59.64ms | tok/sec: 17170.94\n",
            "Step 290/1000 | Loss: 5.6710 | dt: 58.73ms | tok/sec: 17435.83\n",
            "Step 291/1000 | Loss: 5.6122 | dt: 59.54ms | tok/sec: 17197.96\n",
            "Step 292/1000 | Loss: 5.9236 | dt: 60.79ms | tok/sec: 16845.92\n",
            "Step 293/1000 | Loss: 5.7744 | dt: 59.11ms | tok/sec: 17324.91\n",
            "Step 294/1000 | Loss: 5.2658 | dt: 58.38ms | tok/sec: 17541.00\n",
            "Step 295/1000 | Loss: 5.5515 | dt: 56.46ms | tok/sec: 18137.99\n",
            "Step 296/1000 | Loss: 5.4756 | dt: 57.12ms | tok/sec: 17926.55\n",
            "Step 297/1000 | Loss: 5.6043 | dt: 58.83ms | tok/sec: 17405.02\n",
            "Step 298/1000 | Loss: 5.0297 | dt: 59.13ms | tok/sec: 17317.51\n",
            "Step 299/1000 | Loss: 5.4163 | dt: 57.36ms | tok/sec: 17853.52\n",
            "Step 300/1000 | Loss: 5.7305 | dt: 59.61ms | tok/sec: 17178.36\n",
            "Step 301/1000 | Loss: 5.9384 | dt: 60.05ms | tok/sec: 17053.40\n",
            "Step 302/1000 | Loss: 5.5851 | dt: 57.91ms | tok/sec: 17682.84\n",
            "Step 303/1000 | Loss: 5.5939 | dt: 57.33ms | tok/sec: 17860.57\n",
            "Step 304/1000 | Loss: 5.4731 | dt: 57.12ms | tok/sec: 17928.72\n",
            "Step 305/1000 | Loss: 5.1798 | dt: 58.59ms | tok/sec: 17477.90\n",
            "Step 306/1000 | Loss: 5.7008 | dt: 56.56ms | tok/sec: 18103.59\n",
            "Step 307/1000 | Loss: 5.4864 | dt: 58.22ms | tok/sec: 17588.27\n",
            "Step 308/1000 | Loss: 5.3670 | dt: 58.55ms | tok/sec: 17490.50\n",
            "Step 309/1000 | Loss: 5.2090 | dt: 58.68ms | tok/sec: 17451.20\n",
            "Step 310/1000 | Loss: 5.1220 | dt: 58.01ms | tok/sec: 17652.24\n",
            "Step 311/1000 | Loss: 5.1333 | dt: 56.52ms | tok/sec: 18116.11\n",
            "Step 312/1000 | Loss: 5.0374 | dt: 57.30ms | tok/sec: 17872.24\n",
            "Step 313/1000 | Loss: 5.1971 | dt: 59.15ms | tok/sec: 17310.95\n",
            "Step 314/1000 | Loss: 5.1000 | dt: 59.39ms | tok/sec: 17240.56\n",
            "Step 315/1000 | Loss: 5.0572 | dt: 59.70ms | tok/sec: 17153.52\n",
            "Step 316/1000 | Loss: 5.1325 | dt: 58.10ms | tok/sec: 17625.30\n",
            "Step 317/1000 | Loss: 4.9126 | dt: 58.95ms | tok/sec: 17369.61\n",
            "Step 318/1000 | Loss: 4.6399 | dt: 59.39ms | tok/sec: 17242.08\n",
            "Step 319/1000 | Loss: 5.2055 | dt: 58.58ms | tok/sec: 17479.18\n",
            "Step 320/1000 | Loss: 5.6017 | dt: 57.46ms | tok/sec: 17821.88\n",
            "Step 321/1000 | Loss: 5.8101 | dt: 56.65ms | tok/sec: 18074.56\n",
            "Step 322/1000 | Loss: 6.0524 | dt: 57.47ms | tok/sec: 17816.86\n",
            "Step 323/1000 | Loss: 6.0885 | dt: 58.50ms | tok/sec: 17503.05\n",
            "Step 324/1000 | Loss: 5.7524 | dt: 57.26ms | tok/sec: 17884.82\n",
            "Step 325/1000 | Loss: 5.9612 | dt: 60.24ms | tok/sec: 16999.94\n",
            "Step 326/1000 | Loss: 5.8877 | dt: 59.42ms | tok/sec: 17233.91\n",
            "Step 327/1000 | Loss: 5.4823 | dt: 64.33ms | tok/sec: 15918.90\n",
            "Step 328/1000 | Loss: 5.5502 | dt: 59.66ms | tok/sec: 17164.28\n",
            "Step 329/1000 | Loss: 5.6270 | dt: 59.94ms | tok/sec: 17083.04\n",
            "Step 330/1000 | Loss: 5.5594 | dt: 59.59ms | tok/sec: 17183.10\n",
            "Step 331/1000 | Loss: 5.8983 | dt: 59.21ms | tok/sec: 17294.08\n",
            "Step 332/1000 | Loss: 5.5538 | dt: 58.77ms | tok/sec: 17425.15\n",
            "Step 333/1000 | Loss: 5.8884 | dt: 57.21ms | tok/sec: 17900.17\n",
            "Step 334/1000 | Loss: 5.7451 | dt: 57.23ms | tok/sec: 17894.13\n",
            "Step 335/1000 | Loss: 5.8617 | dt: 57.90ms | tok/sec: 17685.75\n",
            "Step 336/1000 | Loss: 5.4218 | dt: 58.58ms | tok/sec: 17480.04\n",
            "Step 337/1000 | Loss: 5.6405 | dt: 58.19ms | tok/sec: 17596.48\n",
            "Step 338/1000 | Loss: 5.5352 | dt: 58.51ms | tok/sec: 17500.34\n",
            "Step 339/1000 | Loss: 5.6203 | dt: 59.78ms | tok/sec: 17129.85\n",
            "Step 340/1000 | Loss: 5.3888 | dt: 59.23ms | tok/sec: 17288.02\n",
            "Step 341/1000 | Loss: 5.3011 | dt: 59.43ms | tok/sec: 17231.29\n",
            "Step 342/1000 | Loss: 5.9516 | dt: 56.60ms | tok/sec: 18093.14\n",
            "Step 343/1000 | Loss: 5.3241 | dt: 56.93ms | tok/sec: 17986.53\n",
            "Step 344/1000 | Loss: 5.5657 | dt: 57.51ms | tok/sec: 17805.11\n",
            "Step 345/1000 | Loss: 5.4885 | dt: 59.22ms | tok/sec: 17292.90\n",
            "Step 346/1000 | Loss: 5.4564 | dt: 59.31ms | tok/sec: 17264.53\n",
            "Step 347/1000 | Loss: 5.2393 | dt: 59.88ms | tok/sec: 17099.64\n",
            "Step 348/1000 | Loss: 5.1147 | dt: 59.32ms | tok/sec: 17260.93\n",
            "Step 349/1000 | Loss: 4.9994 | dt: 59.59ms | tok/sec: 17184.47\n",
            "Step 350/1000 | Loss: 5.1952 | dt: 59.40ms | tok/sec: 17238.55\n",
            "Step 351/1000 | Loss: 4.6912 | dt: 58.59ms | tok/sec: 17477.19\n",
            "Step 352/1000 | Loss: 5.3960 | dt: 57.47ms | tok/sec: 17816.93\n",
            "Step 353/1000 | Loss: 4.7278 | dt: 56.63ms | tok/sec: 18082.55\n",
            "Step 354/1000 | Loss: 4.5062 | dt: 56.16ms | tok/sec: 18234.94\n",
            "Step 355/1000 | Loss: 4.9752 | dt: 58.33ms | tok/sec: 17555.56\n",
            "Step 356/1000 | Loss: 4.8934 | dt: 60.32ms | tok/sec: 16976.56\n",
            "Step 357/1000 | Loss: 4.9900 | dt: 59.92ms | tok/sec: 17090.18\n",
            "Step 358/1000 | Loss: 4.6819 | dt: 61.18ms | tok/sec: 16738.38\n",
            "Step 359/1000 | Loss: 4.9156 | dt: 58.98ms | tok/sec: 17360.98\n",
            "Step 360/1000 | Loss: 4.8741 | dt: 57.95ms | tok/sec: 17671.42\n",
            "Step 361/1000 | Loss: 5.0514 | dt: 56.99ms | tok/sec: 17969.00\n",
            "Step 362/1000 | Loss: 4.8410 | dt: 58.58ms | tok/sec: 17481.53\n",
            "Step 363/1000 | Loss: 5.1795 | dt: 57.14ms | tok/sec: 17921.01\n",
            "Step 364/1000 | Loss: 5.2142 | dt: 59.26ms | tok/sec: 17280.58\n",
            "Step 365/1000 | Loss: 5.1521 | dt: 59.17ms | tok/sec: 17306.27\n",
            "Step 366/1000 | Loss: 4.7070 | dt: 60.10ms | tok/sec: 17036.90\n",
            "Step 367/1000 | Loss: 4.7554 | dt: 59.60ms | tok/sec: 17180.56\n",
            "Step 368/1000 | Loss: 5.1161 | dt: 59.68ms | tok/sec: 17159.21\n",
            "Step 369/1000 | Loss: 4.5622 | dt: 59.67ms | tok/sec: 17160.58\n",
            "Step 370/1000 | Loss: 5.3043 | dt: 59.50ms | tok/sec: 17210.78\n",
            "Step 371/1000 | Loss: 5.1892 | dt: 59.41ms | tok/sec: 17234.88\n",
            "Step 372/1000 | Loss: 5.2166 | dt: 58.00ms | tok/sec: 17655.44\n",
            "Step 373/1000 | Loss: 5.1060 | dt: 54.29ms | tok/sec: 18861.90\n",
            "Step 374/1000 | Loss: 5.1865 | dt: 58.32ms | tok/sec: 17556.99\n",
            "Step 375/1000 | Loss: 5.1344 | dt: 57.87ms | tok/sec: 17695.01\n",
            "Step 376/1000 | Loss: 5.0314 | dt: 60.07ms | tok/sec: 17046.16\n",
            "Step 377/1000 | Loss: 5.7881 | dt: 59.62ms | tok/sec: 17174.85\n",
            "Step 378/1000 | Loss: 5.4419 | dt: 60.02ms | tok/sec: 17060.31\n",
            "Step 379/1000 | Loss: 5.7884 | dt: 59.71ms | tok/sec: 17150.51\n",
            "Step 380/1000 | Loss: 5.2710 | dt: 59.81ms | tok/sec: 17122.07\n",
            "Step 381/1000 | Loss: 5.1608 | dt: 59.92ms | tok/sec: 17089.57\n",
            "Step 382/1000 | Loss: 5.3495 | dt: 59.66ms | tok/sec: 17163.53\n",
            "Step 383/1000 | Loss: 5.4497 | dt: 58.91ms | tok/sec: 17383.18\n",
            "Step 384/1000 | Loss: 5.0804 | dt: 56.50ms | tok/sec: 18125.36\n",
            "Step 385/1000 | Loss: 5.3110 | dt: 57.22ms | tok/sec: 17894.73\n",
            "Step 386/1000 | Loss: 5.1292 | dt: 58.26ms | tok/sec: 17575.82\n",
            "Step 387/1000 | Loss: 5.0881 | dt: 59.53ms | tok/sec: 17200.58\n",
            "Step 388/1000 | Loss: 5.4488 | dt: 59.16ms | tok/sec: 17310.32\n",
            "Step 389/1000 | Loss: 5.0075 | dt: 60.22ms | tok/sec: 17004.32\n",
            "Step 390/1000 | Loss: 4.8910 | dt: 59.69ms | tok/sec: 17156.06\n",
            "Step 391/1000 | Loss: 5.0149 | dt: 59.76ms | tok/sec: 17136.41\n",
            "Step 392/1000 | Loss: 5.1209 | dt: 59.44ms | tok/sec: 17227.83\n",
            "Step 393/1000 | Loss: 5.0901 | dt: 59.90ms | tok/sec: 17094.53\n",
            "Step 394/1000 | Loss: 5.2007 | dt: 58.91ms | tok/sec: 17382.41\n",
            "Step 395/1000 | Loss: 4.9832 | dt: 57.80ms | tok/sec: 17715.30\n",
            "Step 396/1000 | Loss: 4.9266 | dt: 57.00ms | tok/sec: 17963.36\n",
            "Step 397/1000 | Loss: 5.0664 | dt: 58.64ms | tok/sec: 17462.98\n",
            "Step 398/1000 | Loss: 4.4238 | dt: 59.05ms | tok/sec: 17339.81\n",
            "Step 399/1000 | Loss: 4.8447 | dt: 59.34ms | tok/sec: 17256.83\n",
            "Step 400/1000 | Loss: 4.9950 | dt: 59.67ms | tok/sec: 17160.37\n",
            "Step 401/1000 | Loss: 4.8373 | dt: 58.57ms | tok/sec: 17482.24\n",
            "Step 402/1000 | Loss: 4.7909 | dt: 59.84ms | tok/sec: 17113.40\n",
            "Step 403/1000 | Loss: 5.0361 | dt: 59.64ms | tok/sec: 17169.50\n",
            "Step 404/1000 | Loss: 5.2826 | dt: 60.02ms | tok/sec: 17062.21\n",
            "Step 405/1000 | Loss: 5.0740 | dt: 59.68ms | tok/sec: 17157.77\n",
            "Step 406/1000 | Loss: 5.2375 | dt: 59.24ms | tok/sec: 17286.91\n",
            "Step 407/1000 | Loss: 4.8972 | dt: 58.91ms | tok/sec: 17381.42\n",
            "Step 408/1000 | Loss: 4.9597 | dt: 55.03ms | tok/sec: 18608.48\n",
            "Step 409/1000 | Loss: 5.1389 | dt: 58.62ms | tok/sec: 17468.87\n",
            "Step 410/1000 | Loss: 4.9754 | dt: 56.93ms | tok/sec: 17988.19\n",
            "Step 411/1000 | Loss: 5.5943 | dt: 59.85ms | tok/sec: 17108.36\n",
            "Step 412/1000 | Loss: 5.2304 | dt: 59.62ms | tok/sec: 17175.27\n",
            "Step 413/1000 | Loss: 5.1276 | dt: 58.54ms | tok/sec: 17491.00\n",
            "Step 414/1000 | Loss: 5.0906 | dt: 61.53ms | tok/sec: 16641.48\n",
            "Step 415/1000 | Loss: 4.7634 | dt: 60.10ms | tok/sec: 17037.37\n",
            "Step 416/1000 | Loss: 5.0235 | dt: 59.64ms | tok/sec: 17168.26\n",
            "Step 417/1000 | Loss: 4.9362 | dt: 60.23ms | tok/sec: 17000.55\n",
            "Step 418/1000 | Loss: 4.9482 | dt: 59.99ms | tok/sec: 17068.18\n",
            "Step 419/1000 | Loss: 5.4284 | dt: 59.86ms | tok/sec: 17107.47\n",
            "Step 420/1000 | Loss: 5.3790 | dt: 58.97ms | tok/sec: 17365.96\n",
            "Step 421/1000 | Loss: 5.1818 | dt: 57.33ms | tok/sec: 17862.35\n",
            "Step 422/1000 | Loss: 5.3035 | dt: 56.70ms | tok/sec: 18058.52\n",
            "Step 423/1000 | Loss: 5.1313 | dt: 58.52ms | tok/sec: 17497.27\n",
            "Step 424/1000 | Loss: 5.4795 | dt: 59.44ms | tok/sec: 17228.04\n",
            "Step 425/1000 | Loss: 5.5806 | dt: 59.67ms | tok/sec: 17161.27\n",
            "Step 426/1000 | Loss: 5.5657 | dt: 59.15ms | tok/sec: 17312.76\n",
            "Step 427/1000 | Loss: 5.5507 | dt: 60.22ms | tok/sec: 17004.11\n",
            "Step 428/1000 | Loss: 5.3781 | dt: 59.75ms | tok/sec: 17139.15\n",
            "Step 429/1000 | Loss: 5.6559 | dt: 59.89ms | tok/sec: 17096.85\n",
            "Step 430/1000 | Loss: 5.3051 | dt: 59.92ms | tok/sec: 17088.14\n",
            "Step 431/1000 | Loss: 5.2582 | dt: 60.91ms | tok/sec: 16810.31\n",
            "Step 432/1000 | Loss: 5.4648 | dt: 59.70ms | tok/sec: 17153.73\n",
            "Step 433/1000 | Loss: 5.5576 | dt: 59.03ms | tok/sec: 17346.18\n",
            "Step 434/1000 | Loss: 5.2714 | dt: 59.61ms | tok/sec: 17178.36\n",
            "Step 435/1000 | Loss: 5.2954 | dt: 57.85ms | tok/sec: 17701.50\n",
            "Step 436/1000 | Loss: 5.5472 | dt: 56.47ms | tok/sec: 18134.08\n",
            "Step 437/1000 | Loss: 5.1653 | dt: 56.66ms | tok/sec: 18071.97\n",
            "Step 438/1000 | Loss: 5.2288 | dt: 59.43ms | tok/sec: 17229.63\n",
            "Step 439/1000 | Loss: 5.0999 | dt: 60.94ms | tok/sec: 16804.26\n",
            "Step 440/1000 | Loss: 5.2755 | dt: 59.38ms | tok/sec: 17244.64\n",
            "Step 441/1000 | Loss: 5.3645 | dt: 59.83ms | tok/sec: 17115.17\n",
            "Step 442/1000 | Loss: 5.5629 | dt: 59.52ms | tok/sec: 17203.33\n",
            "Step 443/1000 | Loss: 5.5024 | dt: 59.58ms | tok/sec: 17186.19\n",
            "Step 444/1000 | Loss: 5.1785 | dt: 60.19ms | tok/sec: 17013.14\n",
            "Step 445/1000 | Loss: 5.4095 | dt: 62.02ms | tok/sec: 16511.42\n",
            "Step 446/1000 | Loss: 5.0665 | dt: 59.16ms | tok/sec: 17309.27\n",
            "Step 447/1000 | Loss: 5.4519 | dt: 60.19ms | tok/sec: 17011.73\n",
            "Step 448/1000 | Loss: 5.0308 | dt: 61.50ms | tok/sec: 16650.06\n",
            "Step 449/1000 | Loss: 5.2675 | dt: 58.84ms | tok/sec: 17403.47\n",
            "Step 450/1000 | Loss: 5.1319 | dt: 59.19ms | tok/sec: 17301.46\n",
            "Step 451/1000 | Loss: 5.1407 | dt: 59.62ms | tok/sec: 17175.95\n",
            "Step 452/1000 | Loss: 4.9541 | dt: 59.66ms | tok/sec: 17162.50\n",
            "Step 453/1000 | Loss: 5.1302 | dt: 61.39ms | tok/sec: 16681.36\n",
            "Step 454/1000 | Loss: 4.8093 | dt: 60.04ms | tok/sec: 17055.84\n",
            "Step 455/1000 | Loss: 4.4137 | dt: 59.90ms | tok/sec: 17096.57\n",
            "Step 456/1000 | Loss: 4.6705 | dt: 61.03ms | tok/sec: 16778.20\n",
            "Step 457/1000 | Loss: 4.8662 | dt: 60.00ms | tok/sec: 17065.33\n",
            "Step 458/1000 | Loss: 5.5128 | dt: 59.87ms | tok/sec: 17103.25\n",
            "Step 459/1000 | Loss: 5.1444 | dt: 59.95ms | tok/sec: 17080.60\n",
            "Step 460/1000 | Loss: 5.3419 | dt: 58.93ms | tok/sec: 17377.63\n",
            "Step 461/1000 | Loss: 5.5429 | dt: 61.20ms | tok/sec: 16733.36\n",
            "Step 462/1000 | Loss: 5.1141 | dt: 62.02ms | tok/sec: 16510.15\n",
            "Step 463/1000 | Loss: 5.6071 | dt: 60.02ms | tok/sec: 17060.24\n",
            "Step 464/1000 | Loss: 5.3732 | dt: 60.81ms | tok/sec: 16838.72\n",
            "Step 465/1000 | Loss: 5.3402 | dt: 63.43ms | tok/sec: 16143.82\n",
            "Step 466/1000 | Loss: 5.2473 | dt: 59.37ms | tok/sec: 17248.80\n",
            "Step 467/1000 | Loss: 5.9272 | dt: 58.59ms | tok/sec: 17477.40\n",
            "Step 468/1000 | Loss: 5.1601 | dt: 60.69ms | tok/sec: 16872.92\n",
            "Step 469/1000 | Loss: 5.0130 | dt: 60.23ms | tok/sec: 17002.50\n",
            "Step 470/1000 | Loss: 5.4407 | dt: 61.54ms | tok/sec: 16640.90\n",
            "Step 471/1000 | Loss: 5.0928 | dt: 60.12ms | tok/sec: 17033.85\n",
            "Step 472/1000 | Loss: 4.9567 | dt: 58.18ms | tok/sec: 17600.09\n",
            "Step 473/1000 | Loss: 5.7216 | dt: 60.67ms | tok/sec: 16879.08\n",
            "Step 474/1000 | Loss: 5.4570 | dt: 59.86ms | tok/sec: 17107.13\n",
            "Step 475/1000 | Loss: 4.9615 | dt: 60.24ms | tok/sec: 16997.39\n",
            "Step 476/1000 | Loss: 4.9690 | dt: 59.86ms | tok/sec: 17107.74\n",
            "Step 477/1000 | Loss: 5.1406 | dt: 59.86ms | tok/sec: 17105.43\n",
            "Step 478/1000 | Loss: 5.1583 | dt: 58.54ms | tok/sec: 17490.93\n",
            "Step 479/1000 | Loss: 5.2150 | dt: 57.77ms | tok/sec: 17725.75\n",
            "Step 480/1000 | Loss: 4.9983 | dt: 57.79ms | tok/sec: 17719.75\n",
            "Step 481/1000 | Loss: 5.5820 | dt: 58.14ms | tok/sec: 17613.30\n",
            "Step 482/1000 | Loss: 5.2222 | dt: 59.10ms | tok/sec: 17325.61\n",
            "Step 483/1000 | Loss: 5.2737 | dt: 59.40ms | tok/sec: 17239.66\n",
            "Step 484/1000 | Loss: 5.0343 | dt: 60.07ms | tok/sec: 17047.38\n",
            "Step 485/1000 | Loss: 5.0774 | dt: 59.89ms | tok/sec: 17097.46\n",
            "Step 486/1000 | Loss: 4.9465 | dt: 59.52ms | tok/sec: 17203.13\n",
            "Step 487/1000 | Loss: 4.6256 | dt: 57.74ms | tok/sec: 17734.31\n",
            "Step 488/1000 | Loss: 5.0196 | dt: 60.09ms | tok/sec: 17040.34\n",
            "Step 489/1000 | Loss: 4.7077 | dt: 60.86ms | tok/sec: 16824.21\n",
            "Step 490/1000 | Loss: 5.0875 | dt: 61.16ms | tok/sec: 16741.77\n",
            "Step 491/1000 | Loss: 4.9083 | dt: 59.81ms | tok/sec: 17119.88\n",
            "Step 492/1000 | Loss: 4.9877 | dt: 59.73ms | tok/sec: 17143.39\n",
            "Step 493/1000 | Loss: 4.8890 | dt: 58.98ms | tok/sec: 17363.22\n",
            "Step 494/1000 | Loss: 5.3926 | dt: 57.85ms | tok/sec: 17700.99\n",
            "Step 495/1000 | Loss: 5.4593 | dt: 57.74ms | tok/sec: 17734.10\n",
            "Step 496/1000 | Loss: 5.2507 | dt: 58.42ms | tok/sec: 17527.26\n",
            "Step 497/1000 | Loss: 5.2091 | dt: 59.29ms | tok/sec: 17271.41\n",
            "Step 498/1000 | Loss: 4.9376 | dt: 59.59ms | tok/sec: 17182.69\n",
            "Step 499/1000 | Loss: 5.1788 | dt: 59.63ms | tok/sec: 17172.24\n",
            "Step 500/1000 | Loss: 5.1046 | dt: 57.63ms | tok/sec: 17769.39\n",
            "Step 501/1000 | Loss: 5.1146 | dt: 60.64ms | tok/sec: 16887.18\n",
            "Step 502/1000 | Loss: 4.9794 | dt: 60.45ms | tok/sec: 16940.00\n",
            "Step 503/1000 | Loss: 4.9910 | dt: 60.11ms | tok/sec: 17035.81\n",
            "Step 504/1000 | Loss: 4.8852 | dt: 60.38ms | tok/sec: 16958.79\n",
            "Step 505/1000 | Loss: 4.9784 | dt: 59.88ms | tok/sec: 17099.77\n",
            "Step 506/1000 | Loss: 5.4918 | dt: 60.22ms | tok/sec: 17003.78\n",
            "Step 507/1000 | Loss: 5.3303 | dt: 60.13ms | tok/sec: 17031.09\n",
            "Step 508/1000 | Loss: 5.2777 | dt: 59.26ms | tok/sec: 17281.14\n",
            "Step 509/1000 | Loss: 5.4207 | dt: 59.96ms | tok/sec: 17078.97\n",
            "Step 510/1000 | Loss: 5.3204 | dt: 58.77ms | tok/sec: 17423.10\n",
            "Step 511/1000 | Loss: 5.1820 | dt: 57.00ms | tok/sec: 17965.01\n",
            "Step 512/1000 | Loss: 4.7983 | dt: 58.12ms | tok/sec: 17619.22\n",
            "Step 513/1000 | Loss: 5.2349 | dt: 58.55ms | tok/sec: 17489.50\n",
            "Step 514/1000 | Loss: 5.4053 | dt: 59.32ms | tok/sec: 17260.86\n",
            "Step 515/1000 | Loss: 5.0542 | dt: 59.30ms | tok/sec: 17268.56\n",
            "Step 516/1000 | Loss: 5.2912 | dt: 59.14ms | tok/sec: 17314.79\n",
            "Step 517/1000 | Loss: 5.1470 | dt: 61.42ms | tok/sec: 16670.94\n",
            "Step 518/1000 | Loss: 4.6545 | dt: 59.59ms | tok/sec: 17184.68\n",
            "Step 519/1000 | Loss: 4.3002 | dt: 59.50ms | tok/sec: 17209.75\n",
            "Step 520/1000 | Loss: 4.8987 | dt: 59.55ms | tok/sec: 17194.52\n",
            "Step 521/1000 | Loss: 5.2622 | dt: 60.15ms | tok/sec: 17022.85\n",
            "Step 522/1000 | Loss: 5.3145 | dt: 59.34ms | tok/sec: 17255.73\n",
            "Step 523/1000 | Loss: 5.1177 | dt: 59.76ms | tok/sec: 17135.39\n",
            "Step 524/1000 | Loss: 4.8066 | dt: 59.91ms | tok/sec: 17091.13\n",
            "Step 525/1000 | Loss: 4.9714 | dt: 56.81ms | tok/sec: 18025.33\n",
            "Step 526/1000 | Loss: 5.0348 | dt: 57.93ms | tok/sec: 17676.58\n",
            "Step 527/1000 | Loss: 4.8168 | dt: 57.83ms | tok/sec: 17707.34\n",
            "Step 528/1000 | Loss: 4.9340 | dt: 58.86ms | tok/sec: 17398.53\n",
            "Step 529/1000 | Loss: 4.9285 | dt: 59.97ms | tok/sec: 17074.82\n",
            "Step 530/1000 | Loss: 4.7835 | dt: 59.57ms | tok/sec: 17189.50\n",
            "Step 531/1000 | Loss: 5.1636 | dt: 59.95ms | tok/sec: 17080.26\n",
            "Step 532/1000 | Loss: 4.7322 | dt: 59.71ms | tok/sec: 17148.25\n",
            "Step 533/1000 | Loss: 4.9115 | dt: 60.90ms | tok/sec: 16815.44\n",
            "Step 534/1000 | Loss: 5.2618 | dt: 61.16ms | tok/sec: 16743.01\n",
            "Step 535/1000 | Loss: 4.8308 | dt: 60.06ms | tok/sec: 17048.66\n",
            "Step 536/1000 | Loss: 4.3405 | dt: 60.08ms | tok/sec: 17042.57\n",
            "Step 537/1000 | Loss: 4.9483 | dt: 60.11ms | tok/sec: 17034.19\n",
            "Step 538/1000 | Loss: 5.2817 | dt: 59.63ms | tok/sec: 17171.63\n",
            "Step 539/1000 | Loss: 5.6432 | dt: 59.93ms | tok/sec: 17085.62\n",
            "Step 540/1000 | Loss: 5.4601 | dt: 59.70ms | tok/sec: 17151.95\n",
            "Step 541/1000 | Loss: 5.5341 | dt: 60.15ms | tok/sec: 17023.53\n",
            "Step 542/1000 | Loss: 5.5422 | dt: 62.04ms | tok/sec: 16505.20\n",
            "Step 543/1000 | Loss: 5.6790 | dt: 59.72ms | tok/sec: 17147.63\n",
            "Step 544/1000 | Loss: 5.2406 | dt: 59.70ms | tok/sec: 17153.11\n",
            "Step 545/1000 | Loss: 5.2101 | dt: 58.78ms | tok/sec: 17421.96\n",
            "Step 546/1000 | Loss: 5.3797 | dt: 57.16ms | tok/sec: 17916.00\n",
            "Step 547/1000 | Loss: 5.2388 | dt: 57.03ms | tok/sec: 17955.10\n",
            "Step 548/1000 | Loss: 5.2504 | dt: 59.31ms | tok/sec: 17264.05\n",
            "Step 549/1000 | Loss: 5.0082 | dt: 58.71ms | tok/sec: 17440.85\n",
            "Step 550/1000 | Loss: 5.0870 | dt: 58.29ms | tok/sec: 17567.33\n",
            "Step 551/1000 | Loss: 5.1357 | dt: 60.26ms | tok/sec: 16992.41\n",
            "Step 552/1000 | Loss: 5.3908 | dt: 59.99ms | tok/sec: 17068.99\n",
            "Step 553/1000 | Loss: 5.1107 | dt: 59.50ms | tok/sec: 17209.68\n",
            "Step 554/1000 | Loss: 5.4587 | dt: 59.70ms | tok/sec: 17153.45\n",
            "Step 555/1000 | Loss: 5.4205 | dt: 59.76ms | tok/sec: 17134.36\n",
            "Step 556/1000 | Loss: 5.6605 | dt: 60.61ms | tok/sec: 16895.82\n",
            "Step 557/1000 | Loss: 5.6527 | dt: 59.76ms | tok/sec: 17135.86\n",
            "Step 558/1000 | Loss: 5.6837 | dt: 57.96ms | tok/sec: 17668.15\n",
            "Step 559/1000 | Loss: 5.2818 | dt: 57.50ms | tok/sec: 17808.36\n",
            "Step 560/1000 | Loss: 5.4086 | dt: 58.22ms | tok/sec: 17588.77\n",
            "Step 561/1000 | Loss: 5.4185 | dt: 58.94ms | tok/sec: 17372.85\n",
            "Step 562/1000 | Loss: 5.7383 | dt: 59.76ms | tok/sec: 17136.00\n",
            "Step 563/1000 | Loss: 5.2001 | dt: 59.48ms | tok/sec: 17215.33\n",
            "Step 564/1000 | Loss: 5.1622 | dt: 59.80ms | tok/sec: 17123.23\n",
            "Step 565/1000 | Loss: 5.1057 | dt: 59.90ms | tok/sec: 17094.40\n",
            "Step 566/1000 | Loss: 5.0865 | dt: 59.68ms | tok/sec: 17159.00\n",
            "Step 567/1000 | Loss: 5.3873 | dt: 60.09ms | tok/sec: 17040.55\n",
            "Step 568/1000 | Loss: 5.0220 | dt: 60.16ms | tok/sec: 17022.58\n",
            "Step 569/1000 | Loss: 5.0798 | dt: 60.24ms | tok/sec: 16998.60\n",
            "Step 570/1000 | Loss: 5.3689 | dt: 60.18ms | tok/sec: 17014.83\n",
            "Step 571/1000 | Loss: 4.9871 | dt: 59.94ms | tok/sec: 17082.84\n",
            "Step 572/1000 | Loss: 5.1342 | dt: 60.09ms | tok/sec: 17042.17\n",
            "Step 573/1000 | Loss: 5.0395 | dt: 60.35ms | tok/sec: 16967.84\n",
            "Step 574/1000 | Loss: 5.7146 | dt: 59.59ms | tok/sec: 17184.27\n",
            "Step 575/1000 | Loss: 5.4170 | dt: 59.64ms | tok/sec: 17168.95\n",
            "Step 576/1000 | Loss: 4.9023 | dt: 57.48ms | tok/sec: 17814.42\n",
            "Step 577/1000 | Loss: 4.9040 | dt: 57.89ms | tok/sec: 17688.88\n",
            "Step 578/1000 | Loss: 5.3293 | dt: 57.98ms | tok/sec: 17661.17\n",
            "Step 579/1000 | Loss: 5.0787 | dt: 58.53ms | tok/sec: 17494.49\n",
            "Step 580/1000 | Loss: 5.2570 | dt: 59.73ms | tok/sec: 17143.18\n",
            "Step 581/1000 | Loss: 5.3732 | dt: 69.70ms | tok/sec: 14691.53\n",
            "Step 582/1000 | Loss: 5.3044 | dt: 74.22ms | tok/sec: 13797.41\n",
            "Step 583/1000 | Loss: 5.2812 | dt: 68.79ms | tok/sec: 14885.07\n",
            "Step 584/1000 | Loss: 5.1118 | dt: 58.67ms | tok/sec: 17452.76\n",
            "Step 585/1000 | Loss: 4.8938 | dt: 59.22ms | tok/sec: 17291.22\n",
            "Step 586/1000 | Loss: 5.2320 | dt: 59.09ms | tok/sec: 17330.93\n",
            "Step 587/1000 | Loss: 4.8716 | dt: 58.33ms | tok/sec: 17556.35\n",
            "Step 588/1000 | Loss: 4.6787 | dt: 59.34ms | tok/sec: 17256.90\n",
            "Step 589/1000 | Loss: 5.1677 | dt: 59.62ms | tok/sec: 17175.54\n",
            "Step 590/1000 | Loss: 4.9064 | dt: 59.50ms | tok/sec: 17210.64\n",
            "Step 591/1000 | Loss: 4.8832 | dt: 57.70ms | tok/sec: 17746.48\n",
            "Step 592/1000 | Loss: 5.0306 | dt: 60.75ms | tok/sec: 16856.89\n",
            "Step 593/1000 | Loss: 5.0766 | dt: 59.03ms | tok/sec: 17346.67\n",
            "Step 594/1000 | Loss: 5.1771 | dt: 79.73ms | tok/sec: 12843.84\n",
            "Step 595/1000 | Loss: 5.2733 | dt: 84.29ms | tok/sec: 12149.15\n",
            "Step 596/1000 | Loss: 5.5120 | dt: 61.37ms | tok/sec: 16684.60\n",
            "Step 597/1000 | Loss: 5.0179 | dt: 58.48ms | tok/sec: 17510.32\n",
            "Step 598/1000 | Loss: 4.8608 | dt: 61.50ms | tok/sec: 16650.58\n",
            "Step 599/1000 | Loss: 5.2057 | dt: 58.98ms | tok/sec: 17362.31\n",
            "Step 600/1000 | Loss: 5.2441 | dt: 60.20ms | tok/sec: 17010.92\n",
            "Step 601/1000 | Loss: 4.9094 | dt: 60.15ms | tok/sec: 17024.87\n",
            "Step 602/1000 | Loss: 4.8074 | dt: 59.90ms | tok/sec: 17095.89\n",
            "Step 603/1000 | Loss: 5.0259 | dt: 59.79ms | tok/sec: 17125.55\n",
            "Step 604/1000 | Loss: 5.2645 | dt: 60.68ms | tok/sec: 16876.23\n",
            "Step 605/1000 | Loss: 4.6975 | dt: 61.63ms | tok/sec: 16614.51\n",
            "Step 606/1000 | Loss: 4.9771 | dt: 60.27ms | tok/sec: 16991.47\n",
            "Step 607/1000 | Loss: 4.8357 | dt: 60.37ms | tok/sec: 16961.54\n",
            "Step 608/1000 | Loss: 4.6637 | dt: 60.15ms | tok/sec: 17024.54\n",
            "Step 609/1000 | Loss: 4.6548 | dt: 60.07ms | tok/sec: 17047.85\n",
            "Step 610/1000 | Loss: 4.8139 | dt: 60.91ms | tok/sec: 16811.50\n",
            "Step 611/1000 | Loss: 4.4154 | dt: 59.90ms | tok/sec: 17096.44\n",
            "Step 612/1000 | Loss: 4.8143 | dt: 60.54ms | tok/sec: 16913.32\n",
            "Step 613/1000 | Loss: 4.4119 | dt: 60.67ms | tok/sec: 16879.28\n",
            "Step 614/1000 | Loss: 5.0659 | dt: 60.44ms | tok/sec: 16943.74\n",
            "Step 615/1000 | Loss: 5.2253 | dt: 60.27ms | tok/sec: 16991.54\n",
            "Step 616/1000 | Loss: 5.7681 | dt: 59.91ms | tok/sec: 17091.27\n",
            "Step 617/1000 | Loss: 5.0253 | dt: 60.16ms | tok/sec: 17021.30\n",
            "Step 618/1000 | Loss: 5.4128 | dt: 61.67ms | tok/sec: 16603.66\n",
            "Step 619/1000 | Loss: 5.3388 | dt: 59.98ms | tok/sec: 17071.16\n",
            "Step 620/1000 | Loss: 5.0926 | dt: 60.45ms | tok/sec: 16940.93\n",
            "Step 621/1000 | Loss: 5.0506 | dt: 59.63ms | tok/sec: 17171.21\n",
            "Step 622/1000 | Loss: 5.4122 | dt: 58.34ms | tok/sec: 17551.25\n",
            "Step 623/1000 | Loss: 5.1708 | dt: 57.72ms | tok/sec: 17741.64\n",
            "Step 624/1000 | Loss: 4.7043 | dt: 57.56ms | tok/sec: 17789.11\n",
            "Step 625/1000 | Loss: 4.9663 | dt: 58.71ms | tok/sec: 17440.71\n",
            "Step 626/1000 | Loss: 4.9377 | dt: 59.99ms | tok/sec: 17070.41\n",
            "Step 627/1000 | Loss: 5.0876 | dt: 59.58ms | tok/sec: 17186.33\n",
            "Step 628/1000 | Loss: 4.3851 | dt: 59.29ms | tok/sec: 17270.30\n",
            "Step 629/1000 | Loss: 4.9166 | dt: 59.59ms | tok/sec: 17184.41\n",
            "Step 630/1000 | Loss: 5.1829 | dt: 60.24ms | tok/sec: 16999.87\n",
            "Step 631/1000 | Loss: 5.3652 | dt: 60.70ms | tok/sec: 16870.40\n",
            "Step 632/1000 | Loss: 5.0215 | dt: 60.05ms | tok/sec: 17052.79\n",
            "Step 633/1000 | Loss: 5.0623 | dt: 60.08ms | tok/sec: 17045.35\n",
            "Step 634/1000 | Loss: 5.0104 | dt: 60.63ms | tok/sec: 16890.30\n",
            "Step 635/1000 | Loss: 4.7095 | dt: 57.98ms | tok/sec: 17662.41\n",
            "Step 636/1000 | Loss: 5.3693 | dt: 59.83ms | tok/sec: 17114.36\n",
            "Step 637/1000 | Loss: 5.0541 | dt: 60.26ms | tok/sec: 16991.80\n",
            "Step 638/1000 | Loss: 4.9215 | dt: 62.50ms | tok/sec: 16383.19\n",
            "Step 639/1000 | Loss: 4.6683 | dt: 61.61ms | tok/sec: 16619.65\n",
            "Step 640/1000 | Loss: 4.6500 | dt: 61.14ms | tok/sec: 16749.02\n",
            "Step 641/1000 | Loss: 4.7137 | dt: 62.14ms | tok/sec: 16479.55\n",
            "Step 642/1000 | Loss: 4.6122 | dt: 60.05ms | tok/sec: 17051.51\n",
            "Step 643/1000 | Loss: 4.7431 | dt: 60.03ms | tok/sec: 17058.41\n",
            "Step 644/1000 | Loss: 4.7121 | dt: 60.13ms | tok/sec: 17029.87\n",
            "Step 645/1000 | Loss: 4.6338 | dt: 59.60ms | tok/sec: 17182.07\n",
            "Step 646/1000 | Loss: 4.5980 | dt: 60.53ms | tok/sec: 16917.65\n",
            "Step 647/1000 | Loss: 4.4603 | dt: 60.09ms | tok/sec: 17042.17\n",
            "Step 648/1000 | Loss: 4.1909 | dt: 59.99ms | tok/sec: 17069.33\n",
            "Step 649/1000 | Loss: 4.8620 | dt: 61.32ms | tok/sec: 16698.03\n",
            "Step 650/1000 | Loss: 5.2651 | dt: 59.59ms | tok/sec: 17185.37\n",
            "Step 651/1000 | Loss: 5.4388 | dt: 59.43ms | tok/sec: 17229.84\n",
            "Step 652/1000 | Loss: 5.6791 | dt: 60.35ms | tok/sec: 16968.91\n",
            "Step 653/1000 | Loss: 5.7236 | dt: 60.22ms | tok/sec: 17004.52\n",
            "Step 654/1000 | Loss: 5.4252 | dt: 59.70ms | tok/sec: 17151.40\n",
            "Step 655/1000 | Loss: 5.6115 | dt: 63.05ms | tok/sec: 16240.76\n",
            "Step 656/1000 | Loss: 5.5574 | dt: 62.16ms | tok/sec: 16474.12\n",
            "Step 657/1000 | Loss: 5.1548 | dt: 60.45ms | tok/sec: 16939.13\n",
            "Step 658/1000 | Loss: 5.0740 | dt: 59.25ms | tok/sec: 17282.39\n",
            "Step 659/1000 | Loss: 5.2490 | dt: 60.22ms | tok/sec: 17004.52\n",
            "Step 660/1000 | Loss: 5.1533 | dt: 61.74ms | tok/sec: 16586.86\n",
            "Step 661/1000 | Loss: 5.5287 | dt: 60.04ms | tok/sec: 17054.82\n",
            "Step 662/1000 | Loss: 5.2338 | dt: 60.82ms | tok/sec: 16835.48\n",
            "Step 663/1000 | Loss: 5.5853 | dt: 59.41ms | tok/sec: 17235.44\n",
            "Step 664/1000 | Loss: 5.3748 | dt: 61.14ms | tok/sec: 16747.84\n",
            "Step 665/1000 | Loss: 5.5277 | dt: 60.66ms | tok/sec: 16881.60\n",
            "Step 666/1000 | Loss: 5.0985 | dt: 60.70ms | tok/sec: 16871.19\n",
            "Step 667/1000 | Loss: 5.3362 | dt: 60.00ms | tok/sec: 17067.29\n",
            "Step 668/1000 | Loss: 5.1820 | dt: 60.25ms | tok/sec: 16994.90\n",
            "Step 669/1000 | Loss: 5.3047 | dt: 59.38ms | tok/sec: 17246.23\n",
            "Step 670/1000 | Loss: 5.0762 | dt: 59.23ms | tok/sec: 17289.21\n",
            "Step 671/1000 | Loss: 4.9974 | dt: 60.42ms | tok/sec: 16947.89\n",
            "Step 672/1000 | Loss: 5.6946 | dt: 60.21ms | tok/sec: 17008.29\n",
            "Step 673/1000 | Loss: 5.0039 | dt: 60.11ms | tok/sec: 17034.40\n",
            "Step 674/1000 | Loss: 5.2326 | dt: 60.22ms | tok/sec: 17005.53\n",
            "Step 675/1000 | Loss: 5.1842 | dt: 59.83ms | tok/sec: 17114.08\n",
            "Step 676/1000 | Loss: 5.1619 | dt: 60.80ms | tok/sec: 16841.16\n",
            "Step 677/1000 | Loss: 4.9715 | dt: 60.57ms | tok/sec: 16906.06\n",
            "Step 678/1000 | Loss: 4.8155 | dt: 60.12ms | tok/sec: 17032.77\n",
            "Step 679/1000 | Loss: 4.7069 | dt: 59.88ms | tok/sec: 17102.23\n",
            "Step 680/1000 | Loss: 4.9266 | dt: 59.40ms | tok/sec: 17238.83\n",
            "Step 681/1000 | Loss: 4.3858 | dt: 60.96ms | tok/sec: 16797.69\n",
            "Step 682/1000 | Loss: 5.1159 | dt: 59.36ms | tok/sec: 17249.63\n",
            "Step 683/1000 | Loss: 4.4160 | dt: 59.95ms | tok/sec: 17080.94\n",
            "Step 684/1000 | Loss: 4.2318 | dt: 60.97ms | tok/sec: 16795.26\n",
            "Step 685/1000 | Loss: 4.7327 | dt: 59.13ms | tok/sec: 17316.60\n",
            "Step 686/1000 | Loss: 4.6304 | dt: 59.88ms | tok/sec: 17099.84\n",
            "Step 687/1000 | Loss: 4.7089 | dt: 60.91ms | tok/sec: 16812.48\n",
            "Step 688/1000 | Loss: 4.4299 | dt: 60.87ms | tok/sec: 16822.36\n",
            "Step 689/1000 | Loss: 4.6449 | dt: 59.99ms | tok/sec: 17069.53\n",
            "Step 690/1000 | Loss: 4.5801 | dt: 58.98ms | tok/sec: 17360.49\n",
            "Step 691/1000 | Loss: 4.7844 | dt: 57.49ms | tok/sec: 17810.58\n",
            "Step 692/1000 | Loss: 4.5620 | dt: 57.90ms | tok/sec: 17684.44\n",
            "Step 693/1000 | Loss: 4.8866 | dt: 59.15ms | tok/sec: 17312.97\n",
            "Step 694/1000 | Loss: 4.9481 | dt: 59.59ms | tok/sec: 17185.37\n",
            "Step 695/1000 | Loss: 4.9108 | dt: 59.95ms | tok/sec: 17082.02\n",
            "Step 696/1000 | Loss: 4.4536 | dt: 59.46ms | tok/sec: 17221.41\n",
            "Step 697/1000 | Loss: 4.5506 | dt: 59.94ms | tok/sec: 17084.74\n",
            "Step 698/1000 | Loss: 4.8782 | dt: 60.56ms | tok/sec: 16908.19\n",
            "Step 699/1000 | Loss: 4.3864 | dt: 59.99ms | tok/sec: 17070.07\n",
            "Step 700/1000 | Loss: 5.1014 | dt: 60.47ms | tok/sec: 16933.92\n",
            "Step 701/1000 | Loss: 4.9957 | dt: 60.57ms | tok/sec: 16907.26\n",
            "Step 702/1000 | Loss: 5.0010 | dt: 59.89ms | tok/sec: 17097.53\n",
            "Step 703/1000 | Loss: 4.8963 | dt: 59.73ms | tok/sec: 17144.69\n",
            "Step 704/1000 | Loss: 4.9991 | dt: 62.50ms | tok/sec: 16384.06\n",
            "Step 705/1000 | Loss: 4.9393 | dt: 60.12ms | tok/sec: 17033.04\n",
            "Step 706/1000 | Loss: 4.8135 | dt: 58.75ms | tok/sec: 17430.17\n",
            "Step 707/1000 | Loss: 5.4898 | dt: 60.68ms | tok/sec: 16875.37\n",
            "Step 708/1000 | Loss: 5.0788 | dt: 60.16ms | tok/sec: 17020.62\n",
            "Step 709/1000 | Loss: 5.4855 | dt: 60.29ms | tok/sec: 16985.42\n",
            "Step 710/1000 | Loss: 4.8926 | dt: 60.62ms | tok/sec: 16891.17\n",
            "Step 711/1000 | Loss: 4.8234 | dt: 60.52ms | tok/sec: 16921.18\n",
            "Step 712/1000 | Loss: 5.0483 | dt: 60.93ms | tok/sec: 16804.98\n",
            "Step 713/1000 | Loss: 5.1262 | dt: 60.20ms | tok/sec: 17010.58\n",
            "Step 714/1000 | Loss: 4.7967 | dt: 60.05ms | tok/sec: 17051.84\n",
            "Step 715/1000 | Loss: 5.0115 | dt: 58.92ms | tok/sec: 17380.44\n",
            "Step 716/1000 | Loss: 4.8371 | dt: 61.00ms | tok/sec: 16787.71\n",
            "Step 717/1000 | Loss: 4.8420 | dt: 60.51ms | tok/sec: 16922.51\n",
            "Step 718/1000 | Loss: 5.1547 | dt: 60.84ms | tok/sec: 16832.05\n",
            "Step 719/1000 | Loss: 4.7591 | dt: 59.27ms | tok/sec: 17275.44\n",
            "Step 720/1000 | Loss: 4.6603 | dt: 59.55ms | tok/sec: 17194.93\n",
            "Step 721/1000 | Loss: 4.7515 | dt: 63.31ms | tok/sec: 16175.01\n",
            "Step 722/1000 | Loss: 4.8375 | dt: 60.45ms | tok/sec: 16939.93\n",
            "Step 723/1000 | Loss: 4.7665 | dt: 61.19ms | tok/sec: 16735.38\n",
            "Step 724/1000 | Loss: 4.9103 | dt: 60.25ms | tok/sec: 16995.77\n",
            "Step 725/1000 | Loss: 4.7536 | dt: 60.95ms | tok/sec: 16800.38\n",
            "Step 726/1000 | Loss: 4.5352 | dt: 60.96ms | tok/sec: 16796.70\n",
            "Step 727/1000 | Loss: 4.7643 | dt: 59.28ms | tok/sec: 17275.02\n",
            "Step 728/1000 | Loss: 4.1365 | dt: 61.31ms | tok/sec: 16702.58\n",
            "Step 729/1000 | Loss: 4.5321 | dt: 58.99ms | tok/sec: 17358.87\n",
            "Step 730/1000 | Loss: 4.7276 | dt: 61.05ms | tok/sec: 16773.74\n",
            "Step 731/1000 | Loss: 4.5877 | dt: 60.58ms | tok/sec: 16903.00\n",
            "Step 732/1000 | Loss: 4.5237 | dt: 60.53ms | tok/sec: 16918.38\n",
            "Step 733/1000 | Loss: 4.7919 | dt: 60.95ms | tok/sec: 16799.33\n",
            "Step 734/1000 | Loss: 5.0351 | dt: 59.84ms | tok/sec: 17113.67\n",
            "Step 735/1000 | Loss: 4.8418 | dt: 61.11ms | tok/sec: 16756.27\n",
            "Step 736/1000 | Loss: 5.0009 | dt: 60.85ms | tok/sec: 16827.30\n",
            "Step 737/1000 | Loss: 4.6651 | dt: 60.56ms | tok/sec: 16907.59\n",
            "Step 738/1000 | Loss: 4.6797 | dt: 61.98ms | tok/sec: 16522.73\n",
            "Step 739/1000 | Loss: 4.7381 | dt: 59.10ms | tok/sec: 17326.94\n",
            "Step 740/1000 | Loss: 4.5376 | dt: 60.40ms | tok/sec: 16954.18\n",
            "Step 741/1000 | Loss: 5.2488 | dt: 60.75ms | tok/sec: 16855.77\n",
            "Step 742/1000 | Loss: 4.9151 | dt: 60.39ms | tok/sec: 16955.58\n",
            "Step 743/1000 | Loss: 4.7618 | dt: 59.87ms | tok/sec: 17103.11\n",
            "Step 744/1000 | Loss: 4.7118 | dt: 61.01ms | tok/sec: 16785.15\n",
            "Step 745/1000 | Loss: 4.3890 | dt: 60.55ms | tok/sec: 16912.58\n",
            "Step 746/1000 | Loss: 4.6946 | dt: 60.70ms | tok/sec: 16869.80\n",
            "Step 747/1000 | Loss: 4.5907 | dt: 60.40ms | tok/sec: 16952.77\n",
            "Step 748/1000 | Loss: 4.5980 | dt: 60.49ms | tok/sec: 16928.11\n",
            "Step 749/1000 | Loss: 5.1520 | dt: 60.26ms | tok/sec: 16994.22\n",
            "Step 750/1000 | Loss: 5.0878 | dt: 61.01ms | tok/sec: 16784.76\n",
            "Step 751/1000 | Loss: 4.8991 | dt: 60.42ms | tok/sec: 16947.55\n",
            "Step 752/1000 | Loss: 5.0816 | dt: 60.97ms | tok/sec: 16794.08\n",
            "Step 753/1000 | Loss: 4.8801 | dt: 61.85ms | tok/sec: 16557.51\n",
            "Step 754/1000 | Loss: 5.1673 | dt: 60.56ms | tok/sec: 16908.12\n",
            "Step 755/1000 | Loss: 5.2599 | dt: 58.43ms | tok/sec: 17525.11\n",
            "Step 756/1000 | Loss: 5.2375 | dt: 61.00ms | tok/sec: 16788.23\n",
            "Step 757/1000 | Loss: 5.2675 | dt: 60.49ms | tok/sec: 16929.32\n",
            "Step 758/1000 | Loss: 5.1054 | dt: 61.07ms | tok/sec: 16768.70\n",
            "Step 759/1000 | Loss: 5.3661 | dt: 59.20ms | tok/sec: 17297.91\n",
            "Step 760/1000 | Loss: 5.0240 | dt: 60.94ms | tok/sec: 16803.67\n",
            "Step 761/1000 | Loss: 4.9520 | dt: 60.09ms | tok/sec: 17042.37\n",
            "Step 762/1000 | Loss: 5.2085 | dt: 61.12ms | tok/sec: 16754.11\n",
            "Step 763/1000 | Loss: 5.2889 | dt: 61.10ms | tok/sec: 16758.49\n",
            "Step 764/1000 | Loss: 5.0253 | dt: 60.88ms | tok/sec: 16818.87\n",
            "Step 765/1000 | Loss: 5.0333 | dt: 60.48ms | tok/sec: 16931.72\n",
            "Step 766/1000 | Loss: 5.2756 | dt: 60.01ms | tok/sec: 17062.48\n",
            "Step 767/1000 | Loss: 4.8996 | dt: 60.81ms | tok/sec: 16838.72\n",
            "Step 768/1000 | Loss: 4.9342 | dt: 59.11ms | tok/sec: 17324.63\n",
            "Step 769/1000 | Loss: 4.7944 | dt: 61.03ms | tok/sec: 16778.79\n",
            "Step 770/1000 | Loss: 5.0052 | dt: 59.53ms | tok/sec: 17202.85\n",
            "Step 771/1000 | Loss: 5.0623 | dt: 59.58ms | tok/sec: 17185.57\n",
            "Step 772/1000 | Loss: 5.2743 | dt: 57.70ms | tok/sec: 17747.51\n",
            "Step 773/1000 | Loss: 5.2493 | dt: 57.83ms | tok/sec: 17706.09\n",
            "Step 774/1000 | Loss: 4.8948 | dt: 59.11ms | tok/sec: 17323.80\n",
            "Step 775/1000 | Loss: 5.1666 | dt: 59.59ms | tok/sec: 17185.51\n",
            "Step 776/1000 | Loss: 4.7877 | dt: 60.11ms | tok/sec: 17034.13\n",
            "Step 777/1000 | Loss: 5.1994 | dt: 60.20ms | tok/sec: 17010.51\n",
            "Step 778/1000 | Loss: 4.7894 | dt: 60.24ms | tok/sec: 16999.27\n",
            "Step 779/1000 | Loss: 5.0018 | dt: 61.21ms | tok/sec: 16730.16\n",
            "Step 780/1000 | Loss: 4.9127 | dt: 61.39ms | tok/sec: 16680.97\n",
            "Step 781/1000 | Loss: 4.8955 | dt: 58.82ms | tok/sec: 17409.18\n",
            "Step 782/1000 | Loss: 4.7246 | dt: 58.99ms | tok/sec: 17358.66\n",
            "Step 783/1000 | Loss: 4.8926 | dt: 60.91ms | tok/sec: 16812.94\n",
            "Step 784/1000 | Loss: 4.5998 | dt: 61.08ms | tok/sec: 16764.51\n",
            "Step 785/1000 | Loss: 4.1726 | dt: 60.34ms | tok/sec: 16971.33\n",
            "Step 786/1000 | Loss: 4.4390 | dt: 60.94ms | tok/sec: 16802.75\n",
            "Step 787/1000 | Loss: 4.6540 | dt: 60.90ms | tok/sec: 16813.93\n",
            "Step 788/1000 | Loss: 5.3092 | dt: 60.44ms | tok/sec: 16943.67\n",
            "Step 789/1000 | Loss: 4.8828 | dt: 60.81ms | tok/sec: 16840.10\n",
            "Step 790/1000 | Loss: 5.0160 | dt: 60.34ms | tok/sec: 16970.59\n",
            "Step 791/1000 | Loss: 5.2696 | dt: 60.98ms | tok/sec: 16793.75\n",
            "Step 792/1000 | Loss: 4.8579 | dt: 60.75ms | tok/sec: 16856.76\n",
            "Step 793/1000 | Loss: 5.3206 | dt: 60.70ms | tok/sec: 16871.00\n",
            "Step 794/1000 | Loss: 5.0616 | dt: 60.75ms | tok/sec: 16857.02\n",
            "Step 795/1000 | Loss: 5.1028 | dt: 60.92ms | tok/sec: 16809.19\n",
            "Step 796/1000 | Loss: 4.9600 | dt: 60.65ms | tok/sec: 16882.47\n",
            "Step 797/1000 | Loss: 5.7011 | dt: 60.50ms | tok/sec: 16925.71\n",
            "Step 798/1000 | Loss: 4.9037 | dt: 60.95ms | tok/sec: 16801.50\n",
            "Step 799/1000 | Loss: 4.7369 | dt: 60.87ms | tok/sec: 16822.36\n",
            "Step 800/1000 | Loss: 5.1907 | dt: 60.48ms | tok/sec: 16932.05\n",
            "Step 801/1000 | Loss: 4.8194 | dt: 60.57ms | tok/sec: 16905.93\n",
            "Step 802/1000 | Loss: 4.6945 | dt: 60.21ms | tok/sec: 17006.07\n",
            "Step 803/1000 | Loss: 5.4668 | dt: 60.30ms | tok/sec: 16981.39\n",
            "Step 804/1000 | Loss: 5.1703 | dt: 60.81ms | tok/sec: 16839.64\n",
            "Step 805/1000 | Loss: 4.6963 | dt: 60.66ms | tok/sec: 16881.87\n",
            "Step 806/1000 | Loss: 4.7480 | dt: 62.00ms | tok/sec: 16515.29\n",
            "Step 807/1000 | Loss: 4.9391 | dt: 59.53ms | tok/sec: 17200.65\n",
            "Step 808/1000 | Loss: 4.9416 | dt: 60.70ms | tok/sec: 16870.53\n",
            "Step 809/1000 | Loss: 4.9825 | dt: 59.94ms | tok/sec: 17085.08\n",
            "Step 810/1000 | Loss: 4.7815 | dt: 60.33ms | tok/sec: 16973.14\n",
            "Step 811/1000 | Loss: 5.3578 | dt: 60.14ms | tok/sec: 17028.18\n",
            "Step 812/1000 | Loss: 5.0416 | dt: 60.57ms | tok/sec: 16904.93\n",
            "Step 813/1000 | Loss: 5.0811 | dt: 60.34ms | tok/sec: 16969.38\n",
            "Step 814/1000 | Loss: 4.8646 | dt: 60.57ms | tok/sec: 16905.99\n",
            "Step 815/1000 | Loss: 4.8896 | dt: 60.64ms | tok/sec: 16886.12\n",
            "Step 816/1000 | Loss: 4.7681 | dt: 59.60ms | tok/sec: 17180.69\n",
            "Step 817/1000 | Loss: 4.4208 | dt: 61.05ms | tok/sec: 16772.56\n",
            "Step 818/1000 | Loss: 4.8402 | dt: 60.66ms | tok/sec: 16881.14\n",
            "Step 819/1000 | Loss: 4.5334 | dt: 60.86ms | tok/sec: 16824.14\n",
            "Step 820/1000 | Loss: 4.9067 | dt: 61.16ms | tok/sec: 16743.40\n",
            "Step 821/1000 | Loss: 4.7097 | dt: 61.03ms | tok/sec: 16778.07\n",
            "Step 822/1000 | Loss: 4.8156 | dt: 60.60ms | tok/sec: 16897.15\n",
            "Step 823/1000 | Loss: 4.7190 | dt: 60.26ms | tok/sec: 16994.43\n",
            "Step 824/1000 | Loss: 5.1779 | dt: 60.48ms | tok/sec: 16932.19\n",
            "Step 825/1000 | Loss: 5.2549 | dt: 60.92ms | tok/sec: 16808.34\n",
            "Step 826/1000 | Loss: 5.0838 | dt: 60.20ms | tok/sec: 17008.83\n",
            "Step 827/1000 | Loss: 5.0316 | dt: 59.79ms | tok/sec: 17125.55\n",
            "Step 828/1000 | Loss: 4.7440 | dt: 60.13ms | tok/sec: 17030.88\n",
            "Step 829/1000 | Loss: 4.9890 | dt: 60.44ms | tok/sec: 16942.27\n",
            "Step 830/1000 | Loss: 4.8937 | dt: 60.11ms | tok/sec: 17036.49\n",
            "Step 831/1000 | Loss: 4.7982 | dt: 60.91ms | tok/sec: 16812.42\n",
            "Step 832/1000 | Loss: 4.6762 | dt: 60.66ms | tok/sec: 16880.28\n",
            "Step 833/1000 | Loss: 4.7239 | dt: 60.85ms | tok/sec: 16829.61\n",
            "Step 834/1000 | Loss: 4.6294 | dt: 60.92ms | tok/sec: 16808.73\n",
            "Step 835/1000 | Loss: 4.7837 | dt: 60.55ms | tok/sec: 16910.78\n",
            "Step 836/1000 | Loss: 5.2484 | dt: 61.91ms | tok/sec: 16541.18\n",
            "Step 837/1000 | Loss: 5.1515 | dt: 60.82ms | tok/sec: 16835.61\n",
            "Step 838/1000 | Loss: 5.0552 | dt: 61.39ms | tok/sec: 16679.87\n",
            "Step 839/1000 | Loss: 5.1930 | dt: 62.72ms | tok/sec: 16325.34\n",
            "Step 840/1000 | Loss: 5.0993 | dt: 59.50ms | tok/sec: 17210.37\n",
            "Step 841/1000 | Loss: 4.9650 | dt: 59.51ms | tok/sec: 17208.02\n",
            "Step 842/1000 | Loss: 4.5652 | dt: 59.49ms | tok/sec: 17212.85\n",
            "Step 843/1000 | Loss: 5.0409 | dt: 59.53ms | tok/sec: 17201.54\n",
            "Step 844/1000 | Loss: 5.2158 | dt: 60.11ms | tok/sec: 17034.94\n",
            "Step 845/1000 | Loss: 4.8788 | dt: 58.30ms | tok/sec: 17565.54\n",
            "Step 846/1000 | Loss: 5.0874 | dt: 60.11ms | tok/sec: 17034.73\n",
            "Step 847/1000 | Loss: 4.9252 | dt: 60.05ms | tok/sec: 17052.45\n",
            "Step 848/1000 | Loss: 4.4441 | dt: 61.27ms | tok/sec: 16714.21\n",
            "Step 849/1000 | Loss: 4.0137 | dt: 59.94ms | tok/sec: 17082.43\n",
            "Step 850/1000 | Loss: 4.6760 | dt: 58.87ms | tok/sec: 17393.18\n",
            "Step 851/1000 | Loss: 5.0614 | dt: 60.05ms | tok/sec: 17052.79\n",
            "Step 852/1000 | Loss: 5.0926 | dt: 61.52ms | tok/sec: 16645.42\n",
            "Step 853/1000 | Loss: 4.9143 | dt: 60.64ms | tok/sec: 16887.18\n",
            "Step 854/1000 | Loss: 4.6097 | dt: 60.61ms | tok/sec: 16894.22\n",
            "Step 855/1000 | Loss: 4.7583 | dt: 59.89ms | tok/sec: 17098.34\n",
            "Step 856/1000 | Loss: 4.8416 | dt: 61.12ms | tok/sec: 16752.94\n",
            "Step 857/1000 | Loss: 4.6298 | dt: 59.38ms | tok/sec: 17245.40\n",
            "Step 858/1000 | Loss: 4.7587 | dt: 59.86ms | tok/sec: 17107.20\n",
            "Step 859/1000 | Loss: 4.7666 | dt: 61.21ms | tok/sec: 16730.29\n",
            "Step 860/1000 | Loss: 4.6193 | dt: 58.99ms | tok/sec: 17358.03\n",
            "Step 861/1000 | Loss: 5.0034 | dt: 60.87ms | tok/sec: 16822.36\n",
            "Step 862/1000 | Loss: 4.5782 | dt: 59.26ms | tok/sec: 17281.14\n",
            "Step 863/1000 | Loss: 4.7558 | dt: 59.65ms | tok/sec: 17166.96\n",
            "Step 864/1000 | Loss: 5.0845 | dt: 58.40ms | tok/sec: 17534.34\n",
            "Step 865/1000 | Loss: 4.6480 | dt: 61.20ms | tok/sec: 16731.92\n",
            "Step 866/1000 | Loss: 4.1637 | dt: 60.24ms | tok/sec: 16999.61\n",
            "Step 867/1000 | Loss: 4.7634 | dt: 60.38ms | tok/sec: 16959.26\n",
            "Step 868/1000 | Loss: 5.0966 | dt: 60.34ms | tok/sec: 16969.58\n",
            "Step 869/1000 | Loss: 5.3813 | dt: 59.82ms | tok/sec: 17118.24\n",
            "Step 870/1000 | Loss: 5.2193 | dt: 60.40ms | tok/sec: 16952.57\n",
            "Step 871/1000 | Loss: 5.2954 | dt: 60.22ms | tok/sec: 17004.52\n",
            "Step 872/1000 | Loss: 5.2399 | dt: 59.54ms | tok/sec: 17199.82\n",
            "Step 873/1000 | Loss: 5.4242 | dt: 59.05ms | tok/sec: 17342.61\n",
            "Step 874/1000 | Loss: 5.0002 | dt: 60.86ms | tok/sec: 16826.78\n",
            "Step 875/1000 | Loss: 5.0457 | dt: 60.62ms | tok/sec: 16892.83\n",
            "Step 876/1000 | Loss: 5.1668 | dt: 60.30ms | tok/sec: 16980.92\n",
            "Step 877/1000 | Loss: 5.0598 | dt: 60.45ms | tok/sec: 16939.33\n",
            "Step 878/1000 | Loss: 5.0542 | dt: 59.32ms | tok/sec: 17263.49\n",
            "Step 879/1000 | Loss: 4.7889 | dt: 60.28ms | tok/sec: 16987.64\n",
            "Step 880/1000 | Loss: 4.8584 | dt: 60.84ms | tok/sec: 16829.68\n",
            "Step 881/1000 | Loss: 4.9515 | dt: 60.44ms | tok/sec: 16943.67\n",
            "Step 882/1000 | Loss: 5.2364 | dt: 61.10ms | tok/sec: 16758.56\n",
            "Step 883/1000 | Loss: 4.9504 | dt: 60.54ms | tok/sec: 16914.71\n",
            "Step 884/1000 | Loss: 5.3009 | dt: 59.02ms | tok/sec: 17349.83\n",
            "Step 885/1000 | Loss: 5.2453 | dt: 61.18ms | tok/sec: 16736.49\n",
            "Step 886/1000 | Loss: 5.4514 | dt: 61.49ms | tok/sec: 16651.81\n",
            "Step 887/1000 | Loss: 5.4815 | dt: 60.87ms | tok/sec: 16821.90\n",
            "Step 888/1000 | Loss: 5.4720 | dt: 60.35ms | tok/sec: 16966.90\n",
            "Step 889/1000 | Loss: 5.0329 | dt: 60.19ms | tok/sec: 17012.33\n",
            "Step 890/1000 | Loss: 5.2137 | dt: 60.52ms | tok/sec: 16921.18\n",
            "Step 891/1000 | Loss: 5.2084 | dt: 58.86ms | tok/sec: 17398.60\n",
            "Step 892/1000 | Loss: 5.5198 | dt: 61.38ms | tok/sec: 16682.01\n",
            "Step 893/1000 | Loss: 4.9674 | dt: 61.18ms | tok/sec: 16736.68\n",
            "Step 894/1000 | Loss: 4.9930 | dt: 59.64ms | tok/sec: 17169.91\n",
            "Step 895/1000 | Loss: 4.9481 | dt: 60.65ms | tok/sec: 16883.40\n",
            "Step 896/1000 | Loss: 4.8738 | dt: 60.80ms | tok/sec: 16841.69\n",
            "Step 897/1000 | Loss: 5.1909 | dt: 60.89ms | tok/sec: 16818.28\n",
            "Step 898/1000 | Loss: 4.8104 | dt: 60.33ms | tok/sec: 16974.08\n",
            "Step 899/1000 | Loss: 4.8898 | dt: 61.62ms | tok/sec: 16617.85\n",
            "Step 900/1000 | Loss: 5.1964 | dt: 60.59ms | tok/sec: 16900.60\n",
            "Step 901/1000 | Loss: 4.8646 | dt: 60.78ms | tok/sec: 16848.82\n",
            "Step 902/1000 | Loss: 4.9791 | dt: 60.86ms | tok/sec: 16824.54\n",
            "Step 903/1000 | Loss: 4.8917 | dt: 60.61ms | tok/sec: 16894.55\n",
            "Step 904/1000 | Loss: 5.5307 | dt: 61.06ms | tok/sec: 16769.42\n",
            "Step 905/1000 | Loss: 5.2845 | dt: 60.44ms | tok/sec: 16943.41\n",
            "Step 906/1000 | Loss: 4.7668 | dt: 61.04ms | tok/sec: 16774.86\n",
            "Step 907/1000 | Loss: 4.7674 | dt: 60.71ms | tok/sec: 16868.21\n",
            "Step 908/1000 | Loss: 5.0761 | dt: 60.86ms | tok/sec: 16824.14\n",
            "Step 909/1000 | Loss: 4.8409 | dt: 59.90ms | tok/sec: 17095.69\n",
            "Step 910/1000 | Loss: 4.9745 | dt: 59.83ms | tok/sec: 17116.27\n",
            "Step 911/1000 | Loss: 5.1346 | dt: 60.49ms | tok/sec: 16929.78\n",
            "Step 912/1000 | Loss: 5.0631 | dt: 60.62ms | tok/sec: 16891.96\n",
            "Step 913/1000 | Loss: 5.0648 | dt: 60.23ms | tok/sec: 17002.70\n",
            "Step 914/1000 | Loss: 4.9236 | dt: 60.48ms | tok/sec: 16930.38\n",
            "Step 915/1000 | Loss: 4.6657 | dt: 60.98ms | tok/sec: 16793.55\n",
            "Step 916/1000 | Loss: 4.9593 | dt: 60.76ms | tok/sec: 16853.72\n",
            "Step 917/1000 | Loss: 4.6735 | dt: 60.63ms | tok/sec: 16890.57\n",
            "Step 918/1000 | Loss: 4.5520 | dt: 60.71ms | tok/sec: 16866.95\n",
            "Step 919/1000 | Loss: 4.9686 | dt: 60.88ms | tok/sec: 16819.86\n",
            "Step 920/1000 | Loss: 4.6647 | dt: 60.53ms | tok/sec: 16917.91\n",
            "Step 921/1000 | Loss: 4.7662 | dt: 60.59ms | tok/sec: 16900.54\n",
            "Step 922/1000 | Loss: 4.8990 | dt: 60.77ms | tok/sec: 16851.80\n",
            "Step 923/1000 | Loss: 4.8718 | dt: 60.64ms | tok/sec: 16885.92\n",
            "Step 924/1000 | Loss: 4.9899 | dt: 60.60ms | tok/sec: 16897.61\n",
            "Step 925/1000 | Loss: 5.0508 | dt: 60.50ms | tok/sec: 16926.38\n",
            "Step 926/1000 | Loss: 5.3026 | dt: 60.67ms | tok/sec: 16878.42\n",
            "Step 927/1000 | Loss: 4.7621 | dt: 60.32ms | tok/sec: 16974.88\n",
            "Step 928/1000 | Loss: 4.6306 | dt: 60.37ms | tok/sec: 16961.27\n",
            "Step 929/1000 | Loss: 5.0086 | dt: 58.79ms | tok/sec: 17418.71\n",
            "Step 930/1000 | Loss: 5.0203 | dt: 60.66ms | tok/sec: 16881.74\n",
            "Step 931/1000 | Loss: 4.7161 | dt: 59.73ms | tok/sec: 17144.21\n",
            "Step 932/1000 | Loss: 4.5898 | dt: 61.54ms | tok/sec: 16640.26\n",
            "Step 933/1000 | Loss: 4.8351 | dt: 61.06ms | tok/sec: 16770.01\n",
            "Step 934/1000 | Loss: 5.0947 | dt: 61.31ms | tok/sec: 16701.86\n",
            "Step 935/1000 | Loss: 4.5119 | dt: 61.19ms | tok/sec: 16734.14\n",
            "Step 936/1000 | Loss: 4.8159 | dt: 60.20ms | tok/sec: 17011.19\n",
            "Step 937/1000 | Loss: 4.6704 | dt: 59.47ms | tok/sec: 17219.20\n",
            "Step 938/1000 | Loss: 4.5222 | dt: 61.28ms | tok/sec: 16710.76\n",
            "Step 939/1000 | Loss: 4.4786 | dt: 60.49ms | tok/sec: 16929.12\n",
            "Step 940/1000 | Loss: 4.6381 | dt: 61.04ms | tok/sec: 16775.18\n",
            "Step 941/1000 | Loss: 4.2760 | dt: 61.61ms | tok/sec: 16619.59\n",
            "Step 942/1000 | Loss: 4.6744 | dt: 60.28ms | tok/sec: 16986.29\n",
            "Step 943/1000 | Loss: 4.2848 | dt: 60.46ms | tok/sec: 16936.99\n",
            "Step 944/1000 | Loss: 4.8886 | dt: 60.51ms | tok/sec: 16922.51\n",
            "Step 945/1000 | Loss: 5.0799 | dt: 59.83ms | tok/sec: 17114.63\n",
            "Step 946/1000 | Loss: 5.5702 | dt: 60.70ms | tok/sec: 16871.06\n",
            "Step 947/1000 | Loss: 4.8629 | dt: 60.40ms | tok/sec: 16953.57\n",
            "Step 948/1000 | Loss: 5.1525 | dt: 60.79ms | tok/sec: 16844.73\n",
            "Step 949/1000 | Loss: 5.0788 | dt: 60.81ms | tok/sec: 16838.91\n",
            "Step 950/1000 | Loss: 4.8974 | dt: 60.82ms | tok/sec: 16837.79\n",
            "Step 951/1000 | Loss: 4.8047 | dt: 60.52ms | tok/sec: 16920.51\n",
            "Step 952/1000 | Loss: 5.1903 | dt: 60.49ms | tok/sec: 16929.38\n",
            "Step 953/1000 | Loss: 4.9198 | dt: 60.86ms | tok/sec: 16824.27\n",
            "Step 954/1000 | Loss: 4.4850 | dt: 60.77ms | tok/sec: 16851.34\n",
            "Step 955/1000 | Loss: 4.7455 | dt: 60.89ms | tok/sec: 16816.76\n",
            "Step 956/1000 | Loss: 4.7523 | dt: 58.74ms | tok/sec: 17432.57\n",
            "Step 957/1000 | Loss: 4.9004 | dt: 60.46ms | tok/sec: 16936.59\n",
            "Step 958/1000 | Loss: 4.1375 | dt: 62.60ms | tok/sec: 16356.98\n",
            "Step 959/1000 | Loss: 4.7412 | dt: 60.38ms | tok/sec: 16959.93\n",
            "Step 960/1000 | Loss: 4.9998 | dt: 60.32ms | tok/sec: 16975.95\n",
            "Step 961/1000 | Loss: 5.1722 | dt: 60.23ms | tok/sec: 17002.36\n",
            "Step 962/1000 | Loss: 4.8315 | dt: 58.21ms | tok/sec: 17591.29\n",
            "Step 963/1000 | Loss: 4.9073 | dt: 61.20ms | tok/sec: 16731.60\n",
            "Step 964/1000 | Loss: 4.8601 | dt: 60.40ms | tok/sec: 16953.51\n",
            "Step 965/1000 | Loss: 4.5298 | dt: 61.02ms | tok/sec: 16780.03\n",
            "Step 966/1000 | Loss: 5.1983 | dt: 61.07ms | tok/sec: 16766.34\n",
            "Step 967/1000 | Loss: 4.9133 | dt: 61.08ms | tok/sec: 16764.90\n",
            "Step 968/1000 | Loss: 4.7638 | dt: 58.70ms | tok/sec: 17443.40\n",
            "Step 969/1000 | Loss: 4.5273 | dt: 61.18ms | tok/sec: 16738.31\n",
            "Step 970/1000 | Loss: 4.4789 | dt: 61.17ms | tok/sec: 16740.60\n",
            "Step 971/1000 | Loss: 4.5619 | dt: 60.58ms | tok/sec: 16902.27\n",
            "Step 972/1000 | Loss: 4.4669 | dt: 61.74ms | tok/sec: 16586.79\n",
            "Step 973/1000 | Loss: 4.5881 | dt: 60.43ms | tok/sec: 16944.61\n",
            "Step 974/1000 | Loss: 4.5744 | dt: 60.70ms | tok/sec: 16870.80\n",
            "Step 975/1000 | Loss: 4.5040 | dt: 61.15ms | tok/sec: 16746.41\n",
            "Step 976/1000 | Loss: 4.4400 | dt: 60.12ms | tok/sec: 17031.36\n",
            "Step 977/1000 | Loss: 4.3200 | dt: 60.62ms | tok/sec: 16892.69\n",
            "Step 978/1000 | Loss: 4.0753 | dt: 59.30ms | tok/sec: 17266.76\n",
            "Step 979/1000 | Loss: 4.7379 | dt: 60.87ms | tok/sec: 16821.57\n",
            "Step 980/1000 | Loss: 5.1375 | dt: 60.71ms | tok/sec: 16866.75\n",
            "Step 981/1000 | Loss: 5.2387 | dt: 60.69ms | tok/sec: 16871.66\n",
            "Step 982/1000 | Loss: 5.4814 | dt: 61.24ms | tok/sec: 16720.72\n",
            "Step 983/1000 | Loss: 5.5329 | dt: 60.44ms | tok/sec: 16943.61\n",
            "Step 984/1000 | Loss: 5.2667 | dt: 60.81ms | tok/sec: 16840.24\n",
            "Step 985/1000 | Loss: 5.4425 | dt: 60.75ms | tok/sec: 16856.23\n",
            "Step 986/1000 | Loss: 5.3868 | dt: 60.57ms | tok/sec: 16906.39\n",
            "Step 987/1000 | Loss: 5.0444 | dt: 60.60ms | tok/sec: 16897.35\n",
            "Step 988/1000 | Loss: 4.8721 | dt: 60.85ms | tok/sec: 16828.42\n",
            "Step 989/1000 | Loss: 5.0896 | dt: 58.63ms | tok/sec: 17465.82\n",
            "Step 990/1000 | Loss: 5.0052 | dt: 61.65ms | tok/sec: 16609.24\n",
            "Step 991/1000 | Loss: 5.3193 | dt: 59.71ms | tok/sec: 17149.00\n",
            "Step 992/1000 | Loss: 5.0325 | dt: 60.73ms | tok/sec: 16862.45\n",
            "Step 993/1000 | Loss: 5.3589 | dt: 61.30ms | tok/sec: 16705.69\n",
            "Step 994/1000 | Loss: 5.0916 | dt: 60.77ms | tok/sec: 16851.00\n",
            "Step 995/1000 | Loss: 5.3442 | dt: 60.48ms | tok/sec: 16930.25\n",
            "Step 996/1000 | Loss: 4.9213 | dt: 60.36ms | tok/sec: 16965.83\n",
            "Step 997/1000 | Loss: 5.1472 | dt: 59.22ms | tok/sec: 17292.20\n",
            "Step 998/1000 | Loss: 4.9414 | dt: 61.28ms | tok/sec: 16709.46\n",
            "Step 999/1000 | Loss: 5.1237 | dt: 58.93ms | tok/sec: 17376.43\n",
            "Step 1000/1000 | Loss: 4.8948 | dt: 61.28ms | tok/sec: 16711.28\n",
            "Epoch 2/20\n",
            "Step 1/1000 | Loss: 4.8350 | dt: 60.19ms | tok/sec: 17012.87\n",
            "Step 2/1000 | Loss: 5.5797 | dt: 60.93ms | tok/sec: 16807.29\n",
            "Step 3/1000 | Loss: 4.8141 | dt: 60.87ms | tok/sec: 16821.64\n",
            "Step 4/1000 | Loss: 5.0459 | dt: 60.60ms | tok/sec: 16897.68\n",
            "Step 5/1000 | Loss: 5.0495 | dt: 60.80ms | tok/sec: 16841.36\n",
            "Step 6/1000 | Loss: 5.0089 | dt: 60.69ms | tok/sec: 16871.79\n",
            "Step 7/1000 | Loss: 4.8396 | dt: 60.52ms | tok/sec: 16920.38\n",
            "Step 8/1000 | Loss: 4.6152 | dt: 60.62ms | tok/sec: 16893.49\n",
            "Step 9/1000 | Loss: 4.5073 | dt: 60.83ms | tok/sec: 16834.49\n",
            "Step 10/1000 | Loss: 4.7710 | dt: 60.71ms | tok/sec: 16865.96\n",
            "Step 11/1000 | Loss: 4.1711 | dt: 60.95ms | tok/sec: 16800.71\n",
            "Step 12/1000 | Loss: 4.9725 | dt: 61.99ms | tok/sec: 16518.60\n",
            "Step 13/1000 | Loss: 4.2297 | dt: 60.46ms | tok/sec: 16936.19\n",
            "Step 14/1000 | Loss: 4.0818 | dt: 60.34ms | tok/sec: 16970.52\n",
            "Step 15/1000 | Loss: 4.5903 | dt: 60.55ms | tok/sec: 16912.18\n",
            "Step 16/1000 | Loss: 4.5165 | dt: 60.34ms | tok/sec: 16969.52\n",
            "Step 17/1000 | Loss: 4.5964 | dt: 60.58ms | tok/sec: 16902.20\n",
            "Step 18/1000 | Loss: 4.2941 | dt: 60.76ms | tok/sec: 16854.18\n",
            "Step 19/1000 | Loss: 4.5028 | dt: 60.32ms | tok/sec: 16976.36\n",
            "Step 20/1000 | Loss: 4.4824 | dt: 60.58ms | tok/sec: 16904.46\n",
            "Step 21/1000 | Loss: 4.7064 | dt: 60.82ms | tok/sec: 16836.21\n",
            "Step 22/1000 | Loss: 4.4733 | dt: 61.59ms | tok/sec: 16625.64\n",
            "Step 23/1000 | Loss: 4.8334 | dt: 60.41ms | tok/sec: 16951.63\n",
            "Step 24/1000 | Loss: 4.8656 | dt: 60.53ms | tok/sec: 16916.91\n",
            "Step 25/1000 | Loss: 4.8166 | dt: 60.46ms | tok/sec: 16937.86\n",
            "Step 26/1000 | Loss: 4.3504 | dt: 60.55ms | tok/sec: 16910.32\n",
            "Step 27/1000 | Loss: 4.4494 | dt: 60.59ms | tok/sec: 16899.54\n",
            "Step 28/1000 | Loss: 4.8346 | dt: 60.43ms | tok/sec: 16944.41\n",
            "Step 29/1000 | Loss: 4.3011 | dt: 60.72ms | tok/sec: 16863.31\n",
            "Step 30/1000 | Loss: 5.0542 | dt: 60.45ms | tok/sec: 16938.86\n",
            "Step 31/1000 | Loss: 4.9374 | dt: 60.43ms | tok/sec: 16944.94\n",
            "Step 32/1000 | Loss: 4.9440 | dt: 60.55ms | tok/sec: 16911.65\n",
            "Step 33/1000 | Loss: 4.8160 | dt: 60.87ms | tok/sec: 16823.88\n",
            "Step 34/1000 | Loss: 4.9154 | dt: 62.74ms | tok/sec: 16320.68\n",
            "Step 35/1000 | Loss: 4.8544 | dt: 61.04ms | tok/sec: 16775.32\n",
            "Step 36/1000 | Loss: 4.7143 | dt: 61.20ms | tok/sec: 16731.53\n",
            "Step 37/1000 | Loss: 5.4238 | dt: 59.79ms | tok/sec: 17126.30\n",
            "Step 38/1000 | Loss: 4.9330 | dt: 58.70ms | tok/sec: 17443.90\n",
            "Step 39/1000 | Loss: 5.3063 | dt: 60.92ms | tok/sec: 16809.85\n",
            "Step 40/1000 | Loss: 4.5864 | dt: 60.47ms | tok/sec: 16934.92\n",
            "Step 41/1000 | Loss: 4.4764 | dt: 58.85ms | tok/sec: 17400.65\n",
            "Step 42/1000 | Loss: 4.8628 | dt: 61.82ms | tok/sec: 16564.66\n",
            "Step 43/1000 | Loss: 4.9281 | dt: 60.39ms | tok/sec: 16957.32\n",
            "Step 44/1000 | Loss: 4.5594 | dt: 59.71ms | tok/sec: 17149.34\n",
            "Step 45/1000 | Loss: 4.7978 | dt: 60.08ms | tok/sec: 17044.87\n",
            "Step 46/1000 | Loss: 4.6336 | dt: 60.06ms | tok/sec: 17051.03\n",
            "Step 47/1000 | Loss: 4.7130 | dt: 60.02ms | tok/sec: 17059.70\n",
            "Step 48/1000 | Loss: 5.0828 | dt: 60.11ms | tok/sec: 17036.15\n",
            "Step 49/1000 | Loss: 4.7234 | dt: 60.30ms | tok/sec: 16980.38\n",
            "Step 50/1000 | Loss: 4.5781 | dt: 60.37ms | tok/sec: 16962.48\n",
            "Step 51/1000 | Loss: 4.6816 | dt: 61.13ms | tok/sec: 16751.24\n",
            "Step 52/1000 | Loss: 4.7404 | dt: 60.84ms | tok/sec: 16832.25\n",
            "Step 53/1000 | Loss: 4.7092 | dt: 60.29ms | tok/sec: 16984.14\n",
            "Step 54/1000 | Loss: 4.8445 | dt: 60.98ms | tok/sec: 16793.42\n",
            "Step 55/1000 | Loss: 4.7305 | dt: 60.86ms | tok/sec: 16826.64\n",
            "Step 56/1000 | Loss: 4.4560 | dt: 60.25ms | tok/sec: 16996.31\n",
            "Step 57/1000 | Loss: 4.7275 | dt: 59.63ms | tok/sec: 17172.04\n",
            "Step 58/1000 | Loss: 4.0642 | dt: 60.42ms | tok/sec: 16948.82\n",
            "Step 59/1000 | Loss: 4.4134 | dt: 60.04ms | tok/sec: 17056.38\n",
            "Step 60/1000 | Loss: 4.6630 | dt: 60.47ms | tok/sec: 16933.32\n",
            "Step 61/1000 | Loss: 4.5101 | dt: 58.77ms | tok/sec: 17425.00\n",
            "Step 62/1000 | Loss: 4.4043 | dt: 61.21ms | tok/sec: 16730.62\n",
            "Step 63/1000 | Loss: 4.7162 | dt: 60.68ms | tok/sec: 16875.90\n",
            "Step 64/1000 | Loss: 4.9708 | dt: 62.08ms | tok/sec: 16495.00\n",
            "Step 65/1000 | Loss: 4.7543 | dt: 60.05ms | tok/sec: 17051.10\n",
            "Step 66/1000 | Loss: 4.9170 | dt: 59.28ms | tok/sec: 17275.16\n",
            "Step 67/1000 | Loss: 4.5317 | dt: 59.59ms | tok/sec: 17183.31\n",
            "Step 68/1000 | Loss: 4.6212 | dt: 60.09ms | tok/sec: 17040.55\n",
            "Step 69/1000 | Loss: 4.7115 | dt: 60.15ms | tok/sec: 17024.54\n",
            "Step 70/1000 | Loss: 4.5094 | dt: 60.63ms | tok/sec: 16889.90\n",
            "Step 71/1000 | Loss: 5.2360 | dt: 60.56ms | tok/sec: 16910.19\n",
            "Step 72/1000 | Loss: 4.9076 | dt: 60.44ms | tok/sec: 16942.94\n",
            "Step 73/1000 | Loss: 4.6632 | dt: 61.12ms | tok/sec: 16754.77\n",
            "Step 74/1000 | Loss: 4.5813 | dt: 60.47ms | tok/sec: 16935.39\n",
            "Step 75/1000 | Loss: 4.2341 | dt: 60.62ms | tok/sec: 16891.23\n",
            "Step 76/1000 | Loss: 4.5224 | dt: 60.89ms | tok/sec: 16817.68\n",
            "Step 77/1000 | Loss: 4.4289 | dt: 59.39ms | tok/sec: 17243.19\n",
            "Step 78/1000 | Loss: 4.4550 | dt: 60.55ms | tok/sec: 16911.98\n",
            "Step 79/1000 | Loss: 5.1289 | dt: 60.53ms | tok/sec: 16916.31\n",
            "Step 80/1000 | Loss: 5.0549 | dt: 60.87ms | tok/sec: 16823.74\n",
            "Step 81/1000 | Loss: 4.8247 | dt: 58.97ms | tok/sec: 17364.35\n",
            "Step 82/1000 | Loss: 5.0267 | dt: 61.03ms | tok/sec: 16779.90\n",
            "Step 83/1000 | Loss: 4.7389 | dt: 60.80ms | tok/sec: 16843.08\n",
            "Step 84/1000 | Loss: 5.1907 | dt: 60.49ms | tok/sec: 16928.98\n",
            "Step 85/1000 | Loss: 5.2786 | dt: 60.75ms | tok/sec: 16856.82\n",
            "Step 86/1000 | Loss: 5.2547 | dt: 60.91ms | tok/sec: 16812.94\n",
            "Step 87/1000 | Loss: 5.2904 | dt: 60.94ms | tok/sec: 16804.00\n",
            "Step 88/1000 | Loss: 5.0688 | dt: 60.51ms | tok/sec: 16921.58\n",
            "Step 89/1000 | Loss: 5.3159 | dt: 58.68ms | tok/sec: 17451.63\n",
            "Step 90/1000 | Loss: 4.9782 | dt: 61.35ms | tok/sec: 16690.11\n",
            "Step 91/1000 | Loss: 4.8913 | dt: 60.74ms | tok/sec: 16857.95\n",
            "Step 92/1000 | Loss: 5.1687 | dt: 60.74ms | tok/sec: 16858.68\n",
            "Step 93/1000 | Loss: 5.2322 | dt: 60.81ms | tok/sec: 16837.99\n",
            "Step 94/1000 | Loss: 4.9983 | dt: 61.64ms | tok/sec: 16613.16\n",
            "Step 95/1000 | Loss: 5.0253 | dt: 60.52ms | tok/sec: 16920.18\n",
            "Step 96/1000 | Loss: 5.2844 | dt: 59.33ms | tok/sec: 17260.79\n",
            "Step 97/1000 | Loss: 4.8897 | dt: 61.69ms | tok/sec: 16599.10\n",
            "Step 98/1000 | Loss: 4.8981 | dt: 61.22ms | tok/sec: 16726.84\n",
            "Step 99/1000 | Loss: 4.7109 | dt: 61.11ms | tok/sec: 16755.88\n",
            "Step 100/1000 | Loss: 4.9901 | dt: 61.02ms | tok/sec: 16780.95\n",
            "Step 101/1000 | Loss: 5.0512 | dt: 60.70ms | tok/sec: 16870.80\n",
            "Step 102/1000 | Loss: 5.2638 | dt: 60.86ms | tok/sec: 16825.99\n",
            "Step 103/1000 | Loss: 5.2244 | dt: 60.33ms | tok/sec: 16973.20\n",
            "Step 104/1000 | Loss: 4.7912 | dt: 60.80ms | tok/sec: 16842.81\n",
            "Step 105/1000 | Loss: 5.1260 | dt: 60.32ms | tok/sec: 16974.88\n",
            "Step 106/1000 | Loss: 4.6886 | dt: 60.82ms | tok/sec: 16837.73\n",
            "Step 107/1000 | Loss: 5.1966 | dt: 61.19ms | tok/sec: 16734.86\n",
            "Step 108/1000 | Loss: 4.6799 | dt: 60.01ms | tok/sec: 17064.78\n",
            "Step 109/1000 | Loss: 4.9449 | dt: 60.65ms | tok/sec: 16883.93\n",
            "Step 110/1000 | Loss: 4.8270 | dt: 61.05ms | tok/sec: 16773.48\n",
            "Step 111/1000 | Loss: 4.8096 | dt: 60.33ms | tok/sec: 16973.61\n",
            "Step 112/1000 | Loss: 4.5756 | dt: 60.80ms | tok/sec: 16841.75\n",
            "Step 113/1000 | Loss: 4.8366 | dt: 60.92ms | tok/sec: 16809.78\n",
            "Step 114/1000 | Loss: 4.4693 | dt: 61.09ms | tok/sec: 16763.20\n",
            "Step 115/1000 | Loss: 4.0669 | dt: 58.98ms | tok/sec: 17362.66\n",
            "Step 116/1000 | Loss: 4.2420 | dt: 60.20ms | tok/sec: 17009.03\n",
            "Step 117/1000 | Loss: 4.5254 | dt: 60.67ms | tok/sec: 16878.62\n",
            "Step 118/1000 | Loss: 5.3444 | dt: 60.49ms | tok/sec: 16927.78\n",
            "Step 119/1000 | Loss: 4.8455 | dt: 60.55ms | tok/sec: 16911.18\n",
            "Step 120/1000 | Loss: 5.0785 | dt: 60.54ms | tok/sec: 16915.11\n",
            "Step 121/1000 | Loss: 5.3107 | dt: 60.71ms | tok/sec: 16867.28\n",
            "Step 122/1000 | Loss: 4.9245 | dt: 60.80ms | tok/sec: 16841.09\n",
            "Step 123/1000 | Loss: 5.4165 | dt: 60.50ms | tok/sec: 16925.85\n",
            "Step 124/1000 | Loss: 5.0783 | dt: 60.48ms | tok/sec: 16930.05\n",
            "Step 125/1000 | Loss: 5.0976 | dt: 60.45ms | tok/sec: 16939.20\n",
            "Step 126/1000 | Loss: 4.9447 | dt: 61.24ms | tok/sec: 16720.26\n",
            "Step 127/1000 | Loss: 5.6702 | dt: 61.02ms | tok/sec: 16782.46\n",
            "Step 128/1000 | Loss: 4.8627 | dt: 60.74ms | tok/sec: 16859.87\n",
            "Step 129/1000 | Loss: 4.7286 | dt: 60.86ms | tok/sec: 16824.60\n",
            "Step 130/1000 | Loss: 5.1365 | dt: 60.75ms | tok/sec: 16854.64\n",
            "Step 131/1000 | Loss: 4.7988 | dt: 61.33ms | tok/sec: 16695.95\n",
            "Step 132/1000 | Loss: 4.5767 | dt: 61.49ms | tok/sec: 16653.61\n",
            "Step 133/1000 | Loss: 5.4589 | dt: 61.45ms | tok/sec: 16662.85\n",
            "Step 134/1000 | Loss: 5.1081 | dt: 59.16ms | tok/sec: 17310.11\n",
            "Step 135/1000 | Loss: 4.5786 | dt: 61.64ms | tok/sec: 16611.43\n",
            "Step 136/1000 | Loss: 4.6772 | dt: 58.59ms | tok/sec: 17477.40\n",
            "Step 137/1000 | Loss: 4.8886 | dt: 61.25ms | tok/sec: 16717.59\n",
            "Step 138/1000 | Loss: 4.8842 | dt: 61.28ms | tok/sec: 16710.89\n",
            "Step 139/1000 | Loss: 4.9022 | dt: 61.28ms | tok/sec: 16711.35\n",
            "Step 140/1000 | Loss: 4.6672 | dt: 61.33ms | tok/sec: 16697.12\n",
            "Step 141/1000 | Loss: 5.3219 | dt: 60.69ms | tok/sec: 16872.92\n",
            "Step 142/1000 | Loss: 4.9763 | dt: 59.35ms | tok/sec: 17254.89\n",
            "Step 143/1000 | Loss: 4.9703 | dt: 61.71ms | tok/sec: 16593.33\n",
            "Step 144/1000 | Loss: 4.7225 | dt: 60.90ms | tok/sec: 16813.21\n",
            "Step 145/1000 | Loss: 4.8241 | dt: 59.65ms | tok/sec: 17166.20\n",
            "Step 146/1000 | Loss: 4.6843 | dt: 61.05ms | tok/sec: 16774.14\n",
            "Step 147/1000 | Loss: 4.2514 | dt: 60.32ms | tok/sec: 16975.42\n",
            "Step 148/1000 | Loss: 4.8006 | dt: 61.54ms | tok/sec: 16640.26\n",
            "Step 149/1000 | Loss: 4.4128 | dt: 61.10ms | tok/sec: 16760.06\n",
            "Step 150/1000 | Loss: 4.8566 | dt: 60.75ms | tok/sec: 16855.30\n",
            "Step 151/1000 | Loss: 4.6171 | dt: 60.62ms | tok/sec: 16892.03\n",
            "Step 152/1000 | Loss: 4.7527 | dt: 60.44ms | tok/sec: 16942.67\n",
            "Step 153/1000 | Loss: 4.5849 | dt: 61.40ms | tok/sec: 16678.77\n",
            "Step 154/1000 | Loss: 5.1650 | dt: 60.53ms | tok/sec: 16918.45\n",
            "Step 155/1000 | Loss: 5.2457 | dt: 61.35ms | tok/sec: 16690.63\n",
            "Step 156/1000 | Loss: 5.0263 | dt: 60.35ms | tok/sec: 16969.05\n",
            "Step 157/1000 | Loss: 4.9883 | dt: 60.47ms | tok/sec: 16935.19\n",
            "Step 158/1000 | Loss: 4.6150 | dt: 60.68ms | tok/sec: 16876.36\n",
            "Step 159/1000 | Loss: 4.9004 | dt: 60.53ms | tok/sec: 16916.18\n",
            "Step 160/1000 | Loss: 4.8952 | dt: 60.80ms | tok/sec: 16842.41\n",
            "Step 161/1000 | Loss: 4.9780 | dt: 61.36ms | tok/sec: 16687.78\n",
            "Step 162/1000 | Loss: 4.8315 | dt: 60.53ms | tok/sec: 16917.51\n",
            "Step 163/1000 | Loss: 4.8046 | dt: 60.45ms | tok/sec: 16940.87\n",
            "Step 164/1000 | Loss: 4.6377 | dt: 60.31ms | tok/sec: 16978.24\n",
            "Step 165/1000 | Loss: 4.7920 | dt: 61.05ms | tok/sec: 16773.55\n",
            "Step 166/1000 | Loss: 5.2435 | dt: 60.21ms | tok/sec: 17007.82\n",
            "Step 167/1000 | Loss: 5.1328 | dt: 61.09ms | tok/sec: 16762.88\n",
            "Step 168/1000 | Loss: 5.0116 | dt: 60.21ms | tok/sec: 17007.21\n",
            "Step 169/1000 | Loss: 5.1664 | dt: 60.75ms | tok/sec: 16856.36\n",
            "Step 170/1000 | Loss: 5.0491 | dt: 58.79ms | tok/sec: 17416.74\n",
            "Step 171/1000 | Loss: 4.9189 | dt: 60.53ms | tok/sec: 16916.18\n",
            "Step 172/1000 | Loss: 4.4550 | dt: 60.34ms | tok/sec: 16971.53\n",
            "Step 173/1000 | Loss: 4.9995 | dt: 61.16ms | tok/sec: 16742.62\n",
            "Step 174/1000 | Loss: 5.2393 | dt: 61.06ms | tok/sec: 16769.36\n",
            "Step 175/1000 | Loss: 4.8239 | dt: 61.21ms | tok/sec: 16729.45\n",
            "Step 176/1000 | Loss: 4.9870 | dt: 59.71ms | tok/sec: 17150.37\n",
            "Step 177/1000 | Loss: 4.9153 | dt: 60.78ms | tok/sec: 16846.91\n",
            "Step 178/1000 | Loss: 4.4331 | dt: 62.13ms | tok/sec: 16480.31\n",
            "Step 179/1000 | Loss: 4.1010 | dt: 60.71ms | tok/sec: 16867.75\n",
            "Step 180/1000 | Loss: 4.7155 | dt: 60.79ms | tok/sec: 16846.11\n",
            "Step 181/1000 | Loss: 5.0521 | dt: 60.91ms | tok/sec: 16810.97\n",
            "Step 182/1000 | Loss: 5.0746 | dt: 61.23ms | tok/sec: 16722.54\n",
            "Step 183/1000 | Loss: 4.8659 | dt: 60.80ms | tok/sec: 16841.36\n",
            "Step 184/1000 | Loss: 4.5437 | dt: 60.51ms | tok/sec: 16923.05\n",
            "Step 185/1000 | Loss: 4.7513 | dt: 60.57ms | tok/sec: 16905.66\n",
            "Step 186/1000 | Loss: 4.8309 | dt: 60.30ms | tok/sec: 16981.93\n",
            "Step 187/1000 | Loss: 4.5600 | dt: 60.92ms | tok/sec: 16810.11\n",
            "Step 188/1000 | Loss: 4.6949 | dt: 60.22ms | tok/sec: 17003.10\n",
            "Step 189/1000 | Loss: 4.6696 | dt: 60.81ms | tok/sec: 16840.30\n",
            "Step 190/1000 | Loss: 4.5134 | dt: 60.62ms | tok/sec: 16891.96\n",
            "Step 191/1000 | Loss: 4.9345 | dt: 60.79ms | tok/sec: 16845.72\n",
            "Step 192/1000 | Loss: 4.4347 | dt: 59.24ms | tok/sec: 17284.96\n",
            "Step 193/1000 | Loss: 4.6744 | dt: 61.38ms | tok/sec: 16682.27\n",
            "Step 194/1000 | Loss: 5.0616 | dt: 60.64ms | tok/sec: 16887.91\n",
            "Step 195/1000 | Loss: 4.5552 | dt: 62.25ms | tok/sec: 16449.70\n",
            "Step 196/1000 | Loss: 4.0472 | dt: 61.17ms | tok/sec: 16739.36\n",
            "Step 197/1000 | Loss: 4.6882 | dt: 60.23ms | tok/sec: 17000.95\n",
            "Step 198/1000 | Loss: 5.1105 | dt: 60.76ms | tok/sec: 16852.92\n",
            "Step 199/1000 | Loss: 5.5523 | dt: 60.97ms | tok/sec: 16794.08\n",
            "Step 200/1000 | Loss: 5.3515 | dt: 60.52ms | tok/sec: 16919.65\n",
            "Step 201/1000 | Loss: 5.3587 | dt: 60.84ms | tok/sec: 16831.52\n",
            "Step 202/1000 | Loss: 5.2805 | dt: 60.87ms | tok/sec: 16821.44\n",
            "Step 203/1000 | Loss: 5.4426 | dt: 60.95ms | tok/sec: 16800.91\n",
            "Step 204/1000 | Loss: 4.9337 | dt: 60.79ms | tok/sec: 16845.59\n",
            "Step 205/1000 | Loss: 5.0154 | dt: 60.51ms | tok/sec: 16922.71\n",
            "Step 206/1000 | Loss: 5.1581 | dt: 60.49ms | tok/sec: 16929.32\n",
            "Step 207/1000 | Loss: 5.0637 | dt: 60.71ms | tok/sec: 16868.01\n",
            "Step 208/1000 | Loss: 5.0819 | dt: 60.66ms | tok/sec: 16880.15\n",
            "Step 209/1000 | Loss: 4.8093 | dt: 60.66ms | tok/sec: 16880.01\n",
            "Step 210/1000 | Loss: 4.8166 | dt: 61.36ms | tok/sec: 16687.72\n",
            "Step 211/1000 | Loss: 4.9454 | dt: 60.67ms | tok/sec: 16877.09\n",
            "Step 212/1000 | Loss: 5.2021 | dt: 60.90ms | tok/sec: 16815.64\n",
            "Step 213/1000 | Loss: 4.8849 | dt: 60.41ms | tok/sec: 16952.03\n",
            "Step 214/1000 | Loss: 5.2714 | dt: 60.62ms | tok/sec: 16891.36\n",
            "Step 215/1000 | Loss: 5.2748 | dt: 61.15ms | tok/sec: 16745.16\n",
            "Step 216/1000 | Loss: 5.5394 | dt: 60.61ms | tok/sec: 16894.22\n",
            "Step 217/1000 | Loss: 5.4873 | dt: 60.51ms | tok/sec: 16923.25\n",
            "Step 218/1000 | Loss: 5.5369 | dt: 60.13ms | tok/sec: 17028.38\n",
            "Step 219/1000 | Loss: 5.0756 | dt: 61.22ms | tok/sec: 16726.97\n",
            "Step 220/1000 | Loss: 5.2669 | dt: 60.35ms | tok/sec: 16966.57\n",
            "Step 221/1000 | Loss: 5.2698 | dt: 58.77ms | tok/sec: 17423.73\n",
            "Step 222/1000 | Loss: 5.5813 | dt: 61.81ms | tok/sec: 16568.24\n",
            "Step 223/1000 | Loss: 5.0373 | dt: 61.01ms | tok/sec: 16785.08\n",
            "Step 224/1000 | Loss: 4.9699 | dt: 60.86ms | tok/sec: 16826.45\n",
            "Step 225/1000 | Loss: 4.9323 | dt: 60.91ms | tok/sec: 16812.29\n",
            "Step 226/1000 | Loss: 4.8635 | dt: 61.33ms | tok/sec: 16696.93\n",
            "Step 227/1000 | Loss: 5.1704 | dt: 61.41ms | tok/sec: 16675.99\n",
            "Step 228/1000 | Loss: 4.7487 | dt: 60.56ms | tok/sec: 16910.19\n",
            "Step 229/1000 | Loss: 4.8078 | dt: 61.24ms | tok/sec: 16720.13\n",
            "Step 230/1000 | Loss: 5.1737 | dt: 59.62ms | tok/sec: 17174.17\n",
            "Step 231/1000 | Loss: 4.8253 | dt: 61.11ms | tok/sec: 16756.99\n",
            "Step 232/1000 | Loss: 5.0066 | dt: 60.84ms | tok/sec: 16831.85\n",
            "Step 233/1000 | Loss: 4.8141 | dt: 61.02ms | tok/sec: 16781.28\n",
            "Step 234/1000 | Loss: 5.5656 | dt: 60.31ms | tok/sec: 16979.98\n",
            "Step 235/1000 | Loss: 5.2119 | dt: 59.75ms | tok/sec: 17136.89\n",
            "Step 236/1000 | Loss: 4.6887 | dt: 60.34ms | tok/sec: 16969.52\n",
            "Step 237/1000 | Loss: 4.6643 | dt: 59.66ms | tok/sec: 17164.90\n",
            "Step 238/1000 | Loss: 5.2381 | dt: 59.64ms | tok/sec: 17169.77\n",
            "Step 239/1000 | Loss: 5.0041 | dt: 60.61ms | tok/sec: 16894.69\n",
            "Step 240/1000 | Loss: 5.1616 | dt: 60.73ms | tok/sec: 16862.78\n",
            "Step 241/1000 | Loss: 5.2710 | dt: 59.20ms | tok/sec: 17296.31\n",
            "Step 242/1000 | Loss: 5.2060 | dt: 59.09ms | tok/sec: 17329.32\n",
            "Step 243/1000 | Loss: 5.1863 | dt: 61.20ms | tok/sec: 16731.92\n",
            "Step 244/1000 | Loss: 4.9964 | dt: 59.75ms | tok/sec: 17137.92\n",
            "Step 245/1000 | Loss: 4.6819 | dt: 60.68ms | tok/sec: 16874.31\n",
            "Step 246/1000 | Loss: 4.9899 | dt: 60.90ms | tok/sec: 16815.77\n",
            "Step 247/1000 | Loss: 4.6743 | dt: 60.72ms | tok/sec: 16865.63\n",
            "Step 248/1000 | Loss: 4.4638 | dt: 60.57ms | tok/sec: 16905.66\n",
            "Step 249/1000 | Loss: 4.9609 | dt: 60.32ms | tok/sec: 16975.08\n",
            "Step 250/1000 | Loss: 4.7541 | dt: 60.08ms | tok/sec: 17044.60\n",
            "Step 251/1000 | Loss: 4.7049 | dt: 61.23ms | tok/sec: 16722.54\n",
            "Step 252/1000 | Loss: 4.8593 | dt: 60.75ms | tok/sec: 16855.11\n",
            "Step 253/1000 | Loss: 4.8414 | dt: 61.03ms | tok/sec: 16777.35\n",
            "Step 254/1000 | Loss: 4.9094 | dt: 60.54ms | tok/sec: 16914.85\n",
            "Step 255/1000 | Loss: 5.0338 | dt: 61.32ms | tok/sec: 16699.72\n",
            "Step 256/1000 | Loss: 5.3058 | dt: 59.79ms | tok/sec: 17125.96\n",
            "Step 257/1000 | Loss: 4.7335 | dt: 59.56ms | tok/sec: 17192.18\n",
            "Step 258/1000 | Loss: 4.6228 | dt: 60.20ms | tok/sec: 17009.43\n",
            "Step 259/1000 | Loss: 4.9279 | dt: 60.54ms | tok/sec: 16915.31\n",
            "Step 260/1000 | Loss: 5.0370 | dt: 59.95ms | tok/sec: 17079.78\n",
            "Step 261/1000 | Loss: 4.6530 | dt: 60.55ms | tok/sec: 16911.05\n",
            "Step 262/1000 | Loss: 4.4750 | dt: 61.56ms | tok/sec: 16634.46\n",
            "Step 263/1000 | Loss: 4.7246 | dt: 60.53ms | tok/sec: 16916.85\n",
            "Step 264/1000 | Loss: 5.0217 | dt: 59.45ms | tok/sec: 17224.52\n",
            "Step 265/1000 | Loss: 4.3750 | dt: 60.52ms | tok/sec: 16920.91\n",
            "Step 266/1000 | Loss: 4.7342 | dt: 60.63ms | tok/sec: 16889.64\n",
            "Step 267/1000 | Loss: 4.5756 | dt: 60.55ms | tok/sec: 16910.65\n",
            "Step 268/1000 | Loss: 4.3753 | dt: 60.84ms | tok/sec: 16832.18\n",
            "Step 269/1000 | Loss: 4.3782 | dt: 60.52ms | tok/sec: 16921.04\n",
            "Step 270/1000 | Loss: 4.5889 | dt: 61.13ms | tok/sec: 16752.55\n",
            "Step 271/1000 | Loss: 4.1561 | dt: 60.68ms | tok/sec: 16874.57\n",
            "Step 272/1000 | Loss: 4.5102 | dt: 60.26ms | tok/sec: 16993.48\n",
            "Step 273/1000 | Loss: 4.1180 | dt: 61.10ms | tok/sec: 16760.72\n",
            "Step 274/1000 | Loss: 4.8720 | dt: 60.94ms | tok/sec: 16803.41\n",
            "Step 275/1000 | Loss: 5.1287 | dt: 60.65ms | tok/sec: 16884.59\n",
            "Step 276/1000 | Loss: 5.6326 | dt: 61.05ms | tok/sec: 16773.42\n",
            "Step 277/1000 | Loss: 4.8798 | dt: 60.41ms | tok/sec: 16950.63\n",
            "Step 278/1000 | Loss: 5.3657 | dt: 60.85ms | tok/sec: 16828.36\n",
            "Step 279/1000 | Loss: 5.2693 | dt: 60.79ms | tok/sec: 16843.80\n",
            "Step 280/1000 | Loss: 5.0562 | dt: 61.43ms | tok/sec: 16668.48\n",
            "Step 281/1000 | Loss: 4.9215 | dt: 61.04ms | tok/sec: 16776.56\n",
            "Step 282/1000 | Loss: 5.2780 | dt: 60.45ms | tok/sec: 16940.60\n",
            "Step 283/1000 | Loss: 4.9508 | dt: 60.70ms | tok/sec: 16870.73\n",
            "Step 284/1000 | Loss: 4.5169 | dt: 60.68ms | tok/sec: 16875.70\n",
            "Step 285/1000 | Loss: 4.7977 | dt: 60.57ms | tok/sec: 16906.13\n",
            "Step 286/1000 | Loss: 4.7767 | dt: 60.25ms | tok/sec: 16994.83\n",
            "Step 287/1000 | Loss: 4.9272 | dt: 61.35ms | tok/sec: 16691.61\n",
            "Step 288/1000 | Loss: 4.1435 | dt: 59.07ms | tok/sec: 17336.52\n",
            "Step 289/1000 | Loss: 4.6985 | dt: 60.34ms | tok/sec: 16970.39\n",
            "Step 290/1000 | Loss: 5.0029 | dt: 61.05ms | tok/sec: 16773.48\n",
            "Step 291/1000 | Loss: 5.1503 | dt: 59.54ms | tok/sec: 17197.48\n",
            "Step 292/1000 | Loss: 4.7636 | dt: 61.73ms | tok/sec: 16589.35\n",
            "Step 293/1000 | Loss: 4.8356 | dt: 61.08ms | tok/sec: 16765.10\n",
            "Step 294/1000 | Loss: 4.7573 | dt: 61.18ms | tok/sec: 16738.83\n",
            "Step 295/1000 | Loss: 4.3993 | dt: 60.95ms | tok/sec: 16801.57\n",
            "Step 296/1000 | Loss: 5.1894 | dt: 61.92ms | tok/sec: 16538.12\n",
            "Step 297/1000 | Loss: 4.8975 | dt: 59.61ms | tok/sec: 17178.91\n",
            "Step 298/1000 | Loss: 4.7103 | dt: 60.85ms | tok/sec: 16826.97\n",
            "Step 299/1000 | Loss: 4.3925 | dt: 60.36ms | tok/sec: 16965.69\n",
            "Step 300/1000 | Loss: 4.3722 | dt: 60.43ms | tok/sec: 16944.01\n",
            "Step 301/1000 | Loss: 4.4998 | dt: 61.30ms | tok/sec: 16704.01\n",
            "Step 302/1000 | Loss: 4.3648 | dt: 60.26ms | tok/sec: 16993.08\n",
            "Step 303/1000 | Loss: 4.4307 | dt: 59.15ms | tok/sec: 17311.16\n",
            "Step 304/1000 | Loss: 4.3991 | dt: 60.61ms | tok/sec: 16895.09\n",
            "Step 305/1000 | Loss: 4.3888 | dt: 61.18ms | tok/sec: 16738.51\n",
            "Step 306/1000 | Loss: 4.2936 | dt: 59.50ms | tok/sec: 17210.71\n",
            "Step 307/1000 | Loss: 4.0922 | dt: 61.32ms | tok/sec: 16698.74\n",
            "Step 308/1000 | Loss: 3.7900 | dt: 60.51ms | tok/sec: 16922.78\n",
            "Step 309/1000 | Loss: 4.5602 | dt: 60.33ms | tok/sec: 16973.34\n",
            "Step 310/1000 | Loss: 5.1738 | dt: 61.01ms | tok/sec: 16785.28\n",
            "Step 311/1000 | Loss: 5.3996 | dt: 60.33ms | tok/sec: 16974.14\n",
            "Step 312/1000 | Loss: 5.5529 | dt: 60.91ms | tok/sec: 16810.77\n",
            "Step 313/1000 | Loss: 5.5967 | dt: 61.42ms | tok/sec: 16671.26\n",
            "Step 314/1000 | Loss: 5.2777 | dt: 60.62ms | tok/sec: 16893.09\n",
            "Step 315/1000 | Loss: 5.4174 | dt: 61.04ms | tok/sec: 16774.66\n",
            "Step 316/1000 | Loss: 5.3536 | dt: 61.19ms | tok/sec: 16735.05\n",
            "Step 317/1000 | Loss: 4.9874 | dt: 60.67ms | tok/sec: 16876.96\n",
            "Step 318/1000 | Loss: 4.9114 | dt: 59.22ms | tok/sec: 17292.13\n",
            "Step 319/1000 | Loss: 5.0596 | dt: 60.80ms | tok/sec: 16840.90\n",
            "Step 320/1000 | Loss: 4.9648 | dt: 61.30ms | tok/sec: 16704.20\n",
            "Step 321/1000 | Loss: 5.4389 | dt: 60.64ms | tok/sec: 16885.65\n",
            "Step 322/1000 | Loss: 5.1577 | dt: 60.89ms | tok/sec: 16816.96\n",
            "Step 323/1000 | Loss: 5.3664 | dt: 60.79ms | tok/sec: 16844.73\n",
            "Step 324/1000 | Loss: 5.1183 | dt: 61.05ms | tok/sec: 16773.61\n",
            "Step 325/1000 | Loss: 5.3140 | dt: 60.68ms | tok/sec: 16874.24\n",
            "Step 326/1000 | Loss: 4.8913 | dt: 60.87ms | tok/sec: 16821.77\n",
            "Step 327/1000 | Loss: 5.0958 | dt: 60.89ms | tok/sec: 16815.97\n",
            "Step 328/1000 | Loss: 4.8488 | dt: 60.42ms | tok/sec: 16948.56\n",
            "Step 329/1000 | Loss: 5.0147 | dt: 60.90ms | tok/sec: 16813.34\n",
            "Step 330/1000 | Loss: 4.7569 | dt: 61.68ms | tok/sec: 16600.64\n",
            "Step 331/1000 | Loss: 4.6768 | dt: 58.35ms | tok/sec: 17548.03\n",
            "Step 332/1000 | Loss: 5.4076 | dt: 60.99ms | tok/sec: 16789.41\n",
            "Step 333/1000 | Loss: 4.5721 | dt: 61.03ms | tok/sec: 16779.71\n",
            "Step 334/1000 | Loss: 4.8822 | dt: 60.71ms | tok/sec: 16867.42\n",
            "Step 335/1000 | Loss: 4.8384 | dt: 60.67ms | tok/sec: 16876.83\n",
            "Step 336/1000 | Loss: 4.8420 | dt: 61.20ms | tok/sec: 16733.10\n",
            "Step 337/1000 | Loss: 4.6983 | dt: 60.84ms | tok/sec: 16829.94\n",
            "Step 338/1000 | Loss: 4.5256 | dt: 61.02ms | tok/sec: 16782.26\n",
            "Step 339/1000 | Loss: 4.3751 | dt: 60.54ms | tok/sec: 16914.45\n",
            "Step 340/1000 | Loss: 4.6271 | dt: 61.12ms | tok/sec: 16753.59\n",
            "Step 341/1000 | Loss: 3.9996 | dt: 60.17ms | tok/sec: 17019.07\n",
            "Step 342/1000 | Loss: 4.8664 | dt: 61.25ms | tok/sec: 16717.40\n",
            "Step 343/1000 | Loss: 4.0573 | dt: 61.27ms | tok/sec: 16713.30\n",
            "Step 344/1000 | Loss: 3.8820 | dt: 60.34ms | tok/sec: 16971.19\n",
            "Step 345/1000 | Loss: 4.4403 | dt: 60.33ms | tok/sec: 16972.00\n",
            "Step 346/1000 | Loss: 4.3819 | dt: 60.38ms | tok/sec: 16958.46\n",
            "Step 347/1000 | Loss: 4.4278 | dt: 60.58ms | tok/sec: 16902.87\n",
            "Step 348/1000 | Loss: 4.1679 | dt: 61.07ms | tok/sec: 16766.80\n",
            "Step 349/1000 | Loss: 4.4086 | dt: 60.56ms | tok/sec: 16909.72\n",
            "Step 350/1000 | Loss: 4.3602 | dt: 61.36ms | tok/sec: 16689.53\n",
            "Step 351/1000 | Loss: 4.5727 | dt: 60.42ms | tok/sec: 16947.15\n",
            "Step 352/1000 | Loss: 4.3305 | dt: 61.40ms | tok/sec: 16676.51\n",
            "Step 353/1000 | Loss: 4.7298 | dt: 60.54ms | tok/sec: 16913.65\n",
            "Step 354/1000 | Loss: 4.7546 | dt: 60.82ms | tok/sec: 16837.26\n",
            "Step 355/1000 | Loss: 4.6868 | dt: 61.29ms | tok/sec: 16707.38\n",
            "Step 356/1000 | Loss: 4.2228 | dt: 60.95ms | tok/sec: 16801.04\n",
            "Step 357/1000 | Loss: 4.3595 | dt: 60.85ms | tok/sec: 16828.23\n",
            "Step 358/1000 | Loss: 4.6952 | dt: 60.12ms | tok/sec: 17032.71\n",
            "Step 359/1000 | Loss: 4.1771 | dt: 60.46ms | tok/sec: 16937.66\n",
            "Step 360/1000 | Loss: 4.9070 | dt: 60.55ms | tok/sec: 16911.58\n",
            "Step 361/1000 | Loss: 4.7991 | dt: 61.57ms | tok/sec: 16630.21\n",
            "Step 362/1000 | Loss: 4.8001 | dt: 61.10ms | tok/sec: 16758.62\n",
            "Step 363/1000 | Loss: 4.6992 | dt: 60.75ms | tok/sec: 16856.89\n",
            "Step 364/1000 | Loss: 4.7791 | dt: 60.51ms | tok/sec: 16924.05\n",
            "Step 365/1000 | Loss: 4.6877 | dt: 60.16ms | tok/sec: 17021.70\n",
            "Step 366/1000 | Loss: 4.5196 | dt: 60.97ms | tok/sec: 16795.39\n",
            "Step 367/1000 | Loss: 5.2973 | dt: 60.68ms | tok/sec: 16874.77\n",
            "Step 368/1000 | Loss: 4.7746 | dt: 60.84ms | tok/sec: 16831.66\n",
            "Step 369/1000 | Loss: 5.1862 | dt: 60.84ms | tok/sec: 16830.73\n",
            "Step 370/1000 | Loss: 4.3581 | dt: 60.85ms | tok/sec: 16826.97\n",
            "Step 371/1000 | Loss: 4.2875 | dt: 60.63ms | tok/sec: 16888.58\n",
            "Step 372/1000 | Loss: 4.6819 | dt: 60.76ms | tok/sec: 16852.53\n",
            "Step 373/1000 | Loss: 4.7107 | dt: 60.88ms | tok/sec: 16819.46\n",
            "Step 374/1000 | Loss: 4.3667 | dt: 61.04ms | tok/sec: 16775.84\n",
            "Step 375/1000 | Loss: 4.5990 | dt: 61.17ms | tok/sec: 16741.57\n",
            "Step 376/1000 | Loss: 4.4258 | dt: 60.38ms | tok/sec: 16959.26\n",
            "Step 377/1000 | Loss: 4.5283 | dt: 59.35ms | tok/sec: 17253.44\n",
            "Step 378/1000 | Loss: 4.8771 | dt: 61.16ms | tok/sec: 16743.66\n",
            "Step 379/1000 | Loss: 4.5014 | dt: 61.21ms | tok/sec: 16730.23\n",
            "Step 380/1000 | Loss: 4.3950 | dt: 60.79ms | tok/sec: 16844.46\n",
            "Step 381/1000 | Loss: 4.4575 | dt: 60.80ms | tok/sec: 16842.08\n",
            "Step 382/1000 | Loss: 4.5312 | dt: 60.75ms | tok/sec: 16856.30\n",
            "Step 383/1000 | Loss: 4.5218 | dt: 60.79ms | tok/sec: 16845.59\n",
            "Step 384/1000 | Loss: 4.6852 | dt: 60.88ms | tok/sec: 16818.87\n",
            "Step 385/1000 | Loss: 4.5305 | dt: 61.22ms | tok/sec: 16726.45\n",
            "Step 386/1000 | Loss: 4.1794 | dt: 61.12ms | tok/sec: 16755.09\n",
            "Step 387/1000 | Loss: 4.4835 | dt: 61.08ms | tok/sec: 16764.84\n",
            "Step 388/1000 | Loss: 3.7905 | dt: 60.47ms | tok/sec: 16932.92\n",
            "Step 389/1000 | Loss: 4.1807 | dt: 60.84ms | tok/sec: 16830.93\n",
            "Step 390/1000 | Loss: 4.4735 | dt: 60.60ms | tok/sec: 16897.21\n",
            "Step 391/1000 | Loss: 4.3106 | dt: 60.28ms | tok/sec: 16987.10\n",
            "Step 392/1000 | Loss: 4.2243 | dt: 60.65ms | tok/sec: 16884.59\n",
            "Step 393/1000 | Loss: 4.5681 | dt: 60.70ms | tok/sec: 16869.07\n",
            "Step 394/1000 | Loss: 4.7934 | dt: 58.95ms | tok/sec: 17370.53\n",
            "Step 395/1000 | Loss: 4.5907 | dt: 61.14ms | tok/sec: 16749.80\n",
            "Step 396/1000 | Loss: 4.7506 | dt: 60.77ms | tok/sec: 16849.22\n",
            "Step 397/1000 | Loss: 4.3694 | dt: 61.39ms | tok/sec: 16680.52\n",
            "Step 398/1000 | Loss: 4.4176 | dt: 60.54ms | tok/sec: 16915.45\n",
            "Step 399/1000 | Loss: 4.4408 | dt: 59.77ms | tok/sec: 17131.35\n",
            "Step 400/1000 | Loss: 4.2202 | dt: 60.79ms | tok/sec: 16844.40\n",
            "Step 401/1000 | Loss: 5.0149 | dt: 60.71ms | tok/sec: 16865.96\n",
            "Step 402/1000 | Loss: 4.6701 | dt: 61.11ms | tok/sec: 16755.55\n",
            "Step 403/1000 | Loss: 4.3842 | dt: 59.19ms | tok/sec: 17301.32\n",
            "Step 404/1000 | Loss: 4.2963 | dt: 61.71ms | tok/sec: 16593.71\n",
            "Step 405/1000 | Loss: 3.9358 | dt: 61.25ms | tok/sec: 16717.92\n",
            "Step 406/1000 | Loss: 4.2387 | dt: 61.13ms | tok/sec: 16750.78\n",
            "Step 407/1000 | Loss: 4.1633 | dt: 60.86ms | tok/sec: 16826.12\n",
            "Step 408/1000 | Loss: 4.2023 | dt: 58.90ms | tok/sec: 17384.31\n",
            "Step 409/1000 | Loss: 4.8865 | dt: 61.32ms | tok/sec: 16700.24\n",
            "Step 410/1000 | Loss: 4.8386 | dt: 61.30ms | tok/sec: 16703.75\n",
            "Step 411/1000 | Loss: 4.6050 | dt: 62.36ms | tok/sec: 16420.58\n",
            "Step 412/1000 | Loss: 4.8250 | dt: 61.92ms | tok/sec: 16537.68\n",
            "Step 413/1000 | Loss: 4.5213 | dt: 61.57ms | tok/sec: 16632.14\n",
            "Step 414/1000 | Loss: 4.9283 | dt: 61.19ms | tok/sec: 16734.40\n",
            "Step 415/1000 | Loss: 5.0042 | dt: 60.35ms | tok/sec: 16968.58\n",
            "Step 416/1000 | Loss: 5.0051 | dt: 60.30ms | tok/sec: 16981.12\n",
            "Step 417/1000 | Loss: 5.0617 | dt: 61.16ms | tok/sec: 16741.84\n",
            "Step 418/1000 | Loss: 4.8210 | dt: 61.02ms | tok/sec: 16781.80\n",
            "Step 419/1000 | Loss: 5.0262 | dt: 60.69ms | tok/sec: 16872.98\n",
            "Step 420/1000 | Loss: 4.7148 | dt: 60.73ms | tok/sec: 16861.19\n",
            "Step 421/1000 | Loss: 4.6338 | dt: 60.68ms | tok/sec: 16874.51\n",
            "Step 422/1000 | Loss: 4.9419 | dt: 59.56ms | tok/sec: 17191.56\n",
            "Step 423/1000 | Loss: 4.9729 | dt: 61.25ms | tok/sec: 16718.38\n",
            "Step 424/1000 | Loss: 4.7583 | dt: 61.07ms | tok/sec: 16766.87\n",
            "Step 425/1000 | Loss: 4.7244 | dt: 60.46ms | tok/sec: 16937.53\n",
            "Step 426/1000 | Loss: 4.9992 | dt: 60.82ms | tok/sec: 16837.07\n",
            "Step 427/1000 | Loss: 4.6363 | dt: 60.37ms | tok/sec: 16960.87\n",
            "Step 428/1000 | Loss: 4.5792 | dt: 59.75ms | tok/sec: 17138.74\n",
            "Step 429/1000 | Loss: 4.4250 | dt: 57.85ms | tok/sec: 17700.48\n",
            "Step 430/1000 | Loss: 4.7198 | dt: 61.56ms | tok/sec: 16632.91\n",
            "Step 431/1000 | Loss: 4.8126 | dt: 62.36ms | tok/sec: 16420.02\n",
            "Step 432/1000 | Loss: 4.9924 | dt: 61.57ms | tok/sec: 16630.79\n",
            "Step 433/1000 | Loss: 5.0018 | dt: 61.26ms | tok/sec: 16715.84\n",
            "Step 434/1000 | Loss: 4.5219 | dt: 60.97ms | tok/sec: 16794.01\n",
            "Step 435/1000 | Loss: 4.8958 | dt: 60.88ms | tok/sec: 16819.20\n",
            "Step 436/1000 | Loss: 4.4473 | dt: 58.55ms | tok/sec: 17489.93\n",
            "Step 437/1000 | Loss: 4.9543 | dt: 60.51ms | tok/sec: 16921.64\n",
            "Step 438/1000 | Loss: 4.4208 | dt: 60.24ms | tok/sec: 16998.39\n",
            "Step 439/1000 | Loss: 4.6963 | dt: 61.18ms | tok/sec: 16737.07\n",
            "Step 440/1000 | Loss: 4.6288 | dt: 60.78ms | tok/sec: 16846.84\n",
            "Step 441/1000 | Loss: 4.5784 | dt: 60.64ms | tok/sec: 16887.38\n",
            "Step 442/1000 | Loss: 4.3542 | dt: 60.07ms | tok/sec: 17048.12\n",
            "Step 443/1000 | Loss: 4.6470 | dt: 60.95ms | tok/sec: 16799.59\n",
            "Step 444/1000 | Loss: 4.2379 | dt: 60.52ms | tok/sec: 16919.31\n",
            "Step 445/1000 | Loss: 3.8342 | dt: 61.72ms | tok/sec: 16592.05\n",
            "Step 446/1000 | Loss: 4.0322 | dt: 60.07ms | tok/sec: 17045.89\n",
            "Step 447/1000 | Loss: 4.3178 | dt: 60.89ms | tok/sec: 16816.10\n",
            "Step 448/1000 | Loss: 5.1247 | dt: 59.73ms | tok/sec: 17143.73\n",
            "Step 449/1000 | Loss: 4.6245 | dt: 59.45ms | tok/sec: 17225.14\n",
            "Step 450/1000 | Loss: 4.7868 | dt: 60.70ms | tok/sec: 16870.27\n",
            "Step 451/1000 | Loss: 5.0219 | dt: 60.80ms | tok/sec: 16841.69\n",
            "Step 452/1000 | Loss: 4.6942 | dt: 59.82ms | tok/sec: 17117.49\n",
            "Step 453/1000 | Loss: 5.1672 | dt: 60.91ms | tok/sec: 16810.97\n",
            "Step 454/1000 | Loss: 4.7733 | dt: 59.90ms | tok/sec: 17096.30\n",
            "Step 455/1000 | Loss: 4.8158 | dt: 60.26ms | tok/sec: 16992.41\n",
            "Step 456/1000 | Loss: 4.6496 | dt: 59.72ms | tok/sec: 17145.44\n",
            "Step 457/1000 | Loss: 5.3762 | dt: 60.26ms | tok/sec: 16992.54\n",
            "Step 458/1000 | Loss: 4.6187 | dt: 59.67ms | tok/sec: 17160.85\n",
            "Step 459/1000 | Loss: 4.4674 | dt: 61.42ms | tok/sec: 16671.26\n",
            "Step 460/1000 | Loss: 4.8995 | dt: 58.82ms | tok/sec: 17407.84\n",
            "Step 461/1000 | Loss: 4.5477 | dt: 59.32ms | tok/sec: 17263.08\n",
            "Step 462/1000 | Loss: 4.3488 | dt: 60.97ms | tok/sec: 16794.01\n",
            "Step 463/1000 | Loss: 5.1744 | dt: 60.99ms | tok/sec: 16790.92\n",
            "Step 464/1000 | Loss: 4.8425 | dt: 60.95ms | tok/sec: 16799.86\n",
            "Step 465/1000 | Loss: 4.3255 | dt: 59.44ms | tok/sec: 17228.45\n",
            "Step 466/1000 | Loss: 4.4736 | dt: 60.47ms | tok/sec: 16933.65\n",
            "Step 467/1000 | Loss: 4.6837 | dt: 60.51ms | tok/sec: 16923.18\n",
            "Step 468/1000 | Loss: 4.6250 | dt: 60.80ms | tok/sec: 16841.09\n",
            "Step 469/1000 | Loss: 4.6391 | dt: 60.52ms | tok/sec: 16919.78\n",
            "Step 470/1000 | Loss: 4.4160 | dt: 61.03ms | tok/sec: 16777.28\n",
            "Step 471/1000 | Loss: 5.0819 | dt: 60.48ms | tok/sec: 16930.32\n",
            "Step 472/1000 | Loss: 4.7459 | dt: 61.24ms | tok/sec: 16721.43\n",
            "Step 473/1000 | Loss: 4.7119 | dt: 58.83ms | tok/sec: 17407.07\n",
            "Step 474/1000 | Loss: 4.4715 | dt: 60.37ms | tok/sec: 16960.94\n",
            "Step 475/1000 | Loss: 4.5871 | dt: 61.41ms | tok/sec: 16674.37\n",
            "Step 476/1000 | Loss: 4.4575 | dt: 61.76ms | tok/sec: 16579.30\n",
            "Step 477/1000 | Loss: 4.0235 | dt: 61.28ms | tok/sec: 16709.01\n",
            "Step 478/1000 | Loss: 4.6083 | dt: 61.21ms | tok/sec: 16729.12\n",
            "Step 479/1000 | Loss: 4.1767 | dt: 60.76ms | tok/sec: 16852.39\n",
            "Step 480/1000 | Loss: 4.6311 | dt: 60.48ms | tok/sec: 16930.38\n",
            "Step 481/1000 | Loss: 4.3722 | dt: 60.64ms | tok/sec: 16885.59\n",
            "Step 482/1000 | Loss: 4.5273 | dt: 61.07ms | tok/sec: 16768.44\n",
            "Step 483/1000 | Loss: 4.3254 | dt: 60.75ms | tok/sec: 16857.02\n",
            "Step 484/1000 | Loss: 4.9424 | dt: 60.62ms | tok/sec: 16892.89\n",
            "Step 485/1000 | Loss: 5.0618 | dt: 61.25ms | tok/sec: 16719.03\n",
            "Step 486/1000 | Loss: 4.8118 | dt: 60.80ms | tok/sec: 16841.29\n",
            "Step 487/1000 | Loss: 4.8002 | dt: 60.70ms | tok/sec: 16869.01\n",
            "Step 488/1000 | Loss: 4.4001 | dt: 60.45ms | tok/sec: 16940.67\n",
            "Step 489/1000 | Loss: 4.7136 | dt: 60.76ms | tok/sec: 16852.39\n",
            "Step 490/1000 | Loss: 4.6876 | dt: 61.17ms | tok/sec: 16739.23\n",
            "Step 491/1000 | Loss: 4.6883 | dt: 60.47ms | tok/sec: 16933.45\n",
            "Step 492/1000 | Loss: 4.5762 | dt: 60.88ms | tok/sec: 16819.26\n",
            "Step 493/1000 | Loss: 4.5276 | dt: 60.97ms | tok/sec: 16795.19\n",
            "Step 494/1000 | Loss: 4.4327 | dt: 60.77ms | tok/sec: 16849.62\n",
            "Step 495/1000 | Loss: 4.6021 | dt: 60.30ms | tok/sec: 16981.79\n",
            "Step 496/1000 | Loss: 5.0098 | dt: 60.34ms | tok/sec: 16970.45\n",
            "Step 497/1000 | Loss: 4.9357 | dt: 59.19ms | tok/sec: 17300.77\n",
            "Step 498/1000 | Loss: 4.7802 | dt: 61.95ms | tok/sec: 16529.66\n",
            "Step 499/1000 | Loss: 4.9409 | dt: 61.27ms | tok/sec: 16712.13\n",
            "Step 500/1000 | Loss: 4.7795 | dt: 61.22ms | tok/sec: 16726.12\n",
            "Step 501/1000 | Loss: 4.6703 | dt: 60.87ms | tok/sec: 16823.09\n",
            "Step 502/1000 | Loss: 4.2106 | dt: 61.11ms | tok/sec: 16757.32\n",
            "Step 503/1000 | Loss: 4.7588 | dt: 60.74ms | tok/sec: 16857.55\n",
            "Step 504/1000 | Loss: 5.0112 | dt: 60.88ms | tok/sec: 16818.61\n",
            "Step 505/1000 | Loss: 4.6177 | dt: 60.98ms | tok/sec: 16792.11\n",
            "Step 506/1000 | Loss: 4.7285 | dt: 60.87ms | tok/sec: 16821.44\n",
            "Step 507/1000 | Loss: 4.6992 | dt: 61.03ms | tok/sec: 16779.90\n",
            "Step 508/1000 | Loss: 4.1870 | dt: 60.69ms | tok/sec: 16871.46\n",
            "Step 509/1000 | Loss: 3.6680 | dt: 60.59ms | tok/sec: 16900.74\n",
            "Step 510/1000 | Loss: 4.4430 | dt: 60.55ms | tok/sec: 16911.78\n",
            "Step 511/1000 | Loss: 4.7819 | dt: 60.88ms | tok/sec: 16819.13\n",
            "Step 512/1000 | Loss: 4.8207 | dt: 60.78ms | tok/sec: 16847.37\n",
            "Step 513/1000 | Loss: 4.6400 | dt: 61.25ms | tok/sec: 16718.90\n",
            "Step 514/1000 | Loss: 4.3042 | dt: 60.52ms | tok/sec: 16921.18\n",
            "Step 515/1000 | Loss: 4.4788 | dt: 61.76ms | tok/sec: 16580.58\n",
            "Step 516/1000 | Loss: 4.5695 | dt: 59.82ms | tok/sec: 17117.36\n",
            "Step 517/1000 | Loss: 4.3438 | dt: 60.88ms | tok/sec: 16820.58\n",
            "Step 518/1000 | Loss: 4.4604 | dt: 61.00ms | tok/sec: 16786.92\n",
            "Step 519/1000 | Loss: 4.4103 | dt: 60.86ms | tok/sec: 16825.00\n",
            "Step 520/1000 | Loss: 4.2721 | dt: 59.85ms | tok/sec: 17110.13\n",
            "Step 521/1000 | Loss: 4.6788 | dt: 60.61ms | tok/sec: 16894.69\n",
            "Step 522/1000 | Loss: 4.1709 | dt: 61.00ms | tok/sec: 16785.94\n",
            "Step 523/1000 | Loss: 4.4469 | dt: 61.02ms | tok/sec: 16782.72\n",
            "Step 524/1000 | Loss: 4.8572 | dt: 60.93ms | tok/sec: 16806.96\n",
            "Step 525/1000 | Loss: 4.3129 | dt: 58.92ms | tok/sec: 17380.30\n",
            "Step 526/1000 | Loss: 3.8336 | dt: 61.91ms | tok/sec: 16539.46\n",
            "Step 527/1000 | Loss: 4.4841 | dt: 60.98ms | tok/sec: 16791.25\n",
            "Step 528/1000 | Loss: 4.8683 | dt: 60.80ms | tok/sec: 16841.56\n",
            "Step 529/1000 | Loss: 5.2486 | dt: 60.82ms | tok/sec: 16835.42\n",
            "Step 530/1000 | Loss: 5.0804 | dt: 60.39ms | tok/sec: 16957.12\n",
            "Step 531/1000 | Loss: 5.1697 | dt: 61.44ms | tok/sec: 16666.41\n",
            "Step 532/1000 | Loss: 4.9912 | dt: 62.12ms | tok/sec: 16485.50\n",
            "Step 533/1000 | Loss: 5.2031 | dt: 60.39ms | tok/sec: 16956.65\n",
            "Step 534/1000 | Loss: 4.6608 | dt: 59.88ms | tok/sec: 17101.82\n",
            "Step 535/1000 | Loss: 4.7966 | dt: 60.45ms | tok/sec: 16940.73\n",
            "Step 536/1000 | Loss: 4.8719 | dt: 60.55ms | tok/sec: 16912.25\n",
            "Step 537/1000 | Loss: 4.7742 | dt: 61.00ms | tok/sec: 16786.33\n",
            "Step 538/1000 | Loss: 4.8229 | dt: 60.93ms | tok/sec: 16805.77\n",
            "Step 539/1000 | Loss: 4.5234 | dt: 59.17ms | tok/sec: 17307.04\n",
            "Step 540/1000 | Loss: 4.5737 | dt: 60.55ms | tok/sec: 16912.25\n",
            "Step 541/1000 | Loss: 4.6967 | dt: 60.73ms | tok/sec: 16860.93\n",
            "Step 542/1000 | Loss: 4.9589 | dt: 61.28ms | tok/sec: 16709.33\n",
            "Step 543/1000 | Loss: 4.6457 | dt: 60.81ms | tok/sec: 16839.77\n",
            "Step 544/1000 | Loss: 5.0701 | dt: 60.69ms | tok/sec: 16872.98\n",
            "Step 545/1000 | Loss: 5.0546 | dt: 60.44ms | tok/sec: 16942.94\n",
            "Step 546/1000 | Loss: 5.2540 | dt: 59.30ms | tok/sec: 17268.42\n",
            "Step 547/1000 | Loss: 5.2582 | dt: 60.19ms | tok/sec: 17013.61\n",
            "Step 548/1000 | Loss: 5.2965 | dt: 60.07ms | tok/sec: 17046.23\n",
            "Step 549/1000 | Loss: 4.8099 | dt: 61.07ms | tok/sec: 16767.33\n",
            "Step 550/1000 | Loss: 5.0635 | dt: 61.91ms | tok/sec: 16541.31\n",
            "Step 551/1000 | Loss: 5.0444 | dt: 60.85ms | tok/sec: 16829.48\n",
            "Step 552/1000 | Loss: 5.3312 | dt: 61.46ms | tok/sec: 16660.07\n",
            "Step 553/1000 | Loss: 4.7715 | dt: 61.29ms | tok/sec: 16708.03\n",
            "Step 554/1000 | Loss: 4.7102 | dt: 61.28ms | tok/sec: 16709.20\n",
            "Step 555/1000 | Loss: 4.6814 | dt: 61.02ms | tok/sec: 16781.54\n",
            "Step 556/1000 | Loss: 4.6097 | dt: 60.66ms | tok/sec: 16881.41\n",
            "Step 557/1000 | Loss: 4.9236 | dt: 60.61ms | tok/sec: 16894.95\n",
            "Step 558/1000 | Loss: 4.5016 | dt: 61.19ms | tok/sec: 16733.75\n",
            "Step 559/1000 | Loss: 4.5592 | dt: 61.23ms | tok/sec: 16722.48\n",
            "Step 560/1000 | Loss: 4.9622 | dt: 60.89ms | tok/sec: 16817.82\n",
            "Step 561/1000 | Loss: 4.6372 | dt: 60.45ms | tok/sec: 16940.53\n",
            "Step 562/1000 | Loss: 4.7767 | dt: 60.90ms | tok/sec: 16813.21\n",
            "Step 563/1000 | Loss: 4.6018 | dt: 60.83ms | tok/sec: 16833.57\n",
            "Step 564/1000 | Loss: 5.3412 | dt: 61.09ms | tok/sec: 16762.68\n",
            "Step 565/1000 | Loss: 5.0162 | dt: 61.19ms | tok/sec: 16735.90\n",
            "Step 566/1000 | Loss: 4.4851 | dt: 61.07ms | tok/sec: 16767.92\n",
            "Step 567/1000 | Loss: 4.4611 | dt: 60.60ms | tok/sec: 16897.81\n",
            "Step 568/1000 | Loss: 4.9734 | dt: 60.53ms | tok/sec: 16916.25\n",
            "Step 569/1000 | Loss: 4.7397 | dt: 59.53ms | tok/sec: 17201.54\n",
            "Step 570/1000 | Loss: 4.8047 | dt: 59.89ms | tok/sec: 17097.46\n",
            "Step 571/1000 | Loss: 4.9591 | dt: 61.09ms | tok/sec: 16761.57\n",
            "Step 572/1000 | Loss: 4.9306 | dt: 61.43ms | tok/sec: 16670.68\n",
            "Step 573/1000 | Loss: 4.8643 | dt: 61.39ms | tok/sec: 16679.94\n",
            "Step 574/1000 | Loss: 4.6516 | dt: 59.85ms | tok/sec: 17108.70\n",
            "Step 575/1000 | Loss: 4.3702 | dt: 61.12ms | tok/sec: 16753.07\n",
            "Step 576/1000 | Loss: 4.6208 | dt: 60.85ms | tok/sec: 16828.29\n",
            "Step 577/1000 | Loss: 4.3658 | dt: 61.09ms | tok/sec: 16761.50\n",
            "Step 578/1000 | Loss: 4.2336 | dt: 61.08ms | tok/sec: 16763.60\n",
            "Step 579/1000 | Loss: 4.6652 | dt: 60.87ms | tok/sec: 16823.61\n",
            "Step 580/1000 | Loss: 4.4183 | dt: 60.93ms | tok/sec: 16805.05\n",
            "Step 581/1000 | Loss: 4.4315 | dt: 61.32ms | tok/sec: 16699.33\n",
            "Step 582/1000 | Loss: 4.5828 | dt: 60.43ms | tok/sec: 16944.01\n",
            "Step 583/1000 | Loss: 4.5709 | dt: 60.45ms | tok/sec: 16938.53\n",
            "Step 584/1000 | Loss: 4.6449 | dt: 60.78ms | tok/sec: 16846.77\n",
            "Step 585/1000 | Loss: 4.7119 | dt: 61.00ms | tok/sec: 16786.40\n",
            "Step 586/1000 | Loss: 4.9691 | dt: 60.72ms | tok/sec: 16863.18\n",
            "Step 587/1000 | Loss: 4.4242 | dt: 60.72ms | tok/sec: 16863.97\n",
            "Step 588/1000 | Loss: 4.3063 | dt: 61.32ms | tok/sec: 16698.23\n",
            "Step 589/1000 | Loss: 4.6664 | dt: 61.88ms | tok/sec: 16547.87\n",
            "Step 590/1000 | Loss: 4.7373 | dt: 60.36ms | tok/sec: 16963.82\n",
            "Step 591/1000 | Loss: 4.3574 | dt: 59.99ms | tok/sec: 17070.21\n",
            "Step 592/1000 | Loss: 4.2227 | dt: 61.01ms | tok/sec: 16784.82\n",
            "Step 593/1000 | Loss: 4.4819 | dt: 60.95ms | tok/sec: 16800.71\n",
            "Step 594/1000 | Loss: 4.7745 | dt: 60.66ms | tok/sec: 16880.61\n",
            "Step 595/1000 | Loss: 4.1230 | dt: 61.14ms | tok/sec: 16749.47\n",
            "Step 596/1000 | Loss: 4.4886 | dt: 60.34ms | tok/sec: 16969.85\n",
            "Step 597/1000 | Loss: 4.3114 | dt: 60.61ms | tok/sec: 16893.82\n",
            "Step 598/1000 | Loss: 4.1495 | dt: 60.81ms | tok/sec: 16838.98\n",
            "Step 599/1000 | Loss: 4.1596 | dt: 61.16ms | tok/sec: 16742.36\n",
            "Step 600/1000 | Loss: 4.3476 | dt: 60.31ms | tok/sec: 16979.98\n",
            "Step 601/1000 | Loss: 3.9311 | dt: 60.19ms | tok/sec: 17014.08\n",
            "Step 602/1000 | Loss: 4.2850 | dt: 60.88ms | tok/sec: 16819.86\n",
            "Step 603/1000 | Loss: 3.9204 | dt: 61.03ms | tok/sec: 16779.31\n",
            "Step 604/1000 | Loss: 4.6162 | dt: 60.75ms | tok/sec: 16856.36\n",
            "Step 605/1000 | Loss: 4.8813 | dt: 60.81ms | tok/sec: 16840.30\n",
            "Step 606/1000 | Loss: 5.3254 | dt: 60.75ms | tok/sec: 16854.84\n",
            "Step 607/1000 | Loss: 4.6590 | dt: 60.21ms | tok/sec: 17007.55\n",
            "Step 608/1000 | Loss: 5.0027 | dt: 60.96ms | tok/sec: 16797.69\n",
            "Step 609/1000 | Loss: 4.9036 | dt: 60.03ms | tok/sec: 17059.16\n",
            "Step 610/1000 | Loss: 4.7500 | dt: 60.27ms | tok/sec: 16989.92\n",
            "Step 611/1000 | Loss: 4.5731 | dt: 61.25ms | tok/sec: 16718.05\n",
            "Step 612/1000 | Loss: 4.9530 | dt: 60.71ms | tok/sec: 16865.70\n",
            "Step 613/1000 | Loss: 4.6139 | dt: 61.08ms | tok/sec: 16764.38\n",
            "Step 614/1000 | Loss: 4.2256 | dt: 60.78ms | tok/sec: 16848.63\n",
            "Step 615/1000 | Loss: 4.4699 | dt: 60.95ms | tok/sec: 16800.32\n",
            "Step 616/1000 | Loss: 4.4425 | dt: 61.84ms | tok/sec: 16559.29\n",
            "Step 617/1000 | Loss: 4.6314 | dt: 60.31ms | tok/sec: 16978.50\n",
            "Step 618/1000 | Loss: 3.8359 | dt: 60.73ms | tok/sec: 16861.72\n",
            "Step 619/1000 | Loss: 4.3953 | dt: 60.30ms | tok/sec: 16981.52\n",
            "Step 620/1000 | Loss: 4.7034 | dt: 60.13ms | tok/sec: 17029.94\n",
            "Step 621/1000 | Loss: 4.8656 | dt: 60.50ms | tok/sec: 16926.65\n",
            "Step 622/1000 | Loss: 4.4459 | dt: 61.02ms | tok/sec: 16782.00\n",
            "Step 623/1000 | Loss: 4.5332 | dt: 61.00ms | tok/sec: 16787.97\n",
            "Step 624/1000 | Loss: 4.5147 | dt: 60.92ms | tok/sec: 16808.27\n",
            "Step 625/1000 | Loss: 4.1337 | dt: 60.77ms | tok/sec: 16851.27\n",
            "Step 626/1000 | Loss: 4.9324 | dt: 60.60ms | tok/sec: 16897.94\n",
            "Step 627/1000 | Loss: 4.6146 | dt: 60.61ms | tok/sec: 16894.95\n",
            "Step 628/1000 | Loss: 4.4645 | dt: 62.11ms | tok/sec: 16485.63\n",
            "Step 629/1000 | Loss: 4.1276 | dt: 61.35ms | tok/sec: 16691.22\n",
            "Step 630/1000 | Loss: 4.1316 | dt: 58.93ms | tok/sec: 17375.59\n",
            "Step 631/1000 | Loss: 4.2669 | dt: 59.61ms | tok/sec: 17178.29\n",
            "Step 632/1000 | Loss: 4.1714 | dt: 59.96ms | tok/sec: 17078.22\n",
            "Step 633/1000 | Loss: 4.1925 | dt: 60.17ms | tok/sec: 17018.80\n",
            "Step 634/1000 | Loss: 4.1736 | dt: 60.52ms | tok/sec: 16919.58\n",
            "Step 635/1000 | Loss: 4.1485 | dt: 60.15ms | tok/sec: 17025.01\n",
            "Step 636/1000 | Loss: 4.0072 | dt: 61.70ms | tok/sec: 16596.41\n",
            "Step 637/1000 | Loss: 3.8481 | dt: 59.56ms | tok/sec: 17193.90\n",
            "Step 638/1000 | Loss: 3.5817 | dt: 59.68ms | tok/sec: 17158.25\n",
            "Step 639/1000 | Loss: 4.3442 | dt: 60.74ms | tok/sec: 16859.67\n",
            "Step 640/1000 | Loss: 4.9581 | dt: 59.05ms | tok/sec: 17341.28\n",
            "Step 641/1000 | Loss: 5.0785 | dt: 60.50ms | tok/sec: 16926.98\n",
            "Step 642/1000 | Loss: 5.1981 | dt: 60.14ms | tok/sec: 17025.95\n",
            "Step 643/1000 | Loss: 5.2877 | dt: 62.32ms | tok/sec: 16431.76\n",
            "Step 644/1000 | Loss: 4.9260 | dt: 61.11ms | tok/sec: 16755.88\n",
            "Step 645/1000 | Loss: 5.1293 | dt: 62.13ms | tok/sec: 16482.21\n",
            "Step 646/1000 | Loss: 5.1124 | dt: 63.30ms | tok/sec: 16176.11\n",
            "Step 647/1000 | Loss: 4.7480 | dt: 63.20ms | tok/sec: 16202.90\n",
            "Step 648/1000 | Loss: 4.6516 | dt: 61.42ms | tok/sec: 16671.84\n",
            "Step 649/1000 | Loss: 4.8201 | dt: 62.79ms | tok/sec: 16308.04\n",
            "Step 650/1000 | Loss: 4.7392 | dt: 62.81ms | tok/sec: 16303.21\n",
            "Step 651/1000 | Loss: 5.0987 | dt: 61.06ms | tok/sec: 16770.99\n",
            "Step 652/1000 | Loss: 4.8797 | dt: 59.36ms | tok/sec: 17249.97\n",
            "Step 653/1000 | Loss: 5.0718 | dt: 61.44ms | tok/sec: 16666.86\n",
            "Step 654/1000 | Loss: 4.7769 | dt: 60.35ms | tok/sec: 16968.24\n",
            "Step 655/1000 | Loss: 5.0070 | dt: 60.90ms | tok/sec: 16813.86\n",
            "Step 656/1000 | Loss: 4.6213 | dt: 60.80ms | tok/sec: 16841.09\n",
            "Step 657/1000 | Loss: 4.8380 | dt: 59.74ms | tok/sec: 17142.29\n",
            "Step 658/1000 | Loss: 4.5628 | dt: 60.01ms | tok/sec: 17062.41\n",
            "Step 659/1000 | Loss: 4.7443 | dt: 60.52ms | tok/sec: 16921.24\n",
            "Step 660/1000 | Loss: 4.5136 | dt: 60.53ms | tok/sec: 16917.05\n",
            "Step 661/1000 | Loss: 4.4548 | dt: 60.86ms | tok/sec: 16825.66\n",
            "Step 662/1000 | Loss: 5.2292 | dt: 61.04ms | tok/sec: 16775.71\n",
            "Step 663/1000 | Loss: 4.3567 | dt: 60.92ms | tok/sec: 16810.11\n",
            "Step 664/1000 | Loss: 4.7070 | dt: 60.38ms | tok/sec: 16959.73\n",
            "Step 665/1000 | Loss: 4.6709 | dt: 60.65ms | tok/sec: 16883.13\n",
            "Step 666/1000 | Loss: 4.6490 | dt: 60.89ms | tok/sec: 16816.37\n",
            "Step 667/1000 | Loss: 4.5338 | dt: 60.40ms | tok/sec: 16954.98\n",
            "Step 668/1000 | Loss: 4.3346 | dt: 60.63ms | tok/sec: 16890.24\n",
            "Step 669/1000 | Loss: 4.2359 | dt: 60.66ms | tok/sec: 16880.88\n",
            "Step 670/1000 | Loss: 4.4734 | dt: 59.78ms | tok/sec: 17128.28\n",
            "Step 671/1000 | Loss: 3.9005 | dt: 60.86ms | tok/sec: 16824.67\n",
            "Step 672/1000 | Loss: 4.6912 | dt: 60.92ms | tok/sec: 16807.94\n",
            "Step 673/1000 | Loss: 3.9234 | dt: 60.96ms | tok/sec: 16796.77\n",
            "Step 674/1000 | Loss: 3.7451 | dt: 61.24ms | tok/sec: 16719.74\n",
            "Step 675/1000 | Loss: 4.2797 | dt: 60.48ms | tok/sec: 16932.39\n",
            "Step 676/1000 | Loss: 4.2223 | dt: 60.14ms | tok/sec: 17026.16\n",
            "Step 677/1000 | Loss: 4.2373 | dt: 61.04ms | tok/sec: 16776.17\n",
            "Step 678/1000 | Loss: 4.0126 | dt: 61.13ms | tok/sec: 16750.85\n",
            "Step 679/1000 | Loss: 4.2356 | dt: 60.82ms | tok/sec: 16835.55\n",
            "Step 680/1000 | Loss: 4.1742 | dt: 60.91ms | tok/sec: 16811.76\n",
            "Step 681/1000 | Loss: 4.3438 | dt: 61.42ms | tok/sec: 16672.75\n",
            "Step 682/1000 | Loss: 4.1274 | dt: 61.20ms | tok/sec: 16732.90\n",
            "Step 683/1000 | Loss: 4.5033 | dt: 59.75ms | tok/sec: 17137.03\n",
            "Step 684/1000 | Loss: 4.5276 | dt: 60.76ms | tok/sec: 16852.99\n",
            "Step 685/1000 | Loss: 4.5007 | dt: 61.05ms | tok/sec: 16772.76\n",
            "Step 686/1000 | Loss: 4.0166 | dt: 61.47ms | tok/sec: 16658.85\n",
            "Step 687/1000 | Loss: 4.2114 | dt: 60.97ms | tok/sec: 16795.39\n",
            "Step 688/1000 | Loss: 4.5186 | dt: 60.57ms | tok/sec: 16904.73\n",
            "Step 689/1000 | Loss: 4.0280 | dt: 60.33ms | tok/sec: 16973.87\n",
            "Step 690/1000 | Loss: 4.7252 | dt: 61.28ms | tok/sec: 16709.07\n",
            "Step 691/1000 | Loss: 4.6457 | dt: 60.99ms | tok/sec: 16790.79\n",
            "Step 692/1000 | Loss: 4.6024 | dt: 60.62ms | tok/sec: 16892.10\n",
            "Step 693/1000 | Loss: 4.5485 | dt: 60.90ms | tok/sec: 16813.73\n",
            "Step 694/1000 | Loss: 4.6171 | dt: 60.70ms | tok/sec: 16868.68\n",
            "Step 695/1000 | Loss: 4.5495 | dt: 59.92ms | tok/sec: 17090.66\n",
            "Step 696/1000 | Loss: 4.3906 | dt: 61.28ms | tok/sec: 16711.41\n",
            "Step 697/1000 | Loss: 5.0725 | dt: 60.94ms | tok/sec: 16804.79\n",
            "Step 698/1000 | Loss: 4.5499 | dt: 60.62ms | tok/sec: 16892.36\n",
            "Step 699/1000 | Loss: 4.9793 | dt: 60.81ms | tok/sec: 16838.58\n",
            "Step 700/1000 | Loss: 4.1388 | dt: 60.28ms | tok/sec: 16986.02\n",
            "Step 701/1000 | Loss: 4.0971 | dt: 61.03ms | tok/sec: 16778.26\n",
            "Step 702/1000 | Loss: 4.4929 | dt: 61.61ms | tok/sec: 16620.43\n",
            "Step 703/1000 | Loss: 4.5340 | dt: 60.89ms | tok/sec: 16818.01\n",
            "Step 704/1000 | Loss: 4.2265 | dt: 60.58ms | tok/sec: 16904.40\n",
            "Step 705/1000 | Loss: 4.4590 | dt: 60.94ms | tok/sec: 16802.81\n",
            "Step 706/1000 | Loss: 4.2734 | dt: 61.38ms | tok/sec: 16682.98\n",
            "Step 707/1000 | Loss: 4.3216 | dt: 61.34ms | tok/sec: 16693.03\n",
            "Step 708/1000 | Loss: 4.6277 | dt: 60.63ms | tok/sec: 16889.17\n",
            "Step 709/1000 | Loss: 4.2783 | dt: 60.60ms | tok/sec: 16897.41\n",
            "Step 710/1000 | Loss: 4.2268 | dt: 60.56ms | tok/sec: 16909.72\n",
            "Step 711/1000 | Loss: 4.2833 | dt: 60.67ms | tok/sec: 16879.15\n",
            "Step 712/1000 | Loss: 4.3397 | dt: 60.87ms | tok/sec: 16823.61\n",
            "Step 713/1000 | Loss: 4.3130 | dt: 61.01ms | tok/sec: 16783.84\n",
            "Step 714/1000 | Loss: 4.5016 | dt: 60.31ms | tok/sec: 16979.85\n",
            "Step 715/1000 | Loss: 4.3647 | dt: 61.07ms | tok/sec: 16767.65\n",
            "Step 716/1000 | Loss: 3.9145 | dt: 60.80ms | tok/sec: 16841.29\n",
            "Step 717/1000 | Loss: 4.2911 | dt: 60.32ms | tok/sec: 16975.82\n",
            "Step 718/1000 | Loss: 3.6035 | dt: 60.43ms | tok/sec: 16946.48\n",
            "Step 719/1000 | Loss: 3.9805 | dt: 61.16ms | tok/sec: 16744.12\n",
            "Step 720/1000 | Loss: 4.2565 | dt: 59.21ms | tok/sec: 17293.17\n",
            "Step 721/1000 | Loss: 4.1361 | dt: 61.02ms | tok/sec: 16781.41\n",
            "Step 722/1000 | Loss: 4.0372 | dt: 61.29ms | tok/sec: 16707.12\n",
            "Step 723/1000 | Loss: 4.4019 | dt: 61.16ms | tok/sec: 16744.05\n",
            "Step 724/1000 | Loss: 4.6226 | dt: 60.88ms | tok/sec: 16819.53\n",
            "Step 725/1000 | Loss: 4.4133 | dt: 60.82ms | tok/sec: 16837.26\n",
            "Step 726/1000 | Loss: 4.5869 | dt: 61.10ms | tok/sec: 16758.23\n",
            "Step 727/1000 | Loss: 4.2164 | dt: 60.48ms | tok/sec: 16932.12\n",
            "Step 728/1000 | Loss: 4.2070 | dt: 60.69ms | tok/sec: 16873.25\n",
            "Step 729/1000 | Loss: 4.1827 | dt: 60.79ms | tok/sec: 16845.19\n",
            "Step 730/1000 | Loss: 3.9683 | dt: 60.60ms | tok/sec: 16897.15\n",
            "Step 731/1000 | Loss: 4.7973 | dt: 60.77ms | tok/sec: 16851.80\n",
            "Step 732/1000 | Loss: 4.4797 | dt: 60.17ms | tok/sec: 17018.33\n",
            "Step 733/1000 | Loss: 4.1918 | dt: 60.80ms | tok/sec: 16843.34\n",
            "Step 734/1000 | Loss: 4.1139 | dt: 60.90ms | tok/sec: 16813.86\n",
            "Step 735/1000 | Loss: 3.7598 | dt: 59.37ms | tok/sec: 17247.55\n",
            "Step 736/1000 | Loss: 4.0734 | dt: 60.95ms | tok/sec: 16801.37\n",
            "Step 737/1000 | Loss: 4.0061 | dt: 60.56ms | tok/sec: 16907.66\n",
            "Step 738/1000 | Loss: 4.0103 | dt: 61.38ms | tok/sec: 16682.46\n",
            "Step 739/1000 | Loss: 4.6863 | dt: 61.25ms | tok/sec: 16718.51\n",
            "Step 740/1000 | Loss: 4.6475 | dt: 61.01ms | tok/sec: 16784.95\n",
            "Step 741/1000 | Loss: 4.4298 | dt: 60.44ms | tok/sec: 16943.54\n",
            "Step 742/1000 | Loss: 4.6590 | dt: 60.94ms | tok/sec: 16804.33\n",
            "Step 743/1000 | Loss: 4.3529 | dt: 60.92ms | tok/sec: 16807.81\n",
            "Step 744/1000 | Loss: 4.7335 | dt: 59.38ms | tok/sec: 17244.85\n",
            "Step 745/1000 | Loss: 4.8092 | dt: 61.23ms | tok/sec: 16724.95\n",
            "Step 746/1000 | Loss: 4.8046 | dt: 61.14ms | tok/sec: 16748.56\n",
            "Step 747/1000 | Loss: 4.8780 | dt: 60.85ms | tok/sec: 16828.89\n",
            "Step 748/1000 | Loss: 4.6333 | dt: 60.92ms | tok/sec: 16808.21\n",
            "Step 749/1000 | Loss: 4.8523 | dt: 60.77ms | tok/sec: 16851.60\n",
            "Step 750/1000 | Loss: 4.5316 | dt: 61.10ms | tok/sec: 16760.72\n",
            "Step 751/1000 | Loss: 4.4397 | dt: 60.81ms | tok/sec: 16840.10\n",
            "Step 752/1000 | Loss: 4.7498 | dt: 60.60ms | tok/sec: 16898.01\n",
            "Step 753/1000 | Loss: 4.7984 | dt: 61.03ms | tok/sec: 16778.72\n",
            "Step 754/1000 | Loss: 4.5878 | dt: 60.83ms | tok/sec: 16832.51\n",
            "Step 755/1000 | Loss: 4.5311 | dt: 60.97ms | tok/sec: 16793.94\n",
            "Step 756/1000 | Loss: 4.7917 | dt: 60.26ms | tok/sec: 16992.54\n",
            "Step 757/1000 | Loss: 4.4501 | dt: 60.70ms | tok/sec: 16869.67\n",
            "Step 758/1000 | Loss: 4.4364 | dt: 60.36ms | tok/sec: 16964.02\n",
            "Step 759/1000 | Loss: 4.3055 | dt: 60.46ms | tok/sec: 16935.99\n",
            "Step 760/1000 | Loss: 4.5928 | dt: 60.69ms | tok/sec: 16872.85\n",
            "Step 761/1000 | Loss: 4.6455 | dt: 60.54ms | tok/sec: 16913.12\n",
            "Step 762/1000 | Loss: 4.8428 | dt: 60.57ms | tok/sec: 16905.59\n",
            "Step 763/1000 | Loss: 4.8253 | dt: 60.59ms | tok/sec: 16900.74\n",
            "Step 764/1000 | Loss: 4.3730 | dt: 60.85ms | tok/sec: 16829.08\n",
            "Step 765/1000 | Loss: 4.7278 | dt: 59.90ms | tok/sec: 17096.30\n",
            "Step 766/1000 | Loss: 4.2958 | dt: 59.96ms | tok/sec: 17077.95\n",
            "Step 767/1000 | Loss: 4.7913 | dt: 60.32ms | tok/sec: 16976.42\n",
            "Step 768/1000 | Loss: 4.2764 | dt: 61.59ms | tok/sec: 16626.99\n",
            "Step 769/1000 | Loss: 4.5382 | dt: 61.25ms | tok/sec: 16718.25\n",
            "Step 770/1000 | Loss: 4.4854 | dt: 61.20ms | tok/sec: 16731.47\n",
            "Step 771/1000 | Loss: 4.4338 | dt: 61.00ms | tok/sec: 16785.87\n",
            "Step 772/1000 | Loss: 4.2315 | dt: 61.07ms | tok/sec: 16768.18\n",
            "Step 773/1000 | Loss: 4.4908 | dt: 60.90ms | tok/sec: 16814.79\n",
            "Step 774/1000 | Loss: 4.0584 | dt: 61.11ms | tok/sec: 16757.19\n",
            "Step 775/1000 | Loss: 3.6603 | dt: 60.99ms | tok/sec: 16789.41\n",
            "Step 776/1000 | Loss: 3.8963 | dt: 60.86ms | tok/sec: 16826.58\n",
            "Step 777/1000 | Loss: 4.1386 | dt: 59.87ms | tok/sec: 17103.66\n",
            "Step 778/1000 | Loss: 4.9599 | dt: 61.14ms | tok/sec: 16747.91\n",
            "Step 779/1000 | Loss: 4.4440 | dt: 60.59ms | tok/sec: 16900.20\n",
            "Step 780/1000 | Loss: 4.5121 | dt: 60.75ms | tok/sec: 16854.71\n",
            "Step 781/1000 | Loss: 4.7948 | dt: 60.64ms | tok/sec: 16886.78\n",
            "Step 782/1000 | Loss: 4.4666 | dt: 60.81ms | tok/sec: 16839.05\n",
            "Step 783/1000 | Loss: 4.9607 | dt: 60.21ms | tok/sec: 17007.01\n",
            "Step 784/1000 | Loss: 4.5668 | dt: 60.13ms | tok/sec: 17029.60\n",
            "Step 785/1000 | Loss: 4.6610 | dt: 59.53ms | tok/sec: 17200.10\n",
            "Step 786/1000 | Loss: 4.4810 | dt: 61.63ms | tok/sec: 16615.80\n",
            "Step 787/1000 | Loss: 5.1762 | dt: 61.54ms | tok/sec: 16638.97\n",
            "Step 788/1000 | Loss: 4.4386 | dt: 61.09ms | tok/sec: 16762.09\n",
            "Step 789/1000 | Loss: 4.2864 | dt: 60.64ms | tok/sec: 16886.78\n",
            "Step 790/1000 | Loss: 4.7337 | dt: 60.06ms | tok/sec: 17050.29\n",
            "Step 791/1000 | Loss: 4.3867 | dt: 62.45ms | tok/sec: 16396.70\n",
            "Step 792/1000 | Loss: 4.1705 | dt: 60.58ms | tok/sec: 16903.06\n",
            "Step 793/1000 | Loss: 5.0045 | dt: 60.80ms | tok/sec: 16840.76\n",
            "Step 794/1000 | Loss: 4.6216 | dt: 60.72ms | tok/sec: 16864.11\n",
            "Step 795/1000 | Loss: 4.1320 | dt: 61.03ms | tok/sec: 16777.61\n",
            "Step 796/1000 | Loss: 4.3203 | dt: 60.32ms | tok/sec: 16975.89\n",
            "Step 797/1000 | Loss: 4.5494 | dt: 60.86ms | tok/sec: 16824.40\n",
            "Step 798/1000 | Loss: 4.4661 | dt: 61.05ms | tok/sec: 16774.14\n",
            "Step 799/1000 | Loss: 4.4771 | dt: 59.89ms | tok/sec: 17097.26\n",
            "Step 800/1000 | Loss: 4.2373 | dt: 60.80ms | tok/sec: 16842.81\n",
            "Step 801/1000 | Loss: 4.8612 | dt: 63.09ms | tok/sec: 16231.74\n",
            "Step 802/1000 | Loss: 4.5927 | dt: 59.72ms | tok/sec: 17147.08\n",
            "Step 803/1000 | Loss: 4.5464 | dt: 60.41ms | tok/sec: 16949.76\n",
            "Step 804/1000 | Loss: 4.3423 | dt: 59.84ms | tok/sec: 17113.33\n",
            "Step 805/1000 | Loss: 4.4500 | dt: 61.16ms | tok/sec: 16743.86\n",
            "Step 806/1000 | Loss: 4.3241 | dt: 61.23ms | tok/sec: 16723.91\n",
            "Step 807/1000 | Loss: 3.8634 | dt: 60.69ms | tok/sec: 16872.72\n",
            "Step 808/1000 | Loss: 4.4644 | dt: 60.70ms | tok/sec: 16869.93\n",
            "Step 809/1000 | Loss: 4.0503 | dt: 60.94ms | tok/sec: 16804.59\n",
            "Step 810/1000 | Loss: 4.4874 | dt: 59.16ms | tok/sec: 17307.88\n",
            "Step 811/1000 | Loss: 4.2381 | dt: 61.01ms | tok/sec: 16784.49\n",
            "Step 812/1000 | Loss: 4.3844 | dt: 60.80ms | tok/sec: 16840.90\n",
            "Step 813/1000 | Loss: 4.2131 | dt: 60.74ms | tok/sec: 16858.55\n",
            "Step 814/1000 | Loss: 4.8539 | dt: 59.49ms | tok/sec: 17213.40\n",
            "Step 815/1000 | Loss: 4.9190 | dt: 61.10ms | tok/sec: 16759.67\n",
            "Step 816/1000 | Loss: 4.6923 | dt: 60.93ms | tok/sec: 16805.31\n",
            "Step 817/1000 | Loss: 4.6644 | dt: 60.88ms | tok/sec: 16821.11\n",
            "Step 818/1000 | Loss: 4.2785 | dt: 61.35ms | tok/sec: 16692.38\n",
            "Step 819/1000 | Loss: 4.5837 | dt: 59.30ms | tok/sec: 17269.25\n",
            "Step 820/1000 | Loss: 4.4835 | dt: 60.20ms | tok/sec: 17010.65\n",
            "Step 821/1000 | Loss: 4.3303 | dt: 61.22ms | tok/sec: 16726.90\n",
            "Step 822/1000 | Loss: 4.2566 | dt: 62.17ms | tok/sec: 16470.01\n",
            "Step 823/1000 | Loss: 4.2623 | dt: 60.88ms | tok/sec: 16821.24\n",
            "Step 824/1000 | Loss: 4.2457 | dt: 59.06ms | tok/sec: 17338.55\n",
            "Step 825/1000 | Loss: 4.4317 | dt: 59.74ms | tok/sec: 17142.02\n",
            "Step 826/1000 | Loss: 4.8338 | dt: 60.52ms | tok/sec: 16921.11\n",
            "Step 827/1000 | Loss: 4.7866 | dt: 60.65ms | tok/sec: 16883.60\n",
            "Step 828/1000 | Loss: 4.6110 | dt: 62.45ms | tok/sec: 16397.57\n",
            "Step 829/1000 | Loss: 4.7537 | dt: 59.28ms | tok/sec: 17274.26\n",
            "Step 830/1000 | Loss: 4.6394 | dt: 59.40ms | tok/sec: 17238.96\n",
            "Step 831/1000 | Loss: 4.5000 | dt: 58.93ms | tok/sec: 17375.38\n",
            "Step 832/1000 | Loss: 4.0504 | dt: 60.55ms | tok/sec: 16911.58\n",
            "Step 833/1000 | Loss: 4.6393 | dt: 59.73ms | tok/sec: 17144.41\n",
            "Step 834/1000 | Loss: 4.8663 | dt: 60.27ms | tok/sec: 16991.20\n",
            "Step 835/1000 | Loss: 4.4753 | dt: 61.21ms | tok/sec: 16728.99\n",
            "Step 836/1000 | Loss: 4.6152 | dt: 59.36ms | tok/sec: 17249.56\n",
            "Step 837/1000 | Loss: 4.5621 | dt: 60.12ms | tok/sec: 17032.64\n",
            "Step 838/1000 | Loss: 4.0503 | dt: 60.21ms | tok/sec: 17007.68\n",
            "Step 839/1000 | Loss: 3.3661 | dt: 60.63ms | tok/sec: 16889.37\n",
            "Step 840/1000 | Loss: 4.2572 | dt: 60.34ms | tok/sec: 16969.38\n",
            "Step 841/1000 | Loss: 4.6372 | dt: 61.09ms | tok/sec: 16762.03\n",
            "Step 842/1000 | Loss: 4.6206 | dt: 58.20ms | tok/sec: 17595.47\n",
            "Step 843/1000 | Loss: 4.5087 | dt: 61.80ms | tok/sec: 16568.43\n",
            "Step 844/1000 | Loss: 4.1612 | dt: 62.78ms | tok/sec: 16310.39\n",
            "Step 845/1000 | Loss: 4.3325 | dt: 61.39ms | tok/sec: 16680.07\n",
            "Step 846/1000 | Loss: 4.3891 | dt: 62.77ms | tok/sec: 16313.62\n",
            "Step 847/1000 | Loss: 4.1706 | dt: 62.86ms | tok/sec: 16290.41\n",
            "Step 848/1000 | Loss: 4.3316 | dt: 62.76ms | tok/sec: 16315.85\n",
            "Step 849/1000 | Loss: 4.2683 | dt: 62.45ms | tok/sec: 16396.82\n",
            "Step 850/1000 | Loss: 4.1420 | dt: 63.31ms | tok/sec: 16173.92\n",
            "Step 851/1000 | Loss: 4.5070 | dt: 62.94ms | tok/sec: 16268.82\n",
            "Step 852/1000 | Loss: 4.0289 | dt: 67.58ms | tok/sec: 15151.93\n",
            "Step 853/1000 | Loss: 4.2977 | dt: 62.34ms | tok/sec: 16425.61\n",
            "Step 854/1000 | Loss: 4.7135 | dt: 63.17ms | tok/sec: 16210.97\n",
            "Step 855/1000 | Loss: 4.1695 | dt: 62.46ms | tok/sec: 16395.07\n",
            "Step 856/1000 | Loss: 3.7029 | dt: 61.99ms | tok/sec: 16518.09\n",
            "Step 857/1000 | Loss: 4.3355 | dt: 63.30ms | tok/sec: 16176.90\n",
            "Step 858/1000 | Loss: 4.7124 | dt: 60.84ms | tok/sec: 16830.53\n",
            "Step 859/1000 | Loss: 4.9971 | dt: 61.02ms | tok/sec: 16781.80\n",
            "Step 860/1000 | Loss: 4.8547 | dt: 61.27ms | tok/sec: 16713.69\n",
            "Step 861/1000 | Loss: 4.9880 | dt: 61.45ms | tok/sec: 16663.50\n",
            "Step 862/1000 | Loss: 4.8021 | dt: 61.06ms | tok/sec: 16770.47\n",
            "Step 863/1000 | Loss: 5.0777 | dt: 60.95ms | tok/sec: 16800.32\n",
            "Step 864/1000 | Loss: 4.5512 | dt: 60.75ms | tok/sec: 16857.29\n",
            "Step 865/1000 | Loss: 4.7040 | dt: 59.87ms | tok/sec: 17103.79\n",
            "Step 866/1000 | Loss: 4.7689 | dt: 61.57ms | tok/sec: 16631.75\n",
            "Step 867/1000 | Loss: 4.6609 | dt: 60.80ms | tok/sec: 16842.94\n",
            "Step 868/1000 | Loss: 4.6683 | dt: 60.61ms | tok/sec: 16894.16\n",
            "Step 869/1000 | Loss: 4.3631 | dt: 60.53ms | tok/sec: 16916.11\n",
            "Step 870/1000 | Loss: 4.4532 | dt: 60.91ms | tok/sec: 16810.97\n",
            "Step 871/1000 | Loss: 4.5625 | dt: 59.38ms | tok/sec: 17246.16\n",
            "Step 872/1000 | Loss: 4.8672 | dt: 60.85ms | tok/sec: 16828.42\n",
            "Step 873/1000 | Loss: 4.5405 | dt: 61.33ms | tok/sec: 16696.67\n",
            "Step 874/1000 | Loss: 4.9079 | dt: 61.25ms | tok/sec: 16719.48\n",
            "Step 875/1000 | Loss: 4.9304 | dt: 60.87ms | tok/sec: 16822.56\n",
            "Step 876/1000 | Loss: 5.0325 | dt: 60.63ms | tok/sec: 16889.04\n",
            "Step 877/1000 | Loss: 5.1148 | dt: 61.16ms | tok/sec: 16743.14\n",
            "Step 878/1000 | Loss: 5.1069 | dt: 60.99ms | tok/sec: 16789.09\n",
            "Step 879/1000 | Loss: 4.6024 | dt: 60.68ms | tok/sec: 16874.97\n",
            "Step 880/1000 | Loss: 4.8962 | dt: 60.23ms | tok/sec: 17002.77\n",
            "Step 881/1000 | Loss: 4.8565 | dt: 60.60ms | tok/sec: 16897.15\n",
            "Step 882/1000 | Loss: 5.1172 | dt: 60.23ms | tok/sec: 17001.62\n",
            "Step 883/1000 | Loss: 4.5828 | dt: 59.45ms | tok/sec: 17223.20\n",
            "Step 884/1000 | Loss: 4.5694 | dt: 61.52ms | tok/sec: 16645.55\n",
            "Step 885/1000 | Loss: 4.5352 | dt: 60.85ms | tok/sec: 16828.95\n",
            "Step 886/1000 | Loss: 4.4483 | dt: 59.52ms | tok/sec: 17203.33\n",
            "Step 887/1000 | Loss: 4.7761 | dt: 61.39ms | tok/sec: 16679.74\n",
            "Step 888/1000 | Loss: 4.3784 | dt: 60.82ms | tok/sec: 16835.75\n",
            "Step 889/1000 | Loss: 4.4362 | dt: 60.94ms | tok/sec: 16804.72\n",
            "Step 890/1000 | Loss: 4.8468 | dt: 60.91ms | tok/sec: 16810.71\n",
            "Step 891/1000 | Loss: 4.5113 | dt: 60.92ms | tok/sec: 16807.81\n",
            "Step 892/1000 | Loss: 4.6272 | dt: 59.87ms | tok/sec: 17103.25\n",
            "Step 893/1000 | Loss: 4.4668 | dt: 61.04ms | tok/sec: 16775.84\n",
            "Step 894/1000 | Loss: 5.2037 | dt: 60.23ms | tok/sec: 17001.62\n",
            "Step 895/1000 | Loss: 4.9205 | dt: 61.01ms | tok/sec: 16784.23\n",
            "Step 896/1000 | Loss: 4.3673 | dt: 61.03ms | tok/sec: 16778.26\n",
            "Step 897/1000 | Loss: 4.3200 | dt: 61.11ms | tok/sec: 16756.60\n",
            "Step 898/1000 | Loss: 4.7556 | dt: 60.86ms | tok/sec: 16826.05\n",
            "Step 899/1000 | Loss: 4.5191 | dt: 60.72ms | tok/sec: 16862.91\n",
            "Step 900/1000 | Loss: 4.5851 | dt: 60.65ms | tok/sec: 16884.39\n",
            "Step 901/1000 | Loss: 4.7668 | dt: 61.21ms | tok/sec: 16728.08\n",
            "Step 902/1000 | Loss: 4.7221 | dt: 61.31ms | tok/sec: 16702.84\n",
            "Step 903/1000 | Loss: 4.6495 | dt: 59.93ms | tok/sec: 17086.44\n",
            "Step 904/1000 | Loss: 4.4171 | dt: 61.33ms | tok/sec: 16697.71\n",
            "Step 905/1000 | Loss: 4.1768 | dt: 60.75ms | tok/sec: 16855.24\n",
            "Step 906/1000 | Loss: 4.4164 | dt: 60.48ms | tok/sec: 16931.25\n",
            "Step 907/1000 | Loss: 4.2190 | dt: 60.68ms | tok/sec: 16876.43\n",
            "Step 908/1000 | Loss: 4.1045 | dt: 60.25ms | tok/sec: 16997.25\n",
            "Step 909/1000 | Loss: 4.5133 | dt: 60.97ms | tok/sec: 16794.40\n",
            "Step 910/1000 | Loss: 4.2490 | dt: 60.98ms | tok/sec: 16793.29\n",
            "Step 911/1000 | Loss: 4.3058 | dt: 60.17ms | tok/sec: 17019.75\n",
            "Step 912/1000 | Loss: 4.4351 | dt: 60.43ms | tok/sec: 16945.35\n",
            "Step 913/1000 | Loss: 4.4230 | dt: 61.22ms | tok/sec: 16726.32\n",
            "Step 914/1000 | Loss: 4.4767 | dt: 60.28ms | tok/sec: 16986.29\n",
            "Step 915/1000 | Loss: 4.6073 | dt: 60.75ms | tok/sec: 16856.23\n",
            "Step 916/1000 | Loss: 4.8618 | dt: 60.42ms | tok/sec: 16948.22\n",
            "Step 917/1000 | Loss: 4.3083 | dt: 60.62ms | tok/sec: 16893.16\n",
            "Step 918/1000 | Loss: 4.1999 | dt: 61.44ms | tok/sec: 16665.31\n",
            "Step 919/1000 | Loss: 4.5588 | dt: 61.78ms | tok/sec: 16575.27\n",
            "Step 920/1000 | Loss: 4.6067 | dt: 59.39ms | tok/sec: 17241.46\n",
            "Step 921/1000 | Loss: 4.2207 | dt: 61.29ms | tok/sec: 16706.67\n",
            "Step 922/1000 | Loss: 4.0993 | dt: 60.85ms | tok/sec: 16829.35\n",
            "Step 923/1000 | Loss: 4.3935 | dt: 60.47ms | tok/sec: 16934.05\n",
            "Step 924/1000 | Loss: 4.6270 | dt: 60.67ms | tok/sec: 16877.43\n",
            "Step 925/1000 | Loss: 4.0131 | dt: 60.95ms | tok/sec: 16799.66\n",
            "Step 926/1000 | Loss: 4.3691 | dt: 61.80ms | tok/sec: 16569.13\n",
            "Step 927/1000 | Loss: 4.1781 | dt: 60.22ms | tok/sec: 17005.66\n",
            "Step 928/1000 | Loss: 4.0472 | dt: 60.55ms | tok/sec: 16911.58\n",
            "Step 929/1000 | Loss: 4.0462 | dt: 60.64ms | tok/sec: 16886.45\n",
            "Step 930/1000 | Loss: 4.2113 | dt: 59.51ms | tok/sec: 17206.23\n",
            "Step 931/1000 | Loss: 3.8608 | dt: 60.83ms | tok/sec: 16833.17\n",
            "Step 932/1000 | Loss: 4.2023 | dt: 60.83ms | tok/sec: 16835.09\n",
            "Step 933/1000 | Loss: 3.8506 | dt: 60.62ms | tok/sec: 16893.42\n",
            "Step 934/1000 | Loss: 4.5319 | dt: 61.05ms | tok/sec: 16772.04\n",
            "Step 935/1000 | Loss: 4.7858 | dt: 61.17ms | tok/sec: 16741.51\n",
            "Step 936/1000 | Loss: 5.1724 | dt: 61.65ms | tok/sec: 16611.04\n",
            "Step 937/1000 | Loss: 4.5543 | dt: 60.21ms | tok/sec: 17007.55\n",
            "Step 938/1000 | Loss: 4.7866 | dt: 60.60ms | tok/sec: 16897.48\n",
            "Step 939/1000 | Loss: 4.6723 | dt: 60.42ms | tok/sec: 16949.02\n",
            "Step 940/1000 | Loss: 4.5226 | dt: 60.63ms | tok/sec: 16888.11\n",
            "Step 941/1000 | Loss: 4.3677 | dt: 60.54ms | tok/sec: 16913.65\n",
            "Step 942/1000 | Loss: 4.7632 | dt: 61.22ms | tok/sec: 16727.56\n",
            "Step 943/1000 | Loss: 4.4489 | dt: 60.94ms | tok/sec: 16802.81\n",
            "Step 944/1000 | Loss: 4.0441 | dt: 60.66ms | tok/sec: 16880.01\n",
            "Step 945/1000 | Loss: 4.2871 | dt: 60.73ms | tok/sec: 16861.06\n",
            "Step 946/1000 | Loss: 4.2864 | dt: 60.13ms | tok/sec: 17030.48\n",
            "Step 947/1000 | Loss: 4.5038 | dt: 59.74ms | tok/sec: 17140.17\n",
            "Step 948/1000 | Loss: 3.7051 | dt: 61.21ms | tok/sec: 16728.40\n",
            "Step 949/1000 | Loss: 4.2768 | dt: 60.48ms | tok/sec: 16930.32\n",
            "Step 950/1000 | Loss: 4.5484 | dt: 60.84ms | tok/sec: 16831.52\n",
            "Step 951/1000 | Loss: 4.7169 | dt: 60.54ms | tok/sec: 16913.05\n",
            "Step 952/1000 | Loss: 4.3275 | dt: 61.47ms | tok/sec: 16657.23\n",
            "Step 953/1000 | Loss: 4.4121 | dt: 61.01ms | tok/sec: 16784.69\n",
            "Step 954/1000 | Loss: 4.4111 | dt: 59.77ms | tok/sec: 17131.01\n",
            "Step 955/1000 | Loss: 4.0496 | dt: 59.72ms | tok/sec: 17146.74\n",
            "Step 956/1000 | Loss: 4.7205 | dt: 60.50ms | tok/sec: 16926.11\n",
            "Step 957/1000 | Loss: 4.5039 | dt: 60.89ms | tok/sec: 16816.17\n",
            "Step 958/1000 | Loss: 4.3798 | dt: 60.53ms | tok/sec: 16918.05\n",
            "Step 959/1000 | Loss: 4.0021 | dt: 60.76ms | tok/sec: 16852.26\n",
            "Step 960/1000 | Loss: 4.0233 | dt: 60.64ms | tok/sec: 16886.05\n",
            "Step 961/1000 | Loss: 4.1773 | dt: 60.27ms | tok/sec: 16991.06\n",
            "Step 962/1000 | Loss: 4.0512 | dt: 60.68ms | tok/sec: 16876.50\n",
            "Step 963/1000 | Loss: 4.1005 | dt: 60.62ms | tok/sec: 16892.56\n",
            "Step 964/1000 | Loss: 4.0802 | dt: 60.78ms | tok/sec: 16846.38\n",
            "Step 965/1000 | Loss: 4.0158 | dt: 60.84ms | tok/sec: 16831.59\n",
            "Step 966/1000 | Loss: 3.8880 | dt: 60.50ms | tok/sec: 16924.71\n",
            "Step 967/1000 | Loss: 3.7577 | dt: 60.84ms | tok/sec: 16831.52\n",
            "Step 968/1000 | Loss: 3.5170 | dt: 60.89ms | tok/sec: 16818.08\n",
            "Step 969/1000 | Loss: 4.2501 | dt: 61.06ms | tok/sec: 16771.39\n",
            "Step 970/1000 | Loss: 4.8011 | dt: 59.86ms | tok/sec: 17105.84\n",
            "Step 971/1000 | Loss: 4.8739 | dt: 60.62ms | tok/sec: 16892.43\n",
            "Step 972/1000 | Loss: 5.0293 | dt: 61.38ms | tok/sec: 16682.72\n",
            "Step 973/1000 | Loss: 5.1254 | dt: 60.05ms | tok/sec: 17051.51\n",
            "Step 974/1000 | Loss: 4.7747 | dt: 61.19ms | tok/sec: 16734.86\n",
            "Step 975/1000 | Loss: 4.9930 | dt: 61.17ms | tok/sec: 16741.18\n",
            "Step 976/1000 | Loss: 4.9716 | dt: 61.00ms | tok/sec: 16787.77\n",
            "Step 977/1000 | Loss: 4.6174 | dt: 60.81ms | tok/sec: 16839.31\n",
            "Step 978/1000 | Loss: 4.4061 | dt: 60.36ms | tok/sec: 16963.75\n",
            "Step 979/1000 | Loss: 4.6488 | dt: 60.73ms | tok/sec: 16860.60\n",
            "Step 980/1000 | Loss: 4.5830 | dt: 60.67ms | tok/sec: 16877.43\n",
            "Step 981/1000 | Loss: 4.8996 | dt: 60.00ms | tok/sec: 17065.94\n",
            "Step 982/1000 | Loss: 4.7065 | dt: 61.17ms | tok/sec: 16741.31\n",
            "Step 983/1000 | Loss: 4.8661 | dt: 60.65ms | tok/sec: 16882.60\n",
            "Step 984/1000 | Loss: 4.5355 | dt: 60.86ms | tok/sec: 16825.46\n",
            "Step 985/1000 | Loss: 4.8010 | dt: 60.68ms | tok/sec: 16874.31\n",
            "Step 986/1000 | Loss: 4.4060 | dt: 61.25ms | tok/sec: 16718.90\n",
            "Step 987/1000 | Loss: 4.6888 | dt: 60.61ms | tok/sec: 16893.69\n",
            "Step 988/1000 | Loss: 4.3969 | dt: 60.96ms | tok/sec: 16798.28\n",
            "Step 989/1000 | Loss: 4.6056 | dt: 61.43ms | tok/sec: 16669.06\n",
            "Step 990/1000 | Loss: 4.3817 | dt: 60.72ms | tok/sec: 16864.17\n",
            "Step 991/1000 | Loss: 4.3126 | dt: 59.04ms | tok/sec: 17345.34\n",
            "Step 992/1000 | Loss: 5.0732 | dt: 60.87ms | tok/sec: 16822.36\n",
            "Step 993/1000 | Loss: 4.2348 | dt: 60.69ms | tok/sec: 16873.12\n",
            "Step 994/1000 | Loss: 4.5877 | dt: 60.54ms | tok/sec: 16913.18\n",
            "Step 995/1000 | Loss: 4.5605 | dt: 60.54ms | tok/sec: 16915.25\n",
            "Step 996/1000 | Loss: 4.5256 | dt: 60.53ms | tok/sec: 16917.58\n",
            "Step 997/1000 | Loss: 4.4516 | dt: 60.82ms | tok/sec: 16836.41\n",
            "Step 998/1000 | Loss: 4.2224 | dt: 60.72ms | tok/sec: 16863.44\n",
            "Step 999/1000 | Loss: 4.1278 | dt: 60.73ms | tok/sec: 16861.72\n",
            "Step 1000/1000 | Loss: 4.3741 | dt: 60.69ms | tok/sec: 16872.72\n",
            "Epoch 3/20\n",
            "Step 1/1000 | Loss: 3.8074 | dt: 59.51ms | tok/sec: 17208.44\n",
            "Step 2/1000 | Loss: 4.5985 | dt: 61.18ms | tok/sec: 16737.79\n",
            "Step 3/1000 | Loss: 3.8200 | dt: 60.74ms | tok/sec: 16858.68\n",
            "Step 4/1000 | Loss: 3.6728 | dt: 60.83ms | tok/sec: 16832.51\n",
            "Step 5/1000 | Loss: 4.1820 | dt: 60.81ms | tok/sec: 16838.19\n",
            "Step 6/1000 | Loss: 4.1034 | dt: 61.00ms | tok/sec: 16786.20\n",
            "Step 7/1000 | Loss: 4.1266 | dt: 61.30ms | tok/sec: 16705.50\n",
            "Step 8/1000 | Loss: 3.8872 | dt: 60.35ms | tok/sec: 16967.77\n",
            "Step 9/1000 | Loss: 4.1088 | dt: 60.47ms | tok/sec: 16934.59\n",
            "Step 10/1000 | Loss: 4.1106 | dt: 60.31ms | tok/sec: 16977.63\n",
            "Step 11/1000 | Loss: 4.2758 | dt: 60.05ms | tok/sec: 17051.78\n",
            "Step 12/1000 | Loss: 4.0661 | dt: 59.57ms | tok/sec: 17190.94\n",
            "Step 13/1000 | Loss: 4.4476 | dt: 61.05ms | tok/sec: 16773.81\n",
            "Step 14/1000 | Loss: 4.4732 | dt: 61.16ms | tok/sec: 16743.60\n",
            "Step 15/1000 | Loss: 4.4245 | dt: 60.95ms | tok/sec: 16801.70\n",
            "Step 16/1000 | Loss: 3.9581 | dt: 61.07ms | tok/sec: 16767.13\n",
            "Step 17/1000 | Loss: 4.1693 | dt: 61.06ms | tok/sec: 16769.75\n",
            "Step 18/1000 | Loss: 4.5045 | dt: 61.86ms | tok/sec: 16552.66\n",
            "Step 19/1000 | Loss: 4.0132 | dt: 62.18ms | tok/sec: 16469.32\n",
            "Step 20/1000 | Loss: 4.7035 | dt: 64.44ms | tok/sec: 15889.93\n",
            "Step 21/1000 | Loss: 4.6005 | dt: 64.36ms | tok/sec: 15910.53\n",
            "Step 22/1000 | Loss: 4.5886 | dt: 62.54ms | tok/sec: 16374.38\n",
            "Step 23/1000 | Loss: 4.5177 | dt: 62.20ms | tok/sec: 16464.01\n",
            "Step 24/1000 | Loss: 4.5713 | dt: 62.18ms | tok/sec: 16468.62\n",
            "Step 25/1000 | Loss: 4.4420 | dt: 60.45ms | tok/sec: 16940.73\n",
            "Step 26/1000 | Loss: 4.3020 | dt: 61.13ms | tok/sec: 16752.28\n",
            "Step 27/1000 | Loss: 5.0077 | dt: 61.40ms | tok/sec: 16676.96\n",
            "Step 28/1000 | Loss: 4.3986 | dt: 59.77ms | tok/sec: 17131.90\n",
            "Step 29/1000 | Loss: 4.8152 | dt: 61.26ms | tok/sec: 16715.06\n",
            "Step 30/1000 | Loss: 3.9583 | dt: 60.43ms | tok/sec: 16946.01\n",
            "Step 31/1000 | Loss: 3.9084 | dt: 60.47ms | tok/sec: 16934.32\n",
            "Step 32/1000 | Loss: 4.3695 | dt: 60.62ms | tok/sec: 16892.36\n",
            "Step 33/1000 | Loss: 4.4232 | dt: 60.28ms | tok/sec: 16988.44\n",
            "Step 34/1000 | Loss: 4.1336 | dt: 59.34ms | tok/sec: 17257.18\n",
            "Step 35/1000 | Loss: 4.3476 | dt: 62.87ms | tok/sec: 16288.19\n",
            "Step 36/1000 | Loss: 4.1805 | dt: 63.26ms | tok/sec: 16186.23\n",
            "Step 37/1000 | Loss: 4.2616 | dt: 64.49ms | tok/sec: 15878.88\n",
            "Step 38/1000 | Loss: 4.6292 | dt: 63.90ms | tok/sec: 16025.52\n",
            "Step 39/1000 | Loss: 4.3004 | dt: 64.09ms | tok/sec: 15978.72\n",
            "Step 40/1000 | Loss: 4.2108 | dt: 62.41ms | tok/sec: 16408.29\n",
            "Step 41/1000 | Loss: 4.2677 | dt: 61.73ms | tok/sec: 16589.42\n",
            "Step 42/1000 | Loss: 4.3291 | dt: 63.06ms | tok/sec: 16239.04\n",
            "Step 43/1000 | Loss: 4.2935 | dt: 62.82ms | tok/sec: 16301.67\n",
            "Step 44/1000 | Loss: 4.4594 | dt: 65.22ms | tok/sec: 15700.33\n",
            "Step 45/1000 | Loss: 4.3639 | dt: 64.84ms | tok/sec: 15793.11\n",
            "Step 46/1000 | Loss: 3.8900 | dt: 63.60ms | tok/sec: 16100.67\n",
            "Step 47/1000 | Loss: 4.2902 | dt: 61.44ms | tok/sec: 16666.67\n",
            "Step 48/1000 | Loss: 3.6130 | dt: 61.52ms | tok/sec: 16643.93\n",
            "Step 49/1000 | Loss: 3.9638 | dt: 61.75ms | tok/sec: 16581.67\n",
            "Step 50/1000 | Loss: 4.2532 | dt: 59.67ms | tok/sec: 17162.02\n",
            "Step 51/1000 | Loss: 4.0950 | dt: 58.53ms | tok/sec: 17493.92\n",
            "Step 52/1000 | Loss: 4.0350 | dt: 60.57ms | tok/sec: 16905.13\n",
            "Step 53/1000 | Loss: 4.3864 | dt: 61.26ms | tok/sec: 16716.10\n",
            "Step 54/1000 | Loss: 4.6107 | dt: 59.99ms | tok/sec: 17069.80\n",
            "Step 55/1000 | Loss: 4.3759 | dt: 61.11ms | tok/sec: 16755.81\n",
            "Step 56/1000 | Loss: 4.5845 | dt: 61.05ms | tok/sec: 16771.84\n",
            "Step 57/1000 | Loss: 4.1727 | dt: 60.64ms | tok/sec: 16886.18\n",
            "Step 58/1000 | Loss: 4.2107 | dt: 59.66ms | tok/sec: 17162.64\n",
            "Step 59/1000 | Loss: 4.1652 | dt: 59.39ms | tok/sec: 17242.08\n",
            "Step 60/1000 | Loss: 3.9334 | dt: 61.48ms | tok/sec: 16655.49\n",
            "Step 61/1000 | Loss: 4.7607 | dt: 61.52ms | tok/sec: 16645.35\n",
            "Step 62/1000 | Loss: 4.5132 | dt: 61.19ms | tok/sec: 16734.20\n",
            "Step 63/1000 | Loss: 4.1738 | dt: 60.77ms | tok/sec: 16850.74\n",
            "Step 64/1000 | Loss: 4.0600 | dt: 61.00ms | tok/sec: 16786.33\n",
            "Step 65/1000 | Loss: 3.7367 | dt: 61.40ms | tok/sec: 16677.87\n",
            "Step 66/1000 | Loss: 4.0348 | dt: 60.46ms | tok/sec: 16937.53\n",
            "Step 67/1000 | Loss: 3.9668 | dt: 59.63ms | tok/sec: 17173.76\n",
            "Step 68/1000 | Loss: 3.9765 | dt: 60.87ms | tok/sec: 16823.41\n",
            "Step 69/1000 | Loss: 4.7284 | dt: 60.86ms | tok/sec: 16826.31\n",
            "Step 70/1000 | Loss: 4.6565 | dt: 60.71ms | tok/sec: 16867.22\n",
            "Step 71/1000 | Loss: 4.3969 | dt: 60.60ms | tok/sec: 16898.87\n",
            "Step 72/1000 | Loss: 4.6749 | dt: 60.62ms | tok/sec: 16892.49\n",
            "Step 73/1000 | Loss: 4.3390 | dt: 61.47ms | tok/sec: 16659.69\n",
            "Step 74/1000 | Loss: 4.7358 | dt: 60.78ms | tok/sec: 16848.10\n",
            "Step 75/1000 | Loss: 4.8229 | dt: 61.94ms | tok/sec: 16531.12\n",
            "Step 76/1000 | Loss: 4.8056 | dt: 60.89ms | tok/sec: 16816.76\n",
            "Step 77/1000 | Loss: 4.8523 | dt: 60.63ms | tok/sec: 16890.43\n",
            "Step 78/1000 | Loss: 4.5910 | dt: 60.37ms | tok/sec: 16961.27\n",
            "Step 79/1000 | Loss: 4.8156 | dt: 61.18ms | tok/sec: 16737.79\n",
            "Step 80/1000 | Loss: 4.5233 | dt: 61.40ms | tok/sec: 16678.84\n",
            "Step 81/1000 | Loss: 4.3867 | dt: 65.70ms | tok/sec: 15585.59\n",
            "Step 82/1000 | Loss: 4.7855 | dt: 62.77ms | tok/sec: 16312.75\n",
            "Step 83/1000 | Loss: 4.8079 | dt: 62.93ms | tok/sec: 16271.77\n",
            "Step 84/1000 | Loss: 4.5859 | dt: 62.61ms | tok/sec: 16355.61\n",
            "Step 85/1000 | Loss: 4.5481 | dt: 61.53ms | tok/sec: 16643.10\n",
            "Step 86/1000 | Loss: 4.8312 | dt: 61.55ms | tok/sec: 16637.68\n",
            "Step 87/1000 | Loss: 4.4287 | dt: 63.62ms | tok/sec: 16094.64\n",
            "Step 88/1000 | Loss: 4.3698 | dt: 58.63ms | tok/sec: 17464.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EBT2ooj1twY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}