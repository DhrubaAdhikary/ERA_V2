{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uYpP-Z7h_cP1"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "209YchQpAWxC",
        "outputId": "b48b342f-6ffb-4861-89fa-da5a5ee3da33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: kaggle.json: No such file or directory\n",
            "chmod: /Users/kvzm411/.kaggle/kaggle.json: No such file or directory\n",
            "mkdir: corpus: File exists\n",
            "zsh:1: command not found: kaggle\n",
            "unzip:  cannot find or open /content/hindi-wikipedia-articles-172k.zip, /content/hindi-wikipedia-articles-172k.zip.zip or /content/hindi-wikipedia-articles-172k.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !mkdir corpus\n",
        "# !kaggle datasets download -d disisbig/hindi-wikipedia-articles-172k\n",
        "# !unzip /content/hindi-wikipedia-articles-172k.zip -d /content/corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7IblqXoApAk",
        "outputId": "0d0c05f4-2c14-492b-a067-645bff9f0c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error writing to /content/corpus/train/full.txt: [Errno 2] No such file or directory: '/content/corpus/train/full.txt'\n",
            "Error writing to /content/corpus/valid/full_val.txt: [Errno 2] No such file or directory: '/content/corpus/valid/full_val.txt'\n"
          ]
        }
      ],
      "source": [
        "# Get list of training and validation text files\n",
        "train_files = glob.glob(\"/content/corpus/train/train/*.txt\")\n",
        "valid_files = glob.glob(\"/content/corpus/valid/valid/*.txt\")\n",
        "\n",
        "def concatenate_files(file_list, output_path):\n",
        "    \"\"\"\n",
        "    Concatenates text files into a single file with two newlines between each file's content.\n",
        "\n",
        "    Args:\n",
        "    - file_list (list of str): List of file paths to be concatenated.\n",
        "    - output_path (str): Path to the output file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_path, \"wb\") as outfile:\n",
        "            for file_path in file_list:\n",
        "                try:\n",
        "                    with open(file_path, \"rb\") as infile:\n",
        "                        outfile.write(infile.read())\n",
        "                        outfile.write(b\"\\n\\n\")\n",
        "                except FileNotFoundError:\n",
        "                    print(f\"File not found: {file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "        print(f\"Successfully concatenated files into {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to {output_path}: {e}\")\n",
        "\n",
        "# Concatenate train and validation files\n",
        "concatenate_files(train_files, \"/content/corpus/train/full.txt\")\n",
        "concatenate_files(valid_files, \"/content/corpus/valid/full_val.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nzGg0kuaWbZ7"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "No such file or directory (os error 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m corpus_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/corpus/train/full.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizers\u001b[38;5;241m.\u001b[39mSentencePieceBPETokenizer()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhindi_bpe_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tokenizers/implementations/sentencepiece_bpe.py:80\u001b[0m, in \u001b[0;36mSentencePieceBPETokenizer.train\u001b[0;34m(self, files, vocab_size, min_frequency, special_tokens, limit_alphabet, initial_alphabet, show_progress)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     79\u001b[0m     files \u001b[38;5;241m=\u001b[39m [files]\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
          ]
        }
      ],
      "source": [
        "corpus_file = \"/content/corpus/train/full.txt\"\n",
        "tokenizer = tokenizers.SentencePieceBPETokenizer()\n",
        "tokenizer.train(corpus_file)\n",
        "tokenizer.save(\"hindi_bpe_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJITcmdDB5--",
        "outputId": "3eea718f-df1b-470a-a651-34189a91dcdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded text: ['▁यह', '▁एक', '▁उदाहरण', '▁पाठ', '▁है।', '▁हम', '▁बी', 'पी', 'ई', '▁टोक', 'नाइ', 'ज़र', '▁का', '▁उपयोग', '▁करेंगे।']\n",
            "Vocabulary size: 30000\n",
            "Compression ratio: 3.7333333333333334\n",
            "Decoded text: यह एक उदाहरण पाठ है। हम बीपीई टोकनाइज़र का उपयोग करेंगे।\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer.from_file(\"hindi_bpe_tokenizer.json\")\n",
        "\n",
        "def compression_ratio(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    compressed_length = len(encoded.ids)\n",
        "    original_length = len(text)\n",
        "    return original_length / compressed_length\n",
        "\n",
        "sample_text = \"यह एक उदाहरण पाठ है। हम बीपीई टोकनाइज़र का उपयोग करेंगे।\"\n",
        "encoded_text = tokenizer.encode(sample_text)\n",
        "compression = compression_ratio(sample_text, tokenizer)\n",
        "\n",
        "print(f\"Encoded text: {encoded_text.tokens}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer.get_vocab())}\")\n",
        "print(f\"Compression ratio: {compression}\")\n",
        "print(f\"Decoded text: {tokenizer.decode(encoded_text.ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
